Model_ID	Name	Prediction Type	End Point	Group	Title	Number compounds in the training set	Number compounds in the test set	Validation approach	R2	RMSE	Balanced Accuracy	AUC	Descriptors	Machine learning approach	Applicability domain	Reference	Source	Link
CHEMBL1075097	Arginase-1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Arginase-1 -Log(IC50) M model	45	9	external test set validation 	0.62	0.521	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1075104	Leucine-rich repeat serine/threonine-protein kinase 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Leucine-rich repeat serine/threonine-protein kinase 2 -Log(IC50) M model	275	55	external test set validation 	0.34	0.775	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1075121	Putative hydrolase RBBP9 -Log(IC50) M	Regression	-Log(IC50) M	Target	Putative hydrolase RBBP9 -Log(IC50) M model	5	1	external test set validation 	nan	0.98	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1075132	"Heat shock protein 75 kDa, mitochondrial -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Heat shock protein 75 kDa, mitochondrial -Log(IC50) M model"	25	5	external test set validation 	0.15	0.292	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1075138	Tyrosyl-DNA phosphodiesterase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Tyrosyl-DNA phosphodiesterase 1 -Log(IC50) M model	120	24	external test set validation 	0.53	0.297	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1075145	Transitional endoplasmic reticulum ATPase -Log(IC50) M	Regression	-Log(IC50) M	Target	Transitional endoplasmic reticulum ATPase -Log(IC50) M model	325	65	external test set validation 	0.62	0.528	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1075149	Cystine/glutamate transporter -Log(IC50) M	Regression	-Log(IC50) M	Target	Cystine/glutamate transporter -Log(IC50) M model	30	6	external test set validation 	0.95	0.652	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1075152	Glucokinase regulatory protein -Log(IC50) M	Regression	-Log(IC50) M	Target	Glucokinase regulatory protein -Log(IC50) M model	65	13	external test set validation 	0.78	0.552	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1075169	Protein tyrosine phosphatase type IVA 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Protein tyrosine phosphatase type IVA 1 -Log(IC50) M model	20	4	external test set validation 	0.24	0.903	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1075315	C-X-C chemokine receptor type 5 -Log(IC50) M	Regression	-Log(IC50) M	Target	C-X-C chemokine receptor type 5 -Log(IC50) M model	60	12	external test set validation 	0.76	0.545	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1075317	WD repeat-containing protein 5 -Log(IC50) M	Regression	-Log(IC50) M	Target	WD repeat-containing protein 5 -Log(IC50) M model	35	7	external test set validation 	0.04	0.837	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1075319	Transient receptor potential cation channel subfamily M member 8 -Log(IC50) M	Regression	-Log(IC50) M	Target	Transient receptor potential cation channel subfamily M member 8 -Log(IC50) M model	330	66	external test set validation 	0.63	0.574	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1075322	G-protein coupled receptor 55 -Log(IC50) M	Regression	-Log(IC50) M	Target	G-protein coupled receptor 55 -Log(IC50) M model	410	82	external test set validation 	0.13	0.442	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1075323	Endoplasmin -Log(IC50) M	Regression	-Log(IC50) M	Target	Endoplasmin -Log(IC50) M model	70	14	external test set validation 	0.18	0.652	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1163101	Serine/threonine-protein kinase/endoribonuclease IRE1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Serine/threonine-protein kinase/endoribonuclease IRE1 -Log(IC50) M model	245	49	external test set validation 	0.36	0.624	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1163125	Bromodomain-containing protein 4 -Log(IC50) M	Regression	-Log(IC50) M	Target	Bromodomain-containing protein 4 -Log(IC50) M model	660	132	external test set validation 	0.69	0.533	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1250375	NADPH oxidase 4 -Log(IC50) M	Regression	-Log(IC50) M	Target	NADPH oxidase 4 -Log(IC50) M model	35	7	external test set validation 	0.43	0.28	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1250417	Potassium/sodium hyperpolarization-activated cyclic nucleotide-gated channel 4 -Log(IC50) M	Regression	-Log(IC50) M	Target	Potassium/sodium hyperpolarization-activated cyclic nucleotide-gated channel 4 -Log(IC50) M model	15	3	external test set validation 	0.89	0.399	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1255126	Protein Mdm4 -Log(IC50) M	Regression	-Log(IC50) M	Target	Protein Mdm4 -Log(IC50) M model	250	50	external test set validation 	0.69	0.438	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1255137	Protein Wnt-3a -Log(IC50) M	Regression	-Log(IC50) M	Target	Protein Wnt-3a -Log(IC50) M model	35	7	external test set validation 	0.73	0.601	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1275212	Ketohexokinase -Log(IC50) M	Regression	-Log(IC50) M	Target	Ketohexokinase -Log(IC50) M model	200	40	external test set validation 	0.46	0.603	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1275221	Protein arginine N-methyltransferase 6 -Log(IC50) M	Regression	-Log(IC50) M	Target	Protein arginine N-methyltransferase 6 -Log(IC50) M model	30	6	external test set validation 	0.72	0.603	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1275223	Heat shock cognate 71 kDa protein -Log(IC50) M	Regression	-Log(IC50) M	Target	Heat shock cognate 71 kDa protein -Log(IC50) M model	70	14	external test set validation 	0.59	0.579	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1287622	Lethal(3)malignant brain tumor-like protein 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Lethal(3)malignant brain tumor-like protein 1 -Log(IC50) M model	100	20	external test set validation 	0.67	0.343	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1287623	Lethal(3)malignant brain tumor-like protein 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Lethal(3)malignant brain tumor-like protein 3 -Log(IC50) M model	100	20	external test set validation 	0.75	0.442	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1287627	Cytochrome b-245 heavy chain -Log(IC50) M	Regression	-Log(IC50) M	Target	Cytochrome b-245 heavy chain -Log(IC50) M model	10	2	external test set validation 	1	0.142	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1287628	NADPH oxidase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	NADPH oxidase 1 -Log(IC50) M model	95	19	external test set validation 	0.49	0.511	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1293194	"Carnitine O-palmitoyltransferase 1, liver isoform -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Carnitine O-palmitoyltransferase 1, liver isoform -Log(IC50) M model"	410	82	external test set validation 	0.69	0.275	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1293197	Acidic mammalian chitinase -Log(IC50) M	Regression	-Log(IC50) M	Target	Acidic mammalian chitinase -Log(IC50) M model	15	3	external test set validation 	0.8	0.849	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1293222	Nucleotide-binding oligomerization domain-containing protein 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Nucleotide-binding oligomerization domain-containing protein 1 -Log(IC50) M model	205	41	external test set validation 	0.43	0.453	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1293224	Microtubule-associated protein tau -Log(IC50) M	Regression	-Log(IC50) M	Target	Microtubule-associated protein tau -Log(IC50) M model	50	10	external test set validation 	0.94	0.501	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1293226	Lysine-specific demethylase 4E -Log(IC50) M	Regression	-Log(IC50) M	Target	Lysine-specific demethylase 4E -Log(IC50) M model	35	7	external test set validation 	0.3	0.9	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1293229	Nuclear receptor subfamily 4 group A member 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Nuclear receptor subfamily 4 group A member 1 -Log(IC50) M model	115	23	external test set validation 	0	0.594	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1293237	Bloom syndrome protein -Log(IC50) M	Regression	-Log(IC50) M	Target	Bloom syndrome protein -Log(IC50) M model	35	7	external test set validation 	0.47	0.408	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1293249	Krueppel-like factor 5 -Log(IC50) M	Regression	-Log(IC50) M	Target	Krueppel-like factor 5 -Log(IC50) M model	125	25	external test set validation 	0.01	0.402	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1293255	15-hydroxyprostaglandin dehydrogenase [NAD(+)] -Log(IC50) M	Regression	-Log(IC50) M	Target	15-hydroxyprostaglandin dehydrogenase [NAD(+)] -Log(IC50) M model	190	38	external test set validation 	0.33	0.648	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1293258	Mothers against decapentaplegic homolog 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Mothers against decapentaplegic homolog 3 -Log(IC50) M model	45	9	external test set validation 	0.33	0.856	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1293265	Serine/threonine-protein phosphatase -Log(IC50) M	Regression	-Log(IC50) M	Target	Serine/threonine-protein phosphatase -Log(IC50) M model	60	12	external test set validation 	0.08	0.641	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1293266	Nucleotide-binding oligomerization domain-containing protein 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Nucleotide-binding oligomerization domain-containing protein 2 -Log(IC50) M model	50	10	external test set validation 	0.6	0.296	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1293267	G-protein coupled receptor 35 -Log(IC50) M	Regression	-Log(IC50) M	Target	G-protein coupled receptor 35 -Log(IC50) M model	145	29	external test set validation 	0.22	0.505	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1293274	Eukaryotic translation initiation factor 4H -Log(IC50) M	Regression	-Log(IC50) M	Target	Eukaryotic translation initiation factor 4H -Log(IC50) M model	90	18	external test set validation 	0.07	0.425	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1293287	Insulin-degrading enzyme -Log(IC50) M	Regression	-Log(IC50) M	Target	Insulin-degrading enzyme -Log(IC50) M model	105	21	external test set validation 	0.16	0.275	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1293289	Bromodomain-containing protein 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Bromodomain-containing protein 2 -Log(IC50) M model	275	55	external test set validation 	0.82	0.444	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1293292	ATP-sensitive inward rectifier potassium channel 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	ATP-sensitive inward rectifier potassium channel 1 -Log(IC50) M model	160	32	external test set validation 	0.57	0.619	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1293293	Neuropeptides B/W receptor type 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Neuropeptides B/W receptor type 1 -Log(IC50) M model	250	50	external test set validation 	0.45	0.481	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1293296	14-3-3 protein gamma -Log(IC50) M	Regression	-Log(IC50) M	Target	14-3-3 protein gamma -Log(IC50) M model	10	2	external test set validation 	1	0.427	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1293320	Protein phosphatase methylesterase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Protein phosphatase methylesterase 1 -Log(IC50) M model	5	1	external test set validation 	nan	0.16	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1615381	Menin -Log(IC50) M	Regression	-Log(IC50) M	Target	Menin -Log(IC50) M model	100	20	external test set validation 	0.6	0.784	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1615382	Nuclear receptor coactivator 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Nuclear receptor coactivator 3 -Log(IC50) M model	185	37	external test set validation 	0.16	0.437	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1615386	Core-binding factor subunit beta -Log(IC50) M	Regression	-Log(IC50) M	Target	Core-binding factor subunit beta -Log(IC50) M model	25	5	external test set validation 	0.71	0.18	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1615387	Nuclear receptor coactivator 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Nuclear receptor coactivator 1 -Log(IC50) M model	110	22	external test set validation 	0	0.318	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1628461	Oxoeicosanoid receptor 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Oxoeicosanoid receptor 1 -Log(IC50) M model	55	11	external test set validation 	0.58	0.683	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1628481	Apelin receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Apelin receptor -Log(IC50) M model	65	13	external test set validation 	0.96	0.36	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1641347	Solute carrier family 22 member 6 -Log(IC50) M	Regression	-Log(IC50) M	Target	Solute carrier family 22 member 6 -Log(IC50) M model	50	10	external test set validation 	0.41	0.782	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1681611	Prolactin-releasing peptide receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Prolactin-releasing peptide receptor -Log(IC50) M model	45	9	external test set validation 	0.65	0.476	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1697668	Solute carrier organic anion transporter family member 1B1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Solute carrier organic anion transporter family member 1B1 -Log(IC50) M model	125	25	external test set validation 	0.13	0.521	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1697671	Placenta growth factor -Log(IC50) M	Regression	-Log(IC50) M	Target	Placenta growth factor -Log(IC50) M model	15	3	external test set validation 	0.19	0.725	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1741162	Phosphomannomutase 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Phosphomannomutase 2 -Log(IC50) M model	70	14	external test set validation 	0.07	0.741	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1741176	X-box-binding protein 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	X-box-binding protein 1 -Log(IC50) M model	600	120	external test set validation 	0.23	0.37	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1741179	DNA dC->dU-editing enzyme APOBEC-3A -Log(IC50) M	Regression	-Log(IC50) M	Target	DNA dC->dU-editing enzyme APOBEC-3A -Log(IC50) M model	445	89	external test set validation 	0.21	0.35	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1741186	Nuclear receptor ROR-gamma -Log(IC50) M	Regression	-Log(IC50) M	Target	Nuclear receptor ROR-gamma -Log(IC50) M model	465	93	external test set validation 	0.69	0.755	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1741207	Sentrin-specific protease 8 -Log(IC50) M	Regression	-Log(IC50) M	Target	Sentrin-specific protease 8 -Log(IC50) M model	660	132	external test set validation 	0.24	0.467	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1741208	"NACHT, LRR and PYD domains-containing protein 3 -Log(IC50) M"	Regression	-Log(IC50) M	Target	"NACHT, LRR and PYD domains-containing protein 3 -Log(IC50) M model"	80	16	external test set validation 	0.47	0.367	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1741213	Sentrin-specific protease 7 -Log(IC50) M	Regression	-Log(IC50) M	Target	Sentrin-specific protease 7 -Log(IC50) M model	865	173	external test set validation 	0.1	0.399	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1741215	Sentrin-specific protease 6 -Log(IC50) M	Regression	-Log(IC50) M	Target	Sentrin-specific protease 6 -Log(IC50) M model	745	149	external test set validation 	0.24	0.419	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1741217	DNA dC->dU-editing enzyme APOBEC-3G -Log(IC50) M	Regression	-Log(IC50) M	Target	DNA dC->dU-editing enzyme APOBEC-3G -Log(IC50) M model	900	180	external test set validation 	0.35	0.366	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1741220	Bromodomain adjacent to zinc finger domain protein 2B|Bromodomain adjacent to zinc finger domain protein 2B -Log(IC50) M	Regression	-Log(IC50) M	Target	Bromodomain adjacent to zinc finger domain protein 2B|Bromodomain adjacent to zinc finger domain protein 2B -Log(IC50) M model	70	14	external test set validation 	0.84	0.44	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1741221	Cysteine protease ATG4B -Log(IC50) M	Regression	-Log(IC50) M	Target	Cysteine protease ATG4B -Log(IC50) M model	355	71	external test set validation 	0	0.384	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1743121	Solute carrier organic anion transporter family member 1B3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Solute carrier organic anion transporter family member 1B3 -Log(IC50) M model	100	20	external test set validation 	0.27	0.598	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1743122	Solute carrier family 22 member 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Solute carrier family 22 member 2 -Log(IC50) M model	65	13	external test set validation 	0.06	0.899	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1743124	Solute carrier organic anion transporter family member 2B1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Solute carrier organic anion transporter family member 2B1 -Log(IC50) M model	15	3	external test set validation 	0.95	0.762	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1743126	Multidrug and toxin extrusion protein 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Multidrug and toxin extrusion protein 1 -Log(IC50) M model	50	10	external test set validation 	0.01	0.646	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1743127	Multidrug and toxin extrusion protein 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Multidrug and toxin extrusion protein 2 -Log(IC50) M model	40	8	external test set validation 	0.05	0.425	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1743128	Multidrug resistance-associated protein 4 -Log(IC50) M	Regression	-Log(IC50) M	Target	Multidrug resistance-associated protein 4 -Log(IC50) M model	15	3	external test set validation 	0.46	0.33	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1744525	Nicotinamide phosphoribosyltransferase -Log(IC50) M	Regression	-Log(IC50) M	Target	Nicotinamide phosphoribosyltransferase -Log(IC50) M model	355	71	external test set validation 	0.48	0.763	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1781	DNA topoisomerase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	DNA topoisomerase 1 -Log(IC50) M model	235	47	external test set validation 	0.53	0.717	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1782	Farnesyl pyrophosphate synthase -Log(IC50) M	Regression	-Log(IC50) M	Target	Farnesyl pyrophosphate synthase -Log(IC50) M model	160	32	external test set validation 	0.52	0.853	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1783	Vascular endothelial growth factor A -Log(IC50) M	Regression	-Log(IC50) M	Target	Vascular endothelial growth factor A -Log(IC50) M model	30	6	external test set validation 	0.94	0.727	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1784	Glucagon-like peptide 1 receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Glucagon-like peptide 1 receptor -Log(IC50) M model	80	16	external test set validation 	0.63	1.022	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1785	Endothelin B receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Endothelin B receptor -Log(IC50) M model	905	181	external test set validation 	0.82	0.501	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1787	3-oxo-5-alpha-steroid 4-dehydrogenase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	3-oxo-5-alpha-steroid 4-dehydrogenase 1 -Log(IC50) M model	230	46	external test set validation 	0.63	0.6	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1790	Vasopressin V2 receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Vasopressin V2 receptor -Log(IC50) M model	195	39	external test set validation 	0.54	0.713	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1792	Somatostatin receptor type 5 -Log(IC50) M	Regression	-Log(IC50) M	Target	Somatostatin receptor type 5 -Log(IC50) M model	395	79	external test set validation 	0.66	0.621	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1795093	Apoptotic protease-activating factor 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Apoptotic protease-activating factor 1 -Log(IC50) M model	450	90	external test set validation 	0.3	0.393	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1795094	Nuclear receptor subfamily 0 group B member 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Nuclear receptor subfamily 0 group B member 1 -Log(IC50) M model	10	2	external test set validation 	1	0.339	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1795098	Carboxy-terminal domain RNA polymerase II polypeptide A small phosphatase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Carboxy-terminal domain RNA polymerase II polypeptide A small phosphatase 1 -Log(IC50) M model	925	185	external test set validation 	0.15	0.3	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1795143	Poly(ADP-ribose) glycohydrolase -Log(IC50) M	Regression	-Log(IC50) M	Target	Poly(ADP-ribose) glycohydrolase -Log(IC50) M model	10	2	external test set validation 	1	0.47	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1795148	"Arginase-2, mitochondrial -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Arginase-2, mitochondrial -Log(IC50) M model"	45	9	external test set validation 	0.75	0.589	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1795166	Sterol regulatory element-binding protein 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Sterol regulatory element-binding protein 2 -Log(IC50) M model	30	6	external test set validation 	0.55	0.291	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1795168	CDGSH iron-sulfur domain-containing protein 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	CDGSH iron-sulfur domain-containing protein 1 -Log(IC50) M model	20	4	external test set validation 	0.83	0.234	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1795171	Potassium/sodium hyperpolarization-activated cyclic nucleotide-gated channel 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Potassium/sodium hyperpolarization-activated cyclic nucleotide-gated channel 1 -Log(IC50) M model	15	3	external test set validation 	0.99	0.202	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1795176	N-lysine methyltransferase KMT5A -Log(IC50) M	Regression	-Log(IC50) M	Target	N-lysine methyltransferase KMT5A -Log(IC50) M model	35	7	external test set validation 	0.73	0.739	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1795185	Bromodomain testis-specific protein -Log(IC50) M	Regression	-Log(IC50) M	Target	Bromodomain testis-specific protein -Log(IC50) M model	20	4	external test set validation 	0.01	0.447	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1795186	Bromodomain-containing protein 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Bromodomain-containing protein 3 -Log(IC50) M model	250	50	external test set validation 	0.73	0.433	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1798	Cysteinyl leukotriene receptor 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Cysteinyl leukotriene receptor 1 -Log(IC50) M model	195	39	external test set validation 	0.57	0.792	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1800	Corticotropin-releasing factor receptor 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Corticotropin-releasing factor receptor 1 -Log(IC50) M model	545	109	external test set validation 	0.41	0.478	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1801	Plasminogen -Log(IC50) M	Regression	-Log(IC50) M	Target	Plasminogen -Log(IC50) M model	375	75	external test set validation 	0.44	0.815	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1803	Integrin alpha-L|Integrin alpha-L -Log(IC50) M	Regression	-Log(IC50) M	Target	Integrin alpha-L|Integrin alpha-L -Log(IC50) M model	105	21	external test set validation 	0.63	0.737	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1804	Somatostatin receptor type 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Somatostatin receptor type 2 -Log(IC50) M model	230	46	external test set validation 	0.48	0.696	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1806	DNA topoisomerase 2-alpha -Log(IC50) M	Regression	-Log(IC50) M	Target	DNA topoisomerase 2-alpha -Log(IC50) M model	170	34	external test set validation 	0.58	0.519	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1808	Angiotensin-converting enzyme -Log(IC50) M	Regression	-Log(IC50) M	Target	Angiotensin-converting enzyme -Log(IC50) M model	440	88	external test set validation 	0.59	0.976	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1811	Prostaglandin E2 receptor EP1 subtype -Log(IC50) M	Regression	-Log(IC50) M	Target	Prostaglandin E2 receptor EP1 subtype -Log(IC50) M model	415	83	external test set validation 	0.6	0.604	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1821	Muscarinic acetylcholine receptor M4 -Log(IC50) M	Regression	-Log(IC50) M	Target	Muscarinic acetylcholine receptor M4 -Log(IC50) M model	150	30	external test set validation 	0.38	0.801	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1822	Inosine-5'-monophosphate dehydrogenase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Inosine-5'-monophosphate dehydrogenase 1 -Log(IC50) M model	50	10	external test set validation 	0.59	0.684	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1824	Receptor tyrosine-protein kinase erbB-2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Receptor tyrosine-protein kinase erbB-2 -Log(IC50) M model	1610	322	external test set validation 	0.7	0.623	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1825	Tumor necrosis factor -Log(IC50) M	Regression	-Log(IC50) M	Target	Tumor necrosis factor -Log(IC50) M model	485	97	external test set validation 	0.64	0.511	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1827	"cGMP-specific 3',5'-cyclic phosphodiesterase -Log(IC50) M"	Regression	-Log(IC50) M	Target	"cGMP-specific 3',5'-cyclic phosphodiesterase -Log(IC50) M model"	1400	280	external test set validation 	0.84	0.669	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1828	DNA polymerase alpha catalytic subunit -Log(IC50) M	Regression	-Log(IC50) M	Target	DNA polymerase alpha catalytic subunit -Log(IC50) M model	20	4	external test set validation 	0.06	0.715	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1829	Histone deacetylase 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Histone deacetylase 3 -Log(IC50) M model	730	146	external test set validation 	0.68	0.671	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1833	5-hydroxytryptamine receptor 2B -Log(IC50) M	Regression	-Log(IC50) M	Target	5-hydroxytryptamine receptor 2B -Log(IC50) M model	200	40	external test set validation 	0.38	0.901	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1835	Thromboxane-A synthase -Log(IC50) M	Regression	-Log(IC50) M	Target	Thromboxane-A synthase -Log(IC50) M model	820	164	external test set validation 	0.6	0.702	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1836	Prostaglandin E2 receptor EP4 subtype -Log(IC50) M	Regression	-Log(IC50) M	Target	Prostaglandin E2 receptor EP4 subtype -Log(IC50) M model	80	16	external test set validation 	0.86	0.689	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1841	Tyrosine-protein kinase Fyn -Log(IC50) M	Regression	-Log(IC50) M	Target	Tyrosine-protein kinase Fyn -Log(IC50) M model	205	41	external test set validation 	0.3	0.922	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1844	Macrophage colony-stimulating factor 1 receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Macrophage colony-stimulating factor 1 receptor -Log(IC50) M model	820	164	external test set validation 	0.74	0.581	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1845	Sodium channel protein type 1 subunit alpha -Log(IC50) M	Regression	-Log(IC50) M	Target	Sodium channel protein type 1 subunit alpha -Log(IC50) M model	40	8	external test set validation 	0.45	0.615	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1853	Somatostatin receptor type 4 -Log(IC50) M	Regression	-Log(IC50) M	Target	Somatostatin receptor type 4 -Log(IC50) M model	210	42	external test set validation 	0.65	0.646	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1855	Gonadotropin-releasing hormone receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Gonadotropin-releasing hormone receptor -Log(IC50) M model	580	116	external test set validation 	0.69	0.597	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1856	3-oxo-5-alpha-steroid 4-dehydrogenase 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	3-oxo-5-alpha-steroid 4-dehydrogenase 2 -Log(IC50) M model	255	51	external test set validation 	0.79	0.637	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1859	Voltage-dependent T-type calcium channel subunit alpha-1H -Log(IC50) M	Regression	-Log(IC50) M	Target	Voltage-dependent T-type calcium channel subunit alpha-1H -Log(IC50) M model	60	12	external test set validation 	0.44	0.376	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1860	Thyroid hormone receptor alpha -Log(IC50) M	Regression	-Log(IC50) M	Target	Thyroid hormone receptor alpha -Log(IC50) M model	250	50	external test set validation 	0.64	0.63	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1862	Tyrosine-protein kinase ABL1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Tyrosine-protein kinase ABL1 -Log(IC50) M model	790	158	external test set validation 	0.64	0.747	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1865	Histone deacetylase 6 -Log(IC50) M	Regression	-Log(IC50) M	Target	Histone deacetylase 6 -Log(IC50) M model	1515	303	external test set validation 	0.69	0.637	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1867	Alpha-2A adrenergic receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Alpha-2A adrenergic receptor -Log(IC50) M model	225	45	external test set validation 	0.64	0.688	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1868	Vascular endothelial growth factor receptor 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Vascular endothelial growth factor receptor 1 -Log(IC50) M model	720	144	external test set validation 	0.77	0.617	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1871	Androgen receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Androgen receptor -Log(IC50) M model	1215	243	external test set validation 	0.63	0.612	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1873	Tissue-type plasminogen activator -Log(IC50) M	Regression	-Log(IC50) M	Target	Tissue-type plasminogen activator -Log(IC50) M model	135	27	external test set validation 	0.31	1.003	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1875	5-hydroxytryptamine receptor 4 -Log(IC50) M	Regression	-Log(IC50) M	Target	5-hydroxytryptamine receptor 4 -Log(IC50) M model	65	13	external test set validation 	0.04	0.965	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1878	Extracellular calcium-sensing receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Extracellular calcium-sensing receptor -Log(IC50) M model	350	70	external test set validation 	0.56	0.721	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1881	Prostaglandin E2 receptor EP2 subtype -Log(IC50) M	Regression	-Log(IC50) M	Target	Prostaglandin E2 receptor EP2 subtype -Log(IC50) M model	85	17	external test set validation 	0.57	0.602	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1889	Vasopressin V1a receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Vasopressin V1a receptor -Log(IC50) M model	190	38	external test set validation 	0.53	0.73	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1892	Glutamate carboxypeptidase 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Glutamate carboxypeptidase 2 -Log(IC50) M model	140	28	external test set validation 	0.33	0.991	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1898	5-hydroxytryptamine receptor 1B -Log(IC50) M	Regression	-Log(IC50) M	Target	5-hydroxytryptamine receptor 1B -Log(IC50) M model	250	50	external test set validation 	0.68	0.507	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1899	5-hydroxytryptamine receptor 3A -Log(IC50) M	Regression	-Log(IC50) M	Target	5-hydroxytryptamine receptor 3A -Log(IC50) M model	65	13	external test set validation 	0.31	0.968	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1900	Aldose reductase -Log(IC50) M	Regression	-Log(IC50) M	Target	Aldose reductase -Log(IC50) M model	685	137	external test set validation 	0.72	0.669	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1901	Cholecystokinin receptor type A -Log(IC50) M	Regression	-Log(IC50) M	Target	Cholecystokinin receptor type A -Log(IC50) M model	255	51	external test set validation 	0.69	0.747	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1902	Peptidyl-prolyl cis-trans isomerase FKBP1A -Log(IC50) M	Regression	-Log(IC50) M	Target	Peptidyl-prolyl cis-trans isomerase FKBP1A -Log(IC50) M model	160	32	external test set validation 	0.54	0.741	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1903	Sodium- and chloride-dependent GABA transporter 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Sodium- and chloride-dependent GABA transporter 1 -Log(IC50) M model	55	11	external test set validation 	0.12	0.909	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1904	"Glutamate receptor ionotropic, NMDA 2B -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Glutamate receptor ionotropic, NMDA 2B -Log(IC50) M model"	95	19	external test set validation 	0.58	0.705	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1906	RAF proto-oncogene serine/threonine-protein kinase -Log(IC50) M	Regression	-Log(IC50) M	Target	RAF proto-oncogene serine/threonine-protein kinase -Log(IC50) M model	480	96	external test set validation 	0.61	0.611	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1907	Aminopeptidase N -Log(IC50) M	Regression	-Log(IC50) M	Target	Aminopeptidase N -Log(IC50) M model	225	45	external test set validation 	0.45	0.564	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1907589	Neuronal acetylcholine receptor subunit beta-2|Neuronal acetylcholine receptor subunit alpha-4 -Log(IC50) M	Regression	-Log(IC50) M	Target	Neuronal acetylcholine receptor subunit beta-2|Neuronal acetylcholine receptor subunit alpha-4 -Log(IC50) M model	200	40	external test set validation 	0.84	0.568	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1907591	Neuronal acetylcholine receptor subunit alpha-4|Neuronal acetylcholine receptor subunit beta-4 -Log(IC50) M	Regression	-Log(IC50) M	Target	Neuronal acetylcholine receptor subunit alpha-4|Neuronal acetylcholine receptor subunit beta-4 -Log(IC50) M model	15	3	external test set validation 	0.52	0.413	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1907594	Neuronal acetylcholine receptor subunit beta-4|Neuronal acetylcholine receptor subunit alpha-3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Neuronal acetylcholine receptor subunit beta-4|Neuronal acetylcholine receptor subunit alpha-3 -Log(IC50) M model	135	27	external test set validation 	0.89	0.664	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1907598	Integrin beta-3|Integrin alpha-V -Log(IC50) M	Regression	-Log(IC50) M	Target	Integrin beta-3|Integrin alpha-V -Log(IC50) M model	530	106	external test set validation 	0.6	0.955	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1907599	Integrin alpha-4|Integrin beta-1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Integrin alpha-4|Integrin beta-1 -Log(IC50) M model	405	81	external test set validation 	0.83	0.711	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1907600	Cyclin-dependent-like kinase 5|Cyclin-dependent kinase 5 activator 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Cyclin-dependent-like kinase 5|Cyclin-dependent kinase 5 activator 1 -Log(IC50) M model	595	119	external test set validation 	0.52	0.611	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1907601	Cyclin-dependent kinase 4|G1/S-specific cyclin-D1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Cyclin-dependent kinase 4|G1/S-specific cyclin-D1 -Log(IC50) M model	230	46	external test set validation 	0.89	0.503	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1907602	G2/mitotic-specific cyclin-B1|Cyclin-dependent kinase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	G2/mitotic-specific cyclin-B1|Cyclin-dependent kinase 1 -Log(IC50) M model	120	24	external test set validation 	0.17	0.937	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1907603	"Glutamate receptor ionotropic, NMDA 2B|Glutamate receptor ionotropic, NMDA 1 -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Glutamate receptor ionotropic, NMDA 2B|Glutamate receptor ionotropic, NMDA 1 -Log(IC50) M model"	100	20	external test set validation 	0.23	1.106	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1907604	"Glutamate receptor ionotropic, NMDA 1|Glutamate receptor ionotropic, NMDA 2A -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Glutamate receptor ionotropic, NMDA 1|Glutamate receptor ionotropic, NMDA 2A -Log(IC50) M model"	50	10	external test set validation 	0.39	0.915	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1907605	Cyclin-dependent kinase 2|G1/S-specific cyclin-E1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Cyclin-dependent kinase 2|G1/S-specific cyclin-E1 -Log(IC50) M model	300	60	external test set validation 	0.65	0.648	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1908	"Cytochrome P450 11B1, mitochondrial -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Cytochrome P450 11B1, mitochondrial -Log(IC50) M model"	725	145	external test set validation 	0.65	0.582	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1913	Platelet-derived growth factor receptor beta -Log(IC50) M	Regression	-Log(IC50) M	Target	Platelet-derived growth factor receptor beta -Log(IC50) M model	890	178	external test set validation 	0.73	0.654	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1914	Cholinesterase -Log(IC50) M	Regression	-Log(IC50) M	Target	Cholinesterase -Log(IC50) M model	1510	302	external test set validation 	0.78	0.65	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1915	Tubulin beta-1 chain -Log(IC50) M	Regression	-Log(IC50) M	Target	Tubulin beta-1 chain -Log(IC50) M model	70	14	external test set validation 	0.06	0.859	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1916	Alpha-2C adrenergic receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Alpha-2C adrenergic receptor -Log(IC50) M model	115	23	external test set validation 	0.58	0.765	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1917	Somatostatin receptor type 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Somatostatin receptor type 1 -Log(IC50) M model	200	40	external test set validation 	0.58	0.528	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1918	"Glutamate receptor ionotropic, kainate 1 -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Glutamate receptor ionotropic, kainate 1 -Log(IC50) M model"	55	11	external test set validation 	0.44	0.428	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1919	Voltage-dependent calcium channel subunit alpha-2/delta-1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Voltage-dependent calcium channel subunit alpha-2/delta-1 -Log(IC50) M model	165	33	external test set validation 	0.66	0.484	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1921	Vasopressin V1b receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Vasopressin V1b receptor -Log(IC50) M model	135	27	external test set validation 	0.73	0.476	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1929	Xanthine dehydrogenase/oxidase -Log(IC50) M	Regression	-Log(IC50) M	Target	Xanthine dehydrogenase/oxidase -Log(IC50) M model	210	42	external test set validation 	0.72	0.578	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1932895	Natural resistance-associated macrophage protein 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Natural resistance-associated macrophage protein 2 -Log(IC50) M model	45	9	external test set validation 	0.32	0.268	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1935	"Aldehyde dehydrogenase, mitochondrial -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Aldehyde dehydrogenase, mitochondrial -Log(IC50) M model"	130	26	external test set validation 	0.61	0.583	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1936	Mast/stem cell growth factor receptor Kit -Log(IC50) M	Regression	-Log(IC50) M	Target	Mast/stem cell growth factor receptor Kit -Log(IC50) M model	780	156	external test set validation 	0.6	0.674	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1937	Histone deacetylase 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Histone deacetylase 2 -Log(IC50) M model	825	165	external test set validation 	0.73	0.568	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1938209	Lysine-specific demethylase 3A -Log(IC50) M	Regression	-Log(IC50) M	Target	Lysine-specific demethylase 3A -Log(IC50) M model	25	5	external test set validation 	0.09	0.576	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1938210	Lysine-specific demethylase 2A|Lysine-specific demethylase 2A -Log(IC50) M	Regression	-Log(IC50) M	Target	Lysine-specific demethylase 2A|Lysine-specific demethylase 2A -Log(IC50) M model	60	12	external test set validation 	0.41	0.763	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1938211	Lysine-specific demethylase 6B -Log(IC50) M	Regression	-Log(IC50) M	Target	Lysine-specific demethylase 6B -Log(IC50) M model	60	12	external test set validation 	0.75	0.458	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1938212	Histone lysine demethylase PHF8|Histone lysine demethylase PHF8 -Log(IC50) M	Regression	-Log(IC50) M	Target	Histone lysine demethylase PHF8|Histone lysine demethylase PHF8 -Log(IC50) M model	35	7	external test set validation 	0.13	0.678	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1940	Voltage-dependent L-type calcium channel subunit alpha-1C -Log(IC50) M	Regression	-Log(IC50) M	Target	Voltage-dependent L-type calcium channel subunit alpha-1C -Log(IC50) M model	110	22	external test set validation 	0.72	0.69	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1941	Histamine H2 receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Histamine H2 receptor -Log(IC50) M model	115	23	external test set validation 	0.01	0.505	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1942	Alpha-2B adrenergic receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Alpha-2B adrenergic receptor -Log(IC50) M model	120	24	external test set validation 	0.6	0.766	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1943	"HLA class II histocompatibility antigen, DRB1-1 beta chain -Log(IC50) M"	Regression	-Log(IC50) M	Target	"HLA class II histocompatibility antigen, DRB1-1 beta chain -Log(IC50) M model"	40	8	external test set validation 	0.45	1.14	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1944	Neprilysin -Log(IC50) M	Regression	-Log(IC50) M	Target	Neprilysin -Log(IC50) M model	305	61	external test set validation 	0.68	0.933	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1945	Melatonin receptor type 1A -Log(IC50) M	Regression	-Log(IC50) M	Target	Melatonin receptor type 1A -Log(IC50) M model	170	34	external test set validation 	0.66	0.962	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1946	Melatonin receptor type 1B -Log(IC50) M	Regression	-Log(IC50) M	Target	Melatonin receptor type 1B -Log(IC50) M model	75	15	external test set validation 	0.69	0.758	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1947	Thyroid hormone receptor beta -Log(IC50) M	Regression	-Log(IC50) M	Target	Thyroid hormone receptor beta -Log(IC50) M model	330	66	external test set validation 	0.7	0.699	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1949	Peptidyl-prolyl cis-trans isomerase A -Log(IC50) M	Regression	-Log(IC50) M	Target	Peptidyl-prolyl cis-trans isomerase A -Log(IC50) M model	160	32	external test set validation 	0.68	0.876	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1951	Amine oxidase [flavin-containing] A -Log(IC50) M	Regression	-Log(IC50) M	Target	Amine oxidase [flavin-containing] A -Log(IC50) M model	1250	250	external test set validation 	0.65	0.646	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1952	Thymidylate synthase -Log(IC50) M	Regression	-Log(IC50) M	Target	Thymidylate synthase -Log(IC50) M model	305	61	external test set validation 	0.62	0.704	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1954	Ribonucleoside-diphosphate reductase subunit M2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Ribonucleoside-diphosphate reductase subunit M2 -Log(IC50) M model	10	2	external test set validation 	1	0.382	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1955	Vascular endothelial growth factor receptor 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Vascular endothelial growth factor receptor 3 -Log(IC50) M model	215	43	external test set validation 	0.53	0.917	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1957	Insulin-like growth factor 1 receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Insulin-like growth factor 1 receptor -Log(IC50) M model	1455	291	external test set validation 	0.81	0.541	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1966	"Dihydroorotate dehydrogenase (quinone), mitochondrial -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Dihydroorotate dehydrogenase (quinone), mitochondrial -Log(IC50) M model"	465	93	external test set validation 	0.77	0.602	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1968	Epoxide hydrolase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Epoxide hydrolase 1 -Log(IC50) M model	265	53	external test set validation 	0.74	0.655	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1973	Tyrosinase -Log(IC50) M	Regression	-Log(IC50) M	Target	Tyrosinase -Log(IC50) M model	60	12	external test set validation 	0.75	0.447	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1974	Receptor-type tyrosine-protein kinase FLT3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Receptor-type tyrosine-protein kinase FLT3 -Log(IC50) M model	1090	218	external test set validation 	0.64	0.675	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1977	Vitamin D3 receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Vitamin D3 receptor -Log(IC50) M model	210	42	external test set validation 	0.94	0.395	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1978	Aromatase -Log(IC50) M	Regression	-Log(IC50) M	Target	Aromatase -Log(IC50) M model	1345	269	external test set validation 	0.69	0.729	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1980	Sodium channel protein type 5 subunit alpha -Log(IC50) M	Regression	-Log(IC50) M	Target	Sodium channel protein type 5 subunit alpha -Log(IC50) M model	535	107	external test set validation 	0.68	0.543	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1981	Insulin receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Insulin receptor -Log(IC50) M model	555	111	external test set validation 	0.62	0.584	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1983	5-hydroxytryptamine receptor 1D -Log(IC50) M	Regression	-Log(IC50) M	Target	5-hydroxytryptamine receptor 1D -Log(IC50) M model	285	57	external test set validation 	0.8	0.488	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1985	Glucagon receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Glucagon receptor -Log(IC50) M model	590	118	external test set validation 	0.72	0.536	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1987	Prostaglandin F2-alpha receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Prostaglandin F2-alpha receptor -Log(IC50) M model	50	10	external test set validation 	0.83	0.575	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1991	Inhibitor of nuclear factor kappa-B kinase subunit beta -Log(IC50) M	Regression	-Log(IC50) M	Target	Inhibitor of nuclear factor kappa-B kinase subunit beta -Log(IC50) M model	775	155	external test set validation 	0.72	0.522	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1993	DNA (cytosine-5)-methyltransferase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	DNA (cytosine-5)-methyltransferase 1 -Log(IC50) M model	295	59	external test set validation 	0.26	0.462	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1994	Mineralocorticoid receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Mineralocorticoid receptor -Log(IC50) M model	665	133	external test set validation 	0.49	0.531	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1995	Prostacyclin receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Prostacyclin receptor -Log(IC50) M model	75	15	external test set validation 	0.38	0.55	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL1997	Equilibrative nucleoside transporter 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Equilibrative nucleoside transporter 1 -Log(IC50) M model	205	41	external test set validation 	0.58	0.811	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2000	Plasma kallikrein -Log(IC50) M	Regression	-Log(IC50) M	Target	Plasma kallikrein -Log(IC50) M model	90	18	external test set validation 	0.56	0.874	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2001	P2Y purinoceptor 12 -Log(IC50) M	Regression	-Log(IC50) M	Target	P2Y purinoceptor 12 -Log(IC50) M model	545	109	external test set validation 	0.77	0.497	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2002	Inosine-5'-monophosphate dehydrogenase 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Inosine-5'-monophosphate dehydrogenase 2 -Log(IC50) M model	425	85	external test set validation 	0.68	0.57	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2007	Platelet-derived growth factor receptor alpha -Log(IC50) M	Regression	-Log(IC50) M	Target	Platelet-derived growth factor receptor alpha -Log(IC50) M model	270	54	external test set validation 	0.41	0.862	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2007625	Isocitrate dehydrogenase [NADP] cytoplasmic -Log(IC50) M	Regression	-Log(IC50) M	Target	Isocitrate dehydrogenase [NADP] cytoplasmic -Log(IC50) M model	355	71	external test set validation 	0.55	0.507	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2007628	Tyrosine-protein phosphatase non-receptor type 5 -Log(IC50) M	Regression	-Log(IC50) M	Target	Tyrosine-protein phosphatase non-receptor type 5 -Log(IC50) M model	120	24	external test set validation 	0.03	0.336	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2007629	26S proteasome non-ATPase regulatory subunit 14 -Log(IC50) M	Regression	-Log(IC50) M	Target	26S proteasome non-ATPase regulatory subunit 14 -Log(IC50) M model	400	80	external test set validation 	0.1	0.321	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2009	Glutamate receptor 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Glutamate receptor 1 -Log(IC50) M model	15	3	external test set validation 	0	1.638	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2010631	Atypical chemokine receptor 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Atypical chemokine receptor 3 -Log(IC50) M model	30	6	external test set validation 	0.5	0.287	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2014	Nociceptin receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Nociceptin receptor -Log(IC50) M model	315	63	external test set validation 	0.62	0.726	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2015	"Glutamate receptor ionotropic, NMDA 1 -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Glutamate receptor ionotropic, NMDA 1 -Log(IC50) M model"	30	6	external test set validation 	0.64	0.615	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL202	Dihydrofolate reductase -Log(IC50) M	Regression	-Log(IC50) M	Target	Dihydrofolate reductase -Log(IC50) M model	585	117	external test set validation 	0.6	0.779	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2021753	Kinesin-like protein KIF20A -Log(IC50) M	Regression	-Log(IC50) M	Target	Kinesin-like protein KIF20A -Log(IC50) M model	25	5	external test set validation 	0.02	0.717	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2023	Catechol O-methyltransferase -Log(IC50) M	Regression	-Log(IC50) M	Target	Catechol O-methyltransferase -Log(IC50) M model	45	9	external test set validation 	0.33	0.619	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2027	Niemann-Pick C1-like protein 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Niemann-Pick C1-like protein 1 -Log(IC50) M model	95	19	external test set validation 	0.59	0.605	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2028	Somatostatin receptor type 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Somatostatin receptor type 3 -Log(IC50) M model	350	70	external test set validation 	0.69	0.48	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2029198	Rap guanine nucleotide exchange factor 4 -Log(IC50) M	Regression	-Log(IC50) M	Target	Rap guanine nucleotide exchange factor 4 -Log(IC50) M model	70	14	external test set validation 	0.29	0.402	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL203	Epidermal growth factor receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Epidermal growth factor receptor -Log(IC50) M model	4670	934	external test set validation 	0.76	0.674	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2034	Glucocorticoid receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Glucocorticoid receptor -Log(IC50) M model	1135	227	external test set validation 	0.68	0.58	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2034807	DNA repair protein RAD51 homolog 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	DNA repair protein RAD51 homolog 1 -Log(IC50) M model	60	12	external test set validation 	0.4	0.346	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2035	Muscarinic acetylcholine receptor M5 -Log(IC50) M	Regression	-Log(IC50) M	Target	Muscarinic acetylcholine receptor M5 -Log(IC50) M model	205	41	external test set validation 	0.46	0.667	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2039	Amine oxidase [flavin-containing] B -Log(IC50) M	Regression	-Log(IC50) M	Target	Amine oxidase [flavin-containing] B -Log(IC50) M model	1785	357	external test set validation 	0.68	0.769	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL204	Prothrombin -Log(IC50) M	Regression	-Log(IC50) M	Target	Prothrombin -Log(IC50) M model	1810	362	external test set validation 	0.7	0.733	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2041	Proto-oncogene tyrosine-protein kinase receptor Ret -Log(IC50) M	Regression	-Log(IC50) M	Target	Proto-oncogene tyrosine-protein kinase receptor Ret -Log(IC50) M model	455	91	external test set validation 	0.69	0.755	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2047	Bile acid receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Bile acid receptor -Log(IC50) M model	140	28	external test set validation 	0.68	0.633	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2049	Oxytocin receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Oxytocin receptor -Log(IC50) M model	35	7	external test set validation 	0.75	0.617	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL205	Carbonic anhydrase 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Carbonic anhydrase 2 -Log(IC50) M model	545	109	external test set validation 	0.88	0.569	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2052031	Peptidyl-prolyl cis-trans isomerase FKBP5 -Log(IC50) M	Regression	-Log(IC50) M	Target	Peptidyl-prolyl cis-trans isomerase FKBP5 -Log(IC50) M model	40	8	external test set validation 	0.77	0.328	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2056	D(1A) dopamine receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	D(1A) dopamine receptor -Log(IC50) M model	95	19	external test set validation 	0.31	0.676	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL206	Estrogen receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Estrogen receptor -Log(IC50) M model	1520	304	external test set validation 	0.73	0.708	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2060	Lipoprotein lipase -Log(IC50) M	Regression	-Log(IC50) M	Target	Lipoprotein lipase -Log(IC50) M model	15	3	external test set validation 	0.42	1.016	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2061	Retinoic acid receptor RXR-alpha -Log(IC50) M	Regression	-Log(IC50) M	Target	Retinoic acid receptor RXR-alpha -Log(IC50) M model	130	26	external test set validation 	0.81	0.432	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2063	Ceramide glucosyltransferase -Log(IC50) M	Regression	-Log(IC50) M	Target	Ceramide glucosyltransferase -Log(IC50) M model	100	20	external test set validation 	0.7	0.692	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2068	Ephrin type-A receptor 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Ephrin type-A receptor 2 -Log(IC50) M model	75	15	external test set validation 	0.29	0.971	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2069	Thromboxane A2 receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Thromboxane A2 receptor -Log(IC50) M model	410	82	external test set validation 	0.43	0.784	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2072	Sodium channel protein type 4 subunit alpha -Log(IC50) M	Regression	-Log(IC50) M	Target	Sodium channel protein type 4 subunit alpha -Log(IC50) M model	90	18	external test set validation 	0.55	0.63	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2073	Tyrosine-protein kinase Yes -Log(IC50) M	Regression	-Log(IC50) M	Target	Tyrosine-protein kinase Yes -Log(IC50) M model	80	16	external test set validation 	0.73	0.94	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2073663	Monocarboxylate transporter 4 -Log(IC50) M	Regression	-Log(IC50) M	Target	Monocarboxylate transporter 4 -Log(IC50) M model	5	1	external test set validation 	nan	0.513	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2073673	Solute carrier family 22 member 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Solute carrier family 22 member 3 -Log(IC50) M model	10	2	external test set validation 	1	1.709	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2074	"Maltase-glucoamylase, intestinal -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Maltase-glucoamylase, intestinal -Log(IC50) M model"	145	29	external test set validation 	0.68	0.512	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2075	Peptidyl-prolyl cis-trans isomerase B -Log(IC50) M	Regression	-Log(IC50) M	Target	Peptidyl-prolyl cis-trans isomerase B -Log(IC50) M model	35	7	external test set validation 	0.88	0.598	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2079849	Serine protease hepsin -Log(IC50) M	Regression	-Log(IC50) M	Target	Serine protease hepsin -Log(IC50) M model	15	3	external test set validation 	0.15	0.564	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL208	Progesterone receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Progesterone receptor -Log(IC50) M model	1250	250	external test set validation 	0.73	0.565	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2083	"Fatty acid-binding protein, adipocyte -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Fatty acid-binding protein, adipocyte -Log(IC50) M model"	75	15	external test set validation 	0.51	0.467	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2085	Macrophage migration inhibitory factor -Log(IC50) M	Regression	-Log(IC50) M	Target	Macrophage migration inhibitory factor -Log(IC50) M model	185	37	external test set validation 	0.55	0.772	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL209	Trypsin-1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Trypsin-1 -Log(IC50) M model	340	68	external test set validation 	0.76	0.783	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2093869	Integrin beta-3|Integrin alpha-IIb -Log(IC50) M	Regression	-Log(IC50) M	Target	Integrin beta-3|Integrin alpha-IIb -Log(IC50) M model	765	153	external test set validation 	0.72	0.778	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2094	P2X purinoceptor 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	P2X purinoceptor 1 -Log(IC50) M model	15	3	external test set validation 	0.33	0.337	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2094108	Protein farnesyltransferase subunit beta|Protein farnesyltransferase/geranylgeranyltransferase type-1 subunit alpha -Log(IC50) M	Regression	-Log(IC50) M	Target	Protein farnesyltransferase subunit beta|Protein farnesyltransferase/geranylgeranyltransferase type-1 subunit alpha -Log(IC50) M model	630	126	external test set validation 	0.75	0.858	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2094126	G1/S-specific cyclin-E1|Cyclin-dependent kinase 2|G1/S-specific cyclin-E2 -Log(IC50) M	Regression	-Log(IC50) M	Target	G1/S-specific cyclin-E1|Cyclin-dependent kinase 2|G1/S-specific cyclin-E2 -Log(IC50) M model	115	23	external test set validation 	0.67	0.522	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2094127	G2/mitotic-specific cyclin-B2|G2/mitotic-specific cyclin-B1|Cyclin-dependent kinase 1|G2/mitotic-specific cyclin-B3 -Log(IC50) M	Regression	-Log(IC50) M	Target	G2/mitotic-specific cyclin-B2|G2/mitotic-specific cyclin-B1|Cyclin-dependent kinase 1|G2/mitotic-specific cyclin-B3 -Log(IC50) M model	140	28	external test set validation 	0.66	1.012	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2094128	Cyclin-A2|Cyclin-A1|Cyclin-dependent kinase 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Cyclin-A2|Cyclin-A1|Cyclin-dependent kinase 2 -Log(IC50) M model	380	76	external test set validation 	0.87	0.541	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2094135	Gamma-secretase subunit APH-1B|Presenilin-1|Gamma-secretase subunit APH-1A|Nicastrin|Gamma-secretase subunit PEN-2|Presenilin-2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Gamma-secretase subunit APH-1B|Presenilin-1|Gamma-secretase subunit APH-1A|Nicastrin|Gamma-secretase subunit PEN-2|Presenilin-2 -Log(IC50) M model	1500	300	external test set validation 	0.78	0.586	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2095164	Protein farnesyltransferase/geranylgeranyltransferase type-1 subunit alpha|Geranylgeranyl transferase type-1 subunit beta -Log(IC50) M	Regression	-Log(IC50) M	Target	Protein farnesyltransferase/geranylgeranyltransferase type-1 subunit alpha|Geranylgeranyl transferase type-1 subunit beta -Log(IC50) M model	200	40	external test set validation 	0.61	0.717	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2095172	Gamma-aminobutyric acid receptor subunit gamma-2|Gamma-aminobutyric acid receptor subunit beta-2|Gamma-aminobutyric acid receptor subunit alpha-1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Gamma-aminobutyric acid receptor subunit gamma-2|Gamma-aminobutyric acid receptor subunit beta-2|Gamma-aminobutyric acid receptor subunit alpha-1 -Log(IC50) M model	15	3	external test set validation 	0.68	0.583	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2095174	SUMO-activating enzyme subunit 2|SUMO-activating enzyme subunit 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	SUMO-activating enzyme subunit 2|SUMO-activating enzyme subunit 1 -Log(IC50) M model	295	59	external test set validation 	0.15	0.364	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2095184	Integrin alpha-4|Integrin beta-7 -Log(IC50) M	Regression	-Log(IC50) M	Target	Integrin alpha-4|Integrin beta-7 -Log(IC50) M model	175	35	external test set validation 	0.25	0.771	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2095189	Platelet-derived growth factor receptor beta|Platelet-derived growth factor receptor alpha -Log(IC50) M	Regression	-Log(IC50) M	Target	Platelet-derived growth factor receptor beta|Platelet-derived growth factor receptor alpha -Log(IC50) M model	145	29	external test set validation 	0.44	0.72	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2095194	Coagulation factor VII|Tissue factor|Tissue factor -Log(IC50) M	Regression	-Log(IC50) M	Target	Coagulation factor VII|Tissue factor|Tissue factor -Log(IC50) M model	85	17	external test set validation 	0.6	0.513	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2095202	"Troponin I, cardiac muscle|Troponin T, cardiac muscle|Troponin C, slow skeletal and cardiac muscles -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Troponin I, cardiac muscle|Troponin T, cardiac muscle|Troponin C, slow skeletal and cardiac muscles -Log(IC50) M model"	50	10	external test set validation 	0.46	0.43	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2095226	Integrin alpha-5|Integrin beta-1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Integrin alpha-5|Integrin beta-1 -Log(IC50) M model	130	26	external test set validation 	0.54	0.947	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2096618	Tyrosine-protein kinase ABL1|Breakpoint cluster region protein|Breakpoint cluster region protein -Log(IC50) M	Regression	-Log(IC50) M	Target	Tyrosine-protein kinase ABL1|Breakpoint cluster region protein|Breakpoint cluster region protein -Log(IC50) M model	240	48	external test set validation 	0.42	1.011	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2096661	Integrin beta-2|Intercellular adhesion molecule 1|Integrin alpha-L|Integrin alpha-L -Log(IC50) M	Regression	-Log(IC50) M	Target	Integrin beta-2|Intercellular adhesion molecule 1|Integrin alpha-L|Integrin alpha-L -Log(IC50) M model	145	29	external test set validation 	0.61	0.665	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2096675	Integrin alpha-V|Integrin beta-5 -Log(IC50) M	Regression	-Log(IC50) M	Target	Integrin alpha-V|Integrin beta-5 -Log(IC50) M model	120	24	external test set validation 	0.59	0.848	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2096972	ATP-sensitive inward rectifier potassium channel 11|ATP-binding cassette sub-family C member 8 -Log(IC50) M	Regression	-Log(IC50) M	Target	ATP-sensitive inward rectifier potassium channel 11|ATP-binding cassette sub-family C member 8 -Log(IC50) M model	5	1	external test set validation 	nan	3.81	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2097	Putative inactive group IIC secretory phospholipase A2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Putative inactive group IIC secretory phospholipase A2 -Log(IC50) M model	10	2	external test set validation 	1	1.095	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL210	Beta-2 adrenergic receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Beta-2 adrenergic receptor -Log(IC50) M model	385	77	external test set validation 	0.71	0.643	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2104	P2X purinoceptor 4 -Log(IC50) M	Regression	-Log(IC50) M	Target	P2X purinoceptor 4 -Log(IC50) M model	70	14	external test set validation 	0.27	0.55	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2107	C-X-C chemokine receptor type 4 -Log(IC50) M	Regression	-Log(IC50) M	Target	C-X-C chemokine receptor type 4 -Log(IC50) M model	185	37	external test set validation 	0.22	0.882	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL211	Muscarinic acetylcholine receptor M2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Muscarinic acetylcholine receptor M2 -Log(IC50) M model	405	81	external test set validation 	0.73	0.677	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2111328	Inhibitor of nuclear factor kappa-B kinase subunit alpha|Inhibitor of nuclear factor kappa-B kinase subunit beta|NF-kappa-B essential modulator -Log(IC50) M	Regression	-Log(IC50) M	Target	Inhibitor of nuclear factor kappa-B kinase subunit alpha|Inhibitor of nuclear factor kappa-B kinase subunit beta|NF-kappa-B essential modulator -Log(IC50) M model	20	4	external test set validation 	0.52	0.941	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2111363	Histone deacetylase 3|Nuclear receptor corepressor 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Histone deacetylase 3|Nuclear receptor corepressor 2 -Log(IC50) M model	80	16	external test set validation 	0.73	0.465	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2111377	Cell division cycle 7-related protein kinase|Protein DBF4 homolog A -Log(IC50) M	Regression	-Log(IC50) M	Target	Cell division cycle 7-related protein kinase|Protein DBF4 homolog A -Log(IC50) M model	65	13	external test set validation 	0.63	0.871	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2111389	Cyclin-T1|Cyclin-dependent kinase 9 -Log(IC50) M	Regression	-Log(IC50) M	Target	Cyclin-T1|Cyclin-dependent kinase 9 -Log(IC50) M model	210	42	external test set validation 	0.63	0.574	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2111416	Integrin beta-6|Integrin alpha-V -Log(IC50) M	Regression	-Log(IC50) M	Target	Integrin beta-6|Integrin alpha-V -Log(IC50) M model	80	16	external test set validation 	0.24	0.8	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2111421	Transcription factor AP-1|Proto-oncogene c-Fos -Log(IC50) M	Regression	-Log(IC50) M	Target	Transcription factor AP-1|Proto-oncogene c-Fos -Log(IC50) M model	55	11	external test set validation 	0.52	0.564	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2111432	"Phosphatidylinositol 3-kinase regulatory subunit alpha|Phosphatidylinositol 4,5-bisphosphate 3-kinase catalytic subunit delta isoform -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Phosphatidylinositol 3-kinase regulatory subunit alpha|Phosphatidylinositol 4,5-bisphosphate 3-kinase catalytic subunit delta isoform -Log(IC50) M model"	100	20	external test set validation 	0.78	0.793	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2118	Amiloride-sensitive amine oxidase [copper-containing] -Log(IC50) M	Regression	-Log(IC50) M	Target	Amiloride-sensitive amine oxidase [copper-containing] -Log(IC50) M model	20	4	external test set validation 	0.42	0.651	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL212	Integrin alpha-IIb -Log(IC50) M	Regression	-Log(IC50) M	Target	Integrin alpha-IIb -Log(IC50) M model	35	7	external test set validation 	0.35	0.503	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2127	Hepatic triacylglycerol lipase -Log(IC50) M	Regression	-Log(IC50) M	Target	Hepatic triacylglycerol lipase -Log(IC50) M model	340	68	external test set validation 	0.51	0.61	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL213	Beta-1 adrenergic receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Beta-1 adrenergic receptor -Log(IC50) M model	440	88	external test set validation 	0.74	0.633	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2131	Indolethylamine N-methyltransferase -Log(IC50) M	Regression	-Log(IC50) M	Target	Indolethylamine N-methyltransferase -Log(IC50) M model	25	5	external test set validation 	0.13	0.71	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL214	5-hydroxytryptamine receptor 1A -Log(IC50) M	Regression	-Log(IC50) M	Target	5-hydroxytryptamine receptor 1A -Log(IC50) M model	450	90	external test set validation 	0.54	0.811	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2140	"Tryptophan 2,3-dioxygenase -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Tryptophan 2,3-dioxygenase -Log(IC50) M model"	15	3	external test set validation 	0.66	0.196	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2145	Kynurenine 3-monooxygenase -Log(IC50) M	Regression	-Log(IC50) M	Target	Kynurenine 3-monooxygenase -Log(IC50) M model	40	8	external test set validation 	0.13	1.172	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2146302	"Glutaminase kidney isoform, mitochondrial -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Glutaminase kidney isoform, mitochondrial -Log(IC50) M model"	505	101	external test set validation 	0.84	0.455	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2147	Serine/threonine-protein kinase pim-1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Serine/threonine-protein kinase pim-1 -Log(IC50) M model	1100	220	external test set validation 	0.83	0.565	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2148	Tyrosine-protein kinase JAK3|Tyrosine-protein kinase JAK3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Tyrosine-protein kinase JAK3|Tyrosine-protein kinase JAK3 -Log(IC50) M model	1645	329	external test set validation 	0.69	0.691	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL215	Arachidonate 5-lipoxygenase -Log(IC50) M	Regression	-Log(IC50) M	Target	Arachidonate 5-lipoxygenase -Log(IC50) M model	1415	283	external test set validation 	0.65	0.518	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2150837	ATPase family AAA domain-containing protein 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	ATPase family AAA domain-containing protein 2 -Log(IC50) M model	65	13	external test set validation 	0.69	0.482	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2157	Interleukin-8 -Log(IC50) M	Regression	-Log(IC50) M	Target	Interleukin-8 -Log(IC50) M model	20	4	external test set validation 	0.68	0.732	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2157850	Ubiquitin carboxyl-terminal hydrolase 7 -Log(IC50) M	Regression	-Log(IC50) M	Target	Ubiquitin carboxyl-terminal hydrolase 7 -Log(IC50) M model	20	4	external test set validation 	0.96	0.194	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL216	Muscarinic acetylcholine receptor M1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Muscarinic acetylcholine receptor M1 -Log(IC50) M model	535	107	external test set validation 	0.71	0.685	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2163175	Gamma-butyrobetaine dioxygenase -Log(IC50) M	Regression	-Log(IC50) M	Target	Gamma-butyrobetaine dioxygenase -Log(IC50) M model	40	8	external test set validation 	0.02	0.967	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2163176	Lysine-specific demethylase 5C|Lysine-specific demethylase 5C -Log(IC50) M	Regression	-Log(IC50) M	Target	Lysine-specific demethylase 5C|Lysine-specific demethylase 5C -Log(IC50) M model	70	14	external test set validation 	0.64	0.612	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2164	Serine/threonine-protein phosphatase PP1-alpha catalytic subunit -Log(IC50) M	Regression	-Log(IC50) M	Target	Serine/threonine-protein phosphatase PP1-alpha catalytic subunit -Log(IC50) M model	100	20	external test set validation 	0.07	0.505	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2167	GTPase HRas -Log(IC50) M	Regression	-Log(IC50) M	Target	GTPase HRas -Log(IC50) M model	55	11	external test set validation 	0.77	0.613	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2169716	N-lysine methyltransferase SMYD2 -Log(IC50) M	Regression	-Log(IC50) M	Target	N-lysine methyltransferase SMYD2 -Log(IC50) M model	45	9	external test set validation 	0.7	0.822	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2169736	Tyrosyl-DNA phosphodiesterase 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Tyrosyl-DNA phosphodiesterase 2 -Log(IC50) M model	60	12	external test set validation 	0.7	0.405	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL217	D(2) dopamine receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	D(2) dopamine receptor -Log(IC50) M model	405	81	external test set validation 	0.57	0.771	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2176771	Complement factor D -Log(IC50) M	Regression	-Log(IC50) M	Target	Complement factor D -Log(IC50) M model	715	143	external test set validation 	0.67	0.505	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2176846	Advanced glycosylation end product-specific receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Advanced glycosylation end product-specific receptor -Log(IC50) M model	20	4	external test set validation 	0.87	0.18	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2179	Glucosylceramidase -Log(IC50) M	Regression	-Log(IC50) M	Target	Glucosylceramidase -Log(IC50) M model	215	43	external test set validation 	0.43	0.802	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL218	Cannabinoid receptor 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Cannabinoid receptor 1 -Log(IC50) M model	1075	215	external test set validation 	0.77	0.64	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2182	5-hydroxytryptamine receptor 1E -Log(IC50) M	Regression	-Log(IC50) M	Target	5-hydroxytryptamine receptor 1E -Log(IC50) M model	15	3	external test set validation 	0.17	0.395	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2185	Aurora kinase B -Log(IC50) M	Regression	-Log(IC50) M	Target	Aurora kinase B -Log(IC50) M model	1190	238	external test set validation 	0.69	0.683	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2189110	Histone-lysine N-methyltransferase EZH2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Histone-lysine N-methyltransferase EZH2 -Log(IC50) M model	265	53	external test set validation 	0.89	0.485	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL219	D(4) dopamine receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	D(4) dopamine receptor -Log(IC50) M model	140	28	external test set validation 	0.51	0.795	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL220	Acetylcholinesterase -Log(IC50) M	Regression	-Log(IC50) M	Target	Acetylcholinesterase -Log(IC50) M model	2890	578	external test set validation 	0.74	0.703	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2200	Matrix metalloproteinase-16 -Log(IC50) M	Regression	-Log(IC50) M	Target	Matrix metalloproteinase-16 -Log(IC50) M model	70	14	external test set validation 	0.61	0.791	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2208	MAP kinase-activated protein kinase 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	MAP kinase-activated protein kinase 2 -Log(IC50) M model	760	152	external test set validation 	0.75	0.558	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL221	Prostaglandin G/H synthase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Prostaglandin G/H synthase 1 -Log(IC50) M model	1410	282	external test set validation 	0.48	0.685	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2216739	"Carnitine O-palmitoyltransferase 1, muscle isoform -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Carnitine O-palmitoyltransferase 1, muscle isoform -Log(IC50) M model"	180	36	external test set validation 	0.45	0.517	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2219	Tyrosine-protein phosphatase non-receptor type 7 -Log(IC50) M	Regression	-Log(IC50) M	Target	Tyrosine-protein phosphatase non-receptor type 7 -Log(IC50) M model	360	72	external test set validation 	0.18	0.591	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL222	Sodium-dependent noradrenaline transporter -Log(IC50) M	Regression	-Log(IC50) M	Target	Sodium-dependent noradrenaline transporter -Log(IC50) M model	1465	293	external test set validation 	0.7	0.6	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2225	Pro-cathepsin H -Log(IC50) M	Regression	-Log(IC50) M	Target	Pro-cathepsin H -Log(IC50) M model	15	3	external test set validation 	1	0.532	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL223	Alpha-1D adrenergic receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Alpha-1D adrenergic receptor -Log(IC50) M model	220	44	external test set validation 	0.29	0.875	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2231	Cytochrome P450 1A1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Cytochrome P450 1A1 -Log(IC50) M model	145	29	external test set validation 	0.74	0.496	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL224	5-hydroxytryptamine receptor 2A -Log(IC50) M	Regression	-Log(IC50) M	Target	5-hydroxytryptamine receptor 2A -Log(IC50) M model	750	150	external test set validation 	0.64	0.62	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2241	Glutathione S-transferase A2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Glutathione S-transferase A2 -Log(IC50) M model	25	5	external test set validation 	0.81	0.685	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2243	Fatty-acid amide hydrolase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Fatty-acid amide hydrolase 1 -Log(IC50) M model	890	178	external test set validation 	0.78	0.742	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2246	Alcohol dehydrogenase [NADP(+)] -Log(IC50) M	Regression	-Log(IC50) M	Target	Alcohol dehydrogenase [NADP(+)] -Log(IC50) M model	55	11	external test set validation 	0.05	0.454	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL225	5-hydroxytryptamine receptor 2C -Log(IC50) M	Regression	-Log(IC50) M	Target	5-hydroxytryptamine receptor 2C -Log(IC50) M model	765	153	external test set validation 	0.55	0.703	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2252	Dipeptidyl peptidase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Dipeptidyl peptidase 1 -Log(IC50) M model	605	121	external test set validation 	0.63	0.513	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL226	Adenosine receptor A1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Adenosine receptor A1 -Log(IC50) M model	240	48	external test set validation 	0.55	0.859	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2265	Liver carboxylesterase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Liver carboxylesterase 1 -Log(IC50) M model	95	19	external test set validation 	0.44	0.882	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL227	Type-1 angiotensin II receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Type-1 angiotensin II receptor -Log(IC50) M model	780	156	external test set validation 	0.88	0.575	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2273	Caspase-9 -Log(IC50) M	Regression	-Log(IC50) M	Target	Caspase-9 -Log(IC50) M model	10	2	external test set validation 	1	1.35	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2274	Sphingosine 1-phosphate receptor 5 -Log(IC50) M	Regression	-Log(IC50) M	Target	Sphingosine 1-phosphate receptor 5 -Log(IC50) M model	120	24	external test set validation 	0.23	0.654	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2275	Sorbitol dehydrogenase -Log(IC50) M	Regression	-Log(IC50) M	Target	Sorbitol dehydrogenase -Log(IC50) M model	70	14	external test set validation 	0.94	0.43	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2276	Mitogen-activated protein kinase 8 -Log(IC50) M	Regression	-Log(IC50) M	Target	Mitogen-activated protein kinase 8 -Log(IC50) M model	1285	257	external test set validation 	0.71	0.549	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL228	Sodium-dependent serotonin transporter -Log(IC50) M	Regression	-Log(IC50) M	Target	Sodium-dependent serotonin transporter -Log(IC50) M model	2145	429	external test set validation 	0.68	0.594	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2282	Glucose-6-phosphatase -Log(IC50) M	Regression	-Log(IC50) M	Target	Glucose-6-phosphatase -Log(IC50) M model	10	2	external test set validation 	1	0.975	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2284	Glyceraldehyde-3-phosphate dehydrogenase -Log(IC50) M	Regression	-Log(IC50) M	Target	Glyceraldehyde-3-phosphate dehydrogenase -Log(IC50) M model	100	20	external test set validation 	0.5	0.46	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2285	A disintegrin and metalloproteinase with thrombospondin motifs 5 -Log(IC50) M	Regression	-Log(IC50) M	Target	A disintegrin and metalloproteinase with thrombospondin motifs 5 -Log(IC50) M model	370	74	external test set validation 	0.76	0.439	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2288	Peptidyl-prolyl cis-trans isomerase NIMA-interacting 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Peptidyl-prolyl cis-trans isomerase NIMA-interacting 1 -Log(IC50) M model	130	26	external test set validation 	0.67	0.791	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL229	Alpha-1A adrenergic receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Alpha-1A adrenergic receptor -Log(IC50) M model	175	35	external test set validation 	0.45	0.796	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2292	Dual specificity tyrosine-phosphorylation-regulated kinase 1A -Log(IC50) M	Regression	-Log(IC50) M	Target	Dual specificity tyrosine-phosphorylation-regulated kinase 1A -Log(IC50) M model	395	79	external test set validation 	0.77	0.57	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL230	Prostaglandin G/H synthase 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Prostaglandin G/H synthase 2 -Log(IC50) M model	2715	543	external test set validation 	0.65	0.723	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL231	Histamine H1 receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Histamine H1 receptor -Log(IC50) M model	255	51	external test set validation 	0.4	0.983	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2318	A disintegrin and metalloproteinase with thrombospondin motifs 4 -Log(IC50) M	Regression	-Log(IC50) M	Target	A disintegrin and metalloproteinase with thrombospondin motifs 4 -Log(IC50) M model	250	50	external test set validation 	0.76	0.63	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2319	Kallikrein-1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Kallikrein-1 -Log(IC50) M model	35	7	external test set validation 	0.75	0.431	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL232	Alpha-1B adrenergic receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Alpha-1B adrenergic receptor -Log(IC50) M model	130	26	external test set validation 	0.42	0.707	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2321613	Potassium channel subfamily K member 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Potassium channel subfamily K member 3 -Log(IC50) M model	90	18	external test set validation 	0.51	0.578	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2321614	Potassium channel subfamily K member 9 -Log(IC50) M	Regression	-Log(IC50) M	Target	Potassium channel subfamily K member 9 -Log(IC50) M model	45	9	external test set validation 	0.75	0.319	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2321615	Potassium channel subfamily K member 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Potassium channel subfamily K member 2 -Log(IC50) M model	20	4	external test set validation 	0	0.612	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2321627	C-C chemokine receptor-like 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	C-C chemokine receptor-like 2 -Log(IC50) M model	155	31	external test set validation 	0.48	0.597	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2321630	"Alpha-1,6-mannosyl-glycoprotein 2-beta-N-acetylglucosaminyltransferase -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Alpha-1,6-mannosyl-glycoprotein 2-beta-N-acetylglucosaminyltransferase -Log(IC50) M model"	60	12	external test set validation 	0.58	0.983	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2327	Substance-K receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Substance-K receptor -Log(IC50) M model	250	50	external test set validation 	0.68	0.786	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL233	Mu-type opioid receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Mu-type opioid receptor -Log(IC50) M model	715	143	external test set validation 	0.69	0.715	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2331053	"6-phosphofructo-2-kinase/fructose-2,6-bisphosphatase 3 -Log(IC50) M"	Regression	-Log(IC50) M	Target	"6-phosphofructo-2-kinase/fructose-2,6-bisphosphatase 3 -Log(IC50) M model"	230	46	external test set validation 	0.22	0.45	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2334	Caspase-3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Caspase-3 -Log(IC50) M model	1635	327	external test set validation 	0.86	0.521	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2335	Lysosomal Pro-X carboxypeptidase -Log(IC50) M	Regression	-Log(IC50) M	Target	Lysosomal Pro-X carboxypeptidase -Log(IC50) M model	345	69	external test set validation 	0.8	0.762	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2337	Sodium- and chloride-dependent glycine transporter 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Sodium- and chloride-dependent glycine transporter 1 -Log(IC50) M model	335	67	external test set validation 	0.78	0.702	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL234	D(3) dopamine receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	D(3) dopamine receptor -Log(IC50) M model	230	46	external test set validation 	0.44	0.903	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2343	Serine/threonine-protein kinase Sgk1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Serine/threonine-protein kinase Sgk1 -Log(IC50) M model	75	15	external test set validation 	0.26	1.227	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2345	Ribosomal protein S6 kinase alpha-3|Ribosomal protein S6 kinase alpha-3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Ribosomal protein S6 kinase alpha-3|Ribosomal protein S6 kinase alpha-3 -Log(IC50) M model	310	62	external test set validation 	0.72	0.681	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL235	Peroxisome proliferator-activated receptor gamma -Log(IC50) M	Regression	-Log(IC50) M	Target	Peroxisome proliferator-activated receptor gamma -Log(IC50) M model	1275	255	external test set validation 	0.74	0.6	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL236	Delta-type opioid receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Delta-type opioid receptor -Log(IC50) M model	720	144	external test set validation 	0.74	0.805	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2364701	Proteasome subunit beta type-2|Proteasome subunit beta type-1|Proteasome subunit beta type-5|Proteasome subunit beta type-8|Proteasome subunit beta type-9|Proteasome subunit beta type-6|Proteasome subunit alpha type-1|Proteasome subunit alpha type-2|Proteasome subunit alpha type-3|Proteasome subunit alpha type-4|Proteasome subunit alpha type-5|Proteasome subunit alpha type-6|Proteasome subunit alpha type-7|Proteasome subunit alpha type-7-like|Proteasome subunit beta type-10|Proteasome subunit beta type-11|Proteasome subunit beta type-3|Proteasome subunit beta type-4|Proteasome subunit beta type-7|26S proteasome non-ATPase regulatory subunit 1|26S proteasome non-ATPase regulatory subunit 14|Proteasomal ubiquitin receptor ADRM1|26S protease regulatory subunit 7|26S protease regulatory subunit 4|26S protease regulatory subunit 6B|26S protease regulatory subunit 10B|26S protease regulatory subunit 6A|26S protease regulatory subunit 8|26S proteasome non-ATPase regulatory subunit 2|26S proteasome non-ATPase regulatory subunit 3|26S proteasome non-ATPase regulatory subunit 12|26S proteasome non-ATPase regulatory subunit 11|26S proteasome non-ATPase regulatory subunit 6|26S proteasome non-ATPase regulatory subunit 7|26S proteasome non-ATPase regulatory subunit 13|26S proteasome non-ATPase regulatory subunit 4|26S proteasome non-ATPase regulatory subunit 8|26S proteasome complex subunit SEM1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Proteasome subunit beta type-2|Proteasome subunit beta type-1|Proteasome subunit beta type-5|Proteasome subunit beta type-8|Proteasome subunit beta type-9|Proteasome subunit beta type-6|Proteasome subunit alpha type-1|Proteasome subunit alpha type-2|Proteasome subunit alpha type-3|Proteasome subunit alpha type-4|Proteasome subunit alpha type-5|Proteasome subunit alpha type-6|Proteasome subunit alpha type-7|Proteasome subunit alpha type-7-like|Proteasome subunit beta type-10|Proteasome subunit beta type-11|Proteasome subunit beta type-3|Proteasome subunit beta type-4|Proteasome subunit beta type-7|26S proteasome non-ATPase regulatory subunit 1|26S proteasome non-ATPase regulatory subunit 14|Proteasomal ubiquitin receptor ADRM1|26S protease regulatory subunit 7|26S protease regulatory subunit 4|26S protease regulatory subunit 6B|26S protease regulatory subunit 10B|26S protease regulatory subunit 6A|26S protease regulatory subunit 8|26S proteasome non-ATPase regulatory subunit 2|26S proteasome non-ATPase regulatory subunit 3|26S proteasome non-ATPase regulatory subunit 12|26S proteasome non-ATPase regulatory subunit 11|26S proteasome non-ATPase regulatory subunit 6|26S proteasome non-ATPase regulatory subunit 7|26S proteasome non-ATPase regulatory subunit 13|26S proteasome non-ATPase regulatory subunit 4|26S proteasome non-ATPase regulatory subunit 8|26S proteasome complex subunit SEM1 -Log(IC50) M model	20	4	external test set validation 	0.12	1.322	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL237	Kappa-type opioid receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Kappa-type opioid receptor -Log(IC50) M model	775	155	external test set validation 	0.59	0.702	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2373	C5a anaphylatoxin chemotactic receptor 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	C5a anaphylatoxin chemotactic receptor 1 -Log(IC50) M model	400	80	external test set validation 	0.8	0.574	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2378	M-phase inducer phosphatase 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	M-phase inducer phosphatase 3 -Log(IC50) M model	70	14	external test set validation 	0.05	0.887	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL238	Sodium-dependent dopamine transporter -Log(IC50) M	Regression	-Log(IC50) M	Target	Sodium-dependent dopamine transporter -Log(IC50) M model	1150	230	external test set validation 	0.65	0.656	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2384898	N-arachidonyl glycine receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	N-arachidonyl glycine receptor -Log(IC50) M model	10	2	external test set validation 	1	0.111	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2386	Chymotrypsin-C -Log(IC50) M	Regression	-Log(IC50) M	Target	Chymotrypsin-C -Log(IC50) M model	65	13	external test set validation 	0.47	0.889	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL239	Peroxisome proliferator-activated receptor alpha -Log(IC50) M	Regression	-Log(IC50) M	Target	Peroxisome proliferator-activated receptor alpha -Log(IC50) M model	670	134	external test set validation 	0.48	0.57	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2390810	Microtubule-associated protein 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Microtubule-associated protein 2 -Log(IC50) M model	70	14	external test set validation 	0.95	0.511	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2396508	Receptor-type tyrosine-protein phosphatase S -Log(IC50) M	Regression	-Log(IC50) M	Target	Receptor-type tyrosine-protein phosphatase S -Log(IC50) M model	25	5	external test set validation 	0.27	0.576	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL240	Potassium voltage-gated channel subfamily H member 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Potassium voltage-gated channel subfamily H member 2 -Log(IC50) M model	5095	1019	external test set validation 	0.64	0.549	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2409	Bifunctional epoxide hydrolase 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Bifunctional epoxide hydrolase 2 -Log(IC50) M model	1520	304	external test set validation 	0.77	0.621	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL241	"cGMP-inhibited 3',5'-cyclic phosphodiesterase A -Log(IC50) M"	Regression	-Log(IC50) M	Target	"cGMP-inhibited 3',5'-cyclic phosphodiesterase A -Log(IC50) M model"	280	56	external test set validation 	0.78	0.736	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2413	C-C chemokine receptor type 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	C-C chemokine receptor type 1 -Log(IC50) M model	540	108	external test set validation 	0.68	0.716	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2414	C-C chemokine receptor type 4 -Log(IC50) M	Regression	-Log(IC50) M	Target	C-C chemokine receptor type 4 -Log(IC50) M model	335	67	external test set validation 	0.71	0.538	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL242	Estrogen receptor beta -Log(IC50) M	Regression	-Log(IC50) M	Target	Estrogen receptor beta -Log(IC50) M model	1275	255	external test set validation 	0.65	0.727	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2424	Lactoylglutathione lyase -Log(IC50) M	Regression	-Log(IC50) M	Target	Lactoylglutathione lyase -Log(IC50) M model	35	7	external test set validation 	0.47	0.747	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2424504	Lysine-specific demethylase 5A|Lysine-specific demethylase 5A -Log(IC50) M	Regression	-Log(IC50) M	Target	Lysine-specific demethylase 5A|Lysine-specific demethylase 5A -Log(IC50) M model	50	10	external test set validation 	0.94	0.498	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2428	"Myosin light chain kinase, smooth muscle -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Myosin light chain kinase, smooth muscle -Log(IC50) M model"	95	19	external test set validation 	0.87	0.591	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2429708	Poly [ADP-ribose] polymerase 10 -Log(IC50) M	Regression	-Log(IC50) M	Target	Poly [ADP-ribose] polymerase 10 -Log(IC50) M model	25	5	external test set validation 	0.39	0.614	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2431	RAC-beta serine/threonine-protein kinase -Log(IC50) M	Regression	-Log(IC50) M	Target	RAC-beta serine/threonine-protein kinase -Log(IC50) M model	825	165	external test set validation 	0.74	0.515	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2434	C-X-C chemokine receptor type 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	C-X-C chemokine receptor type 2 -Log(IC50) M model	525	105	external test set validation 	0.58	0.624	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2437	Protein phosphatase 1A -Log(IC50) M	Regression	-Log(IC50) M	Target	Protein phosphatase 1A -Log(IC50) M model	10	2	external test set validation 	1	0.251	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2439	Myeloperoxidase -Log(IC50) M	Regression	-Log(IC50) M	Target	Myeloperoxidase -Log(IC50) M model	140	28	external test set validation 	0.49	0.526	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2439944	2-acylglycerol O-acyltransferase 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	2-acylglycerol O-acyltransferase 2 -Log(IC50) M model	190	38	external test set validation 	0.7	0.351	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL244	Coagulation factor X -Log(IC50) M	Regression	-Log(IC50) M	Target	Coagulation factor X -Log(IC50) M model	1980	396	external test set validation 	0.7	0.741	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2443	Kallikrein-7 -Log(IC50) M	Regression	-Log(IC50) M	Target	Kallikrein-7 -Log(IC50) M model	45	9	external test set validation 	0.87	0.713	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2447	Deoxycytidine kinase -Log(IC50) M	Regression	-Log(IC50) M	Target	Deoxycytidine kinase -Log(IC50) M model	110	22	external test set validation 	0.57	0.587	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL245	Muscarinic acetylcholine receptor M3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Muscarinic acetylcholine receptor M3 -Log(IC50) M model	565	113	external test set validation 	0.75	0.749	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL246	Beta-3 adrenergic receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Beta-3 adrenergic receptor -Log(IC50) M model	50	10	external test set validation 	0.04	1.093	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2473	Presenilin-1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Presenilin-1 -Log(IC50) M model	50	10	external test set validation 	0.87	0.442	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2474	Methionine aminopeptidase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Methionine aminopeptidase 1 -Log(IC50) M model	140	28	external test set validation 	0.63	0.475	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2478	Alpha-amylase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Alpha-amylase 1 -Log(IC50) M model	45	9	external test set validation 	0.05	0.758	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL248	Neutrophil elastase -Log(IC50) M	Regression	-Log(IC50) M	Target	Neutrophil elastase -Log(IC50) M model	540	108	external test set validation 	0.7	0.728	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2487	Amyloid beta A4 protein -Log(IC50) M	Regression	-Log(IC50) M	Target	Amyloid beta A4 protein -Log(IC50) M model	475	95	external test set validation 	0.68	0.729	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL249	Substance-P receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Substance-P receptor -Log(IC50) M model	1460	292	external test set validation 	0.68	0.673	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2492	Neuronal acetylcholine receptor subunit alpha-7 -Log(IC50) M	Regression	-Log(IC50) M	Target	Neuronal acetylcholine receptor subunit alpha-7 -Log(IC50) M model	75	15	external test set validation 	0.75	0.638	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL250	Platelet-activating factor receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Platelet-activating factor receptor -Log(IC50) M model	405	81	external test set validation 	0.72	0.602	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2500	Thiopurine S-methyltransferase -Log(IC50) M	Regression	-Log(IC50) M	Target	Thiopurine S-methyltransferase -Log(IC50) M model	25	5	external test set validation 	0.88	0.792	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2508	Cyclin-dependent kinase 6 -Log(IC50) M	Regression	-Log(IC50) M	Target	Cyclin-dependent kinase 6 -Log(IC50) M model	315	63	external test set validation 	0.71	0.495	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL251	Adenosine receptor A2a -Log(IC50) M	Regression	-Log(IC50) M	Target	Adenosine receptor A2a -Log(IC50) M model	265	53	external test set validation 	0.48	0.782	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2518	Bifunctional purine biosynthesis protein PURH -Log(IC50) M	Regression	-Log(IC50) M	Target	Bifunctional purine biosynthesis protein PURH -Log(IC50) M model	15	3	external test set validation 	0	0.771	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL252	Endothelin-1 receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Endothelin-1 receptor -Log(IC50) M model	1160	232	external test set validation 	0.67	0.792	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2522	Beta-galactosidase -Log(IC50) M	Regression	-Log(IC50) M	Target	Beta-galactosidase -Log(IC50) M model	30	6	external test set validation 	0.38	0.611	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2524	Alpha-galactosidase A -Log(IC50) M	Regression	-Log(IC50) M	Target	Alpha-galactosidase A -Log(IC50) M model	10	2	external test set validation 	1	1.39	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2525	Beta-secretase 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Beta-secretase 2 -Log(IC50) M model	600	120	external test set validation 	0.69	0.579	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2527	Serine/threonine-protein kinase Chk2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Serine/threonine-protein kinase Chk2 -Log(IC50) M model	325	65	external test set validation 	0.81	0.523	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL253	Cannabinoid receptor 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Cannabinoid receptor 2 -Log(IC50) M model	735	147	external test set validation 	0.71	0.567	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2531	P2X purinoceptor 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	P2X purinoceptor 2 -Log(IC50) M model	50	10	external test set validation 	0.71	0.571	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2534	3-phosphoinositide-dependent protein kinase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	3-phosphoinositide-dependent protein kinase 1 -Log(IC50) M model	770	154	external test set validation 	0.83	0.552	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2535	"Solute carrier family 2, facilitated glucose transporter member 1 -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Solute carrier family 2, facilitated glucose transporter member 1 -Log(IC50) M model"	505	101	external test set validation 	0.6	0.398	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2536	Phospholipase D1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Phospholipase D1 -Log(IC50) M model	90	18	external test set validation 	0.81	0.585	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL254	"cAMP-specific 3',5'-cyclic phosphodiesterase 4A -Log(IC50) M"	Regression	-Log(IC50) M	Target	"cAMP-specific 3',5'-cyclic phosphodiesterase 4A -Log(IC50) M model"	500	100	external test set validation 	0.67	0.748	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2543	Casein kinase I isoform gamma-2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Casein kinase I isoform gamma-2 -Log(IC50) M model	20	4	external test set validation 	0.76	0.794	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2544	Peptidyl-glycine alpha-amidating monooxygenase -Log(IC50) M	Regression	-Log(IC50) M	Target	Peptidyl-glycine alpha-amidating monooxygenase -Log(IC50) M model	20	4	external test set validation 	0.98	0.492	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL255	Adenosine receptor A2b -Log(IC50) M	Regression	-Log(IC50) M	Target	Adenosine receptor A2b -Log(IC50) M model	330	66	external test set validation 	0.72	0.49	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2553	Ribosomal protein S6 kinase alpha-1|Ribosomal protein S6 kinase alpha-1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Ribosomal protein S6 kinase alpha-1|Ribosomal protein S6 kinase alpha-1 -Log(IC50) M model	65	13	external test set validation 	0.48	1.333	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2558	Death-associated protein kinase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Death-associated protein kinase 1 -Log(IC50) M model	30	6	external test set validation 	0.94	1.072	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL256	Adenosine receptor A3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Adenosine receptor A3 -Log(IC50) M model	405	81	external test set validation 	0.46	0.973	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2563	Histone deacetylase 5 -Log(IC50) M	Regression	-Log(IC50) M	Target	Histone deacetylase 5 -Log(IC50) M model	120	24	external test set validation 	0.53	0.85	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2568	"Glycogen phosphorylase, liver form -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Glycogen phosphorylase, liver form -Log(IC50) M model"	360	72	external test set validation 	0.61	0.532	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL258	Tyrosine-protein kinase Lck -Log(IC50) M	Regression	-Log(IC50) M	Target	Tyrosine-protein kinase Lck -Log(IC50) M model	1250	250	external test set validation 	0.73	0.723	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2581	Cathepsin D -Log(IC50) M	Regression	-Log(IC50) M	Target	Cathepsin D -Log(IC50) M model	1170	234	external test set validation 	0.81	0.582	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL259	Melanocortin receptor 4 -Log(IC50) M	Regression	-Log(IC50) M	Target	Melanocortin receptor 4 -Log(IC50) M model	615	123	external test set validation 	0.7	0.579	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2593	Glycylpeptide N-tetradecanoyltransferase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Glycylpeptide N-tetradecanoyltransferase 1 -Log(IC50) M model	85	17	external test set validation 	0.94	0.507	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2598	Protein kinase C iota type -Log(IC50) M	Regression	-Log(IC50) M	Target	Protein kinase C iota type -Log(IC50) M model	450	90	external test set validation 	0.49	0.527	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2599	Tyrosine-protein kinase SYK -Log(IC50) M	Regression	-Log(IC50) M	Target	Tyrosine-protein kinase SYK -Log(IC50) M model	1860	372	external test set validation 	0.82	0.537	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL260	Mitogen-activated protein kinase 14 -Log(IC50) M	Regression	-Log(IC50) M	Target	Mitogen-activated protein kinase 14 -Log(IC50) M model	3125	625	external test set validation 	0.74	0.583	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2608	Lysosomal alpha-glucosidase -Log(IC50) M	Regression	-Log(IC50) M	Target	Lysosomal alpha-glucosidase -Log(IC50) M model	40	8	external test set validation 	0.04	0.852	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL261	Carbonic anhydrase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Carbonic anhydrase 1 -Log(IC50) M model	135	27	external test set validation 	0.61	0.756	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2617	Tryptase alpha/beta-1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Tryptase alpha/beta-1 -Log(IC50) M model	60	12	external test set validation 	0.3	0.712	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL262	Glycogen synthase kinase-3 beta -Log(IC50) M	Regression	-Log(IC50) M	Target	Glycogen synthase kinase-3 beta -Log(IC50) M model	1740	348	external test set validation 	0.68	0.733	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2632	"HLA class I histocompatibility antigen, A-3 alpha chain -Log(IC50) M"	Regression	-Log(IC50) M	Target	"HLA class I histocompatibility antigen, A-3 alpha chain -Log(IC50) M model"	165	33	external test set validation 	0.31	0.636	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2633	Prostatic acid phosphatase -Log(IC50) M	Regression	-Log(IC50) M	Target	Prostatic acid phosphatase -Log(IC50) M model	40	8	external test set validation 	0.9	0.615	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2634	Tyrosine-protein kinase CSK -Log(IC50) M	Regression	-Log(IC50) M	Target	Tyrosine-protein kinase CSK -Log(IC50) M model	75	15	external test set validation 	0.45	0.84	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2635	Dual specificity protein phosphatase 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Dual specificity protein phosphatase 3 -Log(IC50) M model	450	90	external test set validation 	0.26	0.536	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2637	Mitogen-activated protein kinase 10 -Log(IC50) M	Regression	-Log(IC50) M	Target	Mitogen-activated protein kinase 10 -Log(IC50) M model	1075	215	external test set validation 	0.79	0.587	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL264	Histamine H3 receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Histamine H3 receptor -Log(IC50) M model	395	79	external test set validation 	0.78	0.643	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2641	Kallikrein-14 -Log(IC50) M	Regression	-Log(IC50) M	Target	Kallikrein-14 -Log(IC50) M model	20	4	external test set validation 	0.95	0.528	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2652	"cGMP-dependent 3',5'-cyclic phosphodiesterase -Log(IC50) M"	Regression	-Log(IC50) M	Target	"cGMP-dependent 3',5'-cyclic phosphodiesterase -Log(IC50) M model"	575	115	external test set validation 	0.85	0.531	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2664	Adenosylhomocysteinase -Log(IC50) M	Regression	-Log(IC50) M	Target	Adenosylhomocysteinase -Log(IC50) M model	80	16	external test set validation 	0.39	1.053	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL267	Proto-oncogene tyrosine-protein kinase Src -Log(IC50) M	Regression	-Log(IC50) M	Target	Proto-oncogene tyrosine-protein kinase Src -Log(IC50) M model	2315	463	external test set validation 	0.75	0.705	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL268	Cathepsin K -Log(IC50) M	Regression	-Log(IC50) M	Target	Cathepsin K -Log(IC50) M model	1195	239	external test set validation 	0.69	0.657	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2688	Hexokinase-1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Hexokinase-1 -Log(IC50) M model	50	10	external test set validation 	0.48	0.848	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2689	Macrophage-stimulating protein receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Macrophage-stimulating protein receptor -Log(IC50) M model	90	18	external test set validation 	0.61	0.813	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2693	Leucyl-cystinyl aminopeptidase -Log(IC50) M	Regression	-Log(IC50) M	Target	Leucyl-cystinyl aminopeptidase -Log(IC50) M model	75	15	external test set validation 	0.67	0.568	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2695	Focal adhesion kinase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Focal adhesion kinase 1 -Log(IC50) M model	1000	200	external test set validation 	0.87	0.468	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2706	Receptor-type tyrosine-protein phosphatase beta -Log(IC50) M	Regression	-Log(IC50) M	Target	Receptor-type tyrosine-protein phosphatase beta -Log(IC50) M model	45	9	external test set validation 	0.67	0.409	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2716	Histone deacetylase 7 -Log(IC50) M	Regression	-Log(IC50) M	Target	Histone deacetylase 7 -Log(IC50) M model	140	28	external test set validation 	0.44	0.817	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2717	"Dual 3',5'-cyclic-AMP and -GMP phosphodiesterase 11A -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Dual 3',5'-cyclic-AMP and -GMP phosphodiesterase 11A -Log(IC50) M model"	95	19	external test set validation 	0.45	0.725	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2721	Excitatory amino acid transporter 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Excitatory amino acid transporter 3 -Log(IC50) M model	35	7	external test set validation 	0.2	0.742	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2722	"Cytochrome P450 11B2, mitochondrial -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Cytochrome P450 11B2, mitochondrial -Log(IC50) M model"	770	154	external test set validation 	0.64	0.577	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2728	Beta-glucuronidase -Log(IC50) M	Regression	-Log(IC50) M	Target	Beta-glucuronidase -Log(IC50) M model	70	14	external test set validation 	0.11	0.708	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2730	Protein-glutamine gamma-glutamyltransferase 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Protein-glutamine gamma-glutamyltransferase 2 -Log(IC50) M model	265	53	external test set validation 	0.73	0.473	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2731	Galanin receptor type 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Galanin receptor type 3 -Log(IC50) M model	145	29	external test set validation 	0.21	0.405	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2734	Phospholipase D2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Phospholipase D2 -Log(IC50) M model	75	15	external test set validation 	0.3	0.755	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2736	Metabotropic glutamate receptor 4 -Log(IC50) M	Regression	-Log(IC50) M	Target	Metabotropic glutamate receptor 4 -Log(IC50) M model	20	4	external test set validation 	0.84	0.547	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2738	Acrosin -Log(IC50) M	Regression	-Log(IC50) M	Target	Acrosin -Log(IC50) M model	85	17	external test set validation 	0.98	0.433	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL274	C-C chemokine receptor type 5 -Log(IC50) M	Regression	-Log(IC50) M	Target	C-C chemokine receptor type 5 -Log(IC50) M model	1635	327	external test set validation 	0.78	0.673	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2742	Fibroblast growth factor receptor 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Fibroblast growth factor receptor 3 -Log(IC50) M model	555	111	external test set validation 	0.7	0.589	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2748	"Sucrase-isomaltase, intestinal -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Sucrase-isomaltase, intestinal -Log(IC50) M model"	70	14	external test set validation 	0.42	0.737	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL275	"cAMP-specific 3',5'-cyclic phosphodiesterase 4B -Log(IC50) M"	Regression	-Log(IC50) M	Target	"cAMP-specific 3',5'-cyclic phosphodiesterase 4B -Log(IC50) M model"	790	158	external test set validation 	0.75	0.709	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2755	"Glutathione reductase, mitochondrial -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Glutathione reductase, mitochondrial -Log(IC50) M model"	95	19	external test set validation 	0.02	0.662	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2758	Mannose-6-phosphate isomerase -Log(IC50) M	Regression	-Log(IC50) M	Target	Mannose-6-phosphate isomerase -Log(IC50) M model	295	59	external test set validation 	0.03	0.509	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2778	Ileal sodium/bile acid cotransporter -Log(IC50) M	Regression	-Log(IC50) M	Target	Ileal sodium/bile acid cotransporter -Log(IC50) M model	230	46	external test set validation 	0.58	0.72	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL278	Integrin alpha-4 -Log(IC50) M	Regression	-Log(IC50) M	Target	Integrin alpha-4 -Log(IC50) M model	135	27	external test set validation 	0.9	0.485	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2781	Sodium/hydrogen exchanger 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Sodium/hydrogen exchanger 1 -Log(IC50) M model	260	52	external test set validation 	0.73	0.593	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2782	Sterol O-acyltransferase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Sterol O-acyltransferase 1 -Log(IC50) M model	270	54	external test set validation 	0.71	0.545	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2789	Estradiol 17-beta-dehydrogenase 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Estradiol 17-beta-dehydrogenase 2 -Log(IC50) M model	400	80	external test set validation 	0.6	0.447	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL279	Vascular endothelial growth factor receptor 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Vascular endothelial growth factor receptor 2 -Log(IC50) M model	5215	1043	external test set validation 	0.69	0.622	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2793	Casein kinase I isoform alpha -Log(IC50) M	Regression	-Log(IC50) M	Target	Casein kinase I isoform alpha -Log(IC50) M model	60	12	external test set validation 	0.07	0.924	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL280	Collagenase 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Collagenase 3 -Log(IC50) M model	2045	409	external test set validation 	0.75	0.733	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2801	Calcium/calmodulin-dependent protein kinase type II subunit delta -Log(IC50) M	Regression	-Log(IC50) M	Target	Calcium/calmodulin-dependent protein kinase type II subunit delta -Log(IC50) M model	75	15	external test set validation 	0.47	0.545	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2803	Tyrosine-protein kinase ZAP-70 -Log(IC50) M	Regression	-Log(IC50) M	Target	Tyrosine-protein kinase ZAP-70 -Log(IC50) M model	245	49	external test set validation 	0.59	0.811	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2808	Oxysterols receptor LXR-alpha -Log(IC50) M	Regression	-Log(IC50) M	Target	Oxysterols receptor LXR-alpha -Log(IC50) M model	500	100	external test set validation 	0.75	0.672	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2810	Protein-glutamine gamma-glutamyltransferase K -Log(IC50) M	Regression	-Log(IC50) M	Target	Protein-glutamine gamma-glutamyltransferase K -Log(IC50) M model	75	15	external test set validation 	0	0.475	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2815	High affinity nerve growth factor receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	High affinity nerve growth factor receptor -Log(IC50) M model	1030	206	external test set validation 	0.69	0.679	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2820	Coagulation factor XI -Log(IC50) M	Regression	-Log(IC50) M	Target	Coagulation factor XI -Log(IC50) M model	155	31	external test set validation 	0.77	0.608	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2821	Coagulation factor XII -Log(IC50) M	Regression	-Log(IC50) M	Target	Coagulation factor XII -Log(IC50) M model	30	6	external test set validation 	0.73	0.664	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2828	Casein kinase I isoform delta -Log(IC50) M	Regression	-Log(IC50) M	Target	Casein kinase I isoform delta -Log(IC50) M model	170	34	external test set validation 	0.39	0.748	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL283	Stromelysin-1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Stromelysin-1 -Log(IC50) M model	1135	227	external test set validation 	0.76	0.669	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2835	Tyrosine-protein kinase JAK1|Tyrosine-protein kinase JAK1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Tyrosine-protein kinase JAK1|Tyrosine-protein kinase JAK1 -Log(IC50) M model	1155	231	external test set validation 	0.73	0.631	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL284	Dipeptidyl peptidase 4 -Log(IC50) M	Regression	-Log(IC50) M	Target	Dipeptidyl peptidase 4 -Log(IC50) M model	3050	610	external test set validation 	0.75	0.658	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2842	Serine/threonine-protein kinase mTOR -Log(IC50) M	Regression	-Log(IC50) M	Target	Serine/threonine-protein kinase mTOR -Log(IC50) M model	2665	533	external test set validation 	0.79	0.541	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2845	Protein phosphatase 1B -Log(IC50) M	Regression	-Log(IC50) M	Target	Protein phosphatase 1B -Log(IC50) M model	40	8	external test set validation 	0.05	1.152	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2850	Glycogen synthase kinase-3 alpha -Log(IC50) M	Regression	-Log(IC50) M	Target	Glycogen synthase kinase-3 alpha -Log(IC50) M model	320	64	external test set validation 	0.64	1.006	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL286	Renin -Log(IC50) M	Regression	-Log(IC50) M	Target	Renin -Log(IC50) M model	1995	399	external test set validation 	0.71	0.705	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2864	Methylated-DNA--protein-cysteine methyltransferase -Log(IC50) M	Regression	-Log(IC50) M	Target	Methylated-DNA--protein-cysteine methyltransferase -Log(IC50) M model	35	7	external test set validation 	0.76	0.905	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL287	Sigma non-opioid intracellular receptor 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Sigma non-opioid intracellular receptor 1 -Log(IC50) M model	655	131	external test set validation 	0.62	0.669	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2870	"Methionine--tRNA ligase, cytoplasmic -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Methionine--tRNA ligase, cytoplasmic -Log(IC50) M model"	45	9	external test set validation 	0.79	0.765	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2872	Mitogen-activated protein kinase kinase kinase 9 -Log(IC50) M	Regression	-Log(IC50) M	Target	Mitogen-activated protein kinase kinase kinase 9 -Log(IC50) M model	40	8	external test set validation 	0.42	0.441	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL288	"cAMP-specific 3',5'-cyclic phosphodiesterase 4D -Log(IC50) M"	Regression	-Log(IC50) M	Target	"cAMP-specific 3',5'-cyclic phosphodiesterase 4D -Log(IC50) M model"	605	121	external test set validation 	0.77	0.642	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2883	"Thymidine kinase, cytosolic -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Thymidine kinase, cytosolic -Log(IC50) M model"	50	10	external test set validation 	0.55	0.458	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2886	Phosphoglycerate kinase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Phosphoglycerate kinase 1 -Log(IC50) M model	100	20	external test set validation 	0.43	0.693	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2888	Metabotropic glutamate receptor 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Metabotropic glutamate receptor 3 -Log(IC50) M model	70	14	external test set validation 	0.68	0.458	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2889	Tyrosine-protein phosphatase non-receptor type 22 -Log(IC50) M	Regression	-Log(IC50) M	Target	Tyrosine-protein phosphatase non-receptor type 22 -Log(IC50) M model	230	46	external test set validation 	0.61	0.464	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL289	Cytochrome P450 2D6 -Log(IC50) M	Regression	-Log(IC50) M	Target	Cytochrome P450 2D6 -Log(IC50) M model	1825	365	external test set validation 	0.47	0.587	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL290	"cGMP-inhibited 3',5'-cyclic phosphodiesterase B -Log(IC50) M"	Regression	-Log(IC50) M	Target	"cGMP-inhibited 3',5'-cyclic phosphodiesterase B -Log(IC50) M model"	100	20	external test set validation 	0.86	0.699	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2903	Arachidonate 15-lipoxygenase -Log(IC50) M	Regression	-Log(IC50) M	Target	Arachidonate 15-lipoxygenase -Log(IC50) M model	260	52	external test set validation 	0.54	0.648	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL291	"cAMP-specific 3',5'-cyclic phosphodiesterase 4C -Log(IC50) M"	Regression	-Log(IC50) M	Target	"cAMP-specific 3',5'-cyclic phosphodiesterase 4C -Log(IC50) M model"	95	19	external test set validation 	0.44	0.883	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2911	"Phosphoenolpyruvate carboxykinase, cytosolic [GTP] -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Phosphoenolpyruvate carboxykinase, cytosolic [GTP] -Log(IC50) M model"	15	3	external test set validation 	1	0.261	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2916	Telomerase reverse transcriptase -Log(IC50) M	Regression	-Log(IC50) M	Target	Telomerase reverse transcriptase -Log(IC50) M model	520	104	external test set validation 	0.58	0.487	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2938	Protein kinase C gamma type -Log(IC50) M	Regression	-Log(IC50) M	Target	Protein kinase C gamma type -Log(IC50) M model	95	19	external test set validation 	0.22	0.776	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2940	Low affinity immunoglobulin epsilon Fc receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Low affinity immunoglobulin epsilon Fc receptor -Log(IC50) M model	75	15	external test set validation 	0.1	0.82	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2954	Cathepsin S -Log(IC50) M	Regression	-Log(IC50) M	Target	Cathepsin S -Log(IC50) M model	1375	275	external test set validation 	0.75	0.636	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2955	Sphingosine 1-phosphate receptor 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Sphingosine 1-phosphate receptor 2 -Log(IC50) M model	160	32	external test set validation 	0.71	0.624	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2959	Tyrosine-protein kinase ITK/TSK -Log(IC50) M	Regression	-Log(IC50) M	Target	Tyrosine-protein kinase ITK/TSK -Log(IC50) M model	370	74	external test set validation 	0.64	0.632	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2971	Tyrosine-protein kinase JAK2|Tyrosine-protein kinase JAK2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Tyrosine-protein kinase JAK2|Tyrosine-protein kinase JAK2 -Log(IC50) M model	2090	418	external test set validation 	0.68	0.658	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2973	Rho-associated protein kinase 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Rho-associated protein kinase 2 -Log(IC50) M model	930	186	external test set validation 	0.65	0.731	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2976	Tyrosine-protein phosphatase non-receptor type 13 -Log(IC50) M	Regression	-Log(IC50) M	Target	Tyrosine-protein phosphatase non-receptor type 13 -Log(IC50) M model	15	3	external test set validation 	0.27	0.618	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL298	Gastrin/cholecystokinin type B receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Gastrin/cholecystokinin type B receptor -Log(IC50) M model	850	170	external test set validation 	0.79	0.635	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL299	Protein kinase C alpha type -Log(IC50) M	Regression	-Log(IC50) M	Target	Protein kinase C alpha type -Log(IC50) M model	355	71	external test set validation 	0.7	0.746	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2996	Protein kinase C delta type -Log(IC50) M	Regression	-Log(IC50) M	Target	Protein kinase C delta type -Log(IC50) M model	390	78	external test set validation 	0.6	0.705	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL2998	P2X purinoceptor 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	P2X purinoceptor 3 -Log(IC50) M model	950	190	external test set validation 	0.76	0.352	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3004	Multidrug resistance-associated protein 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Multidrug resistance-associated protein 1 -Log(IC50) M model	260	52	external test set validation 	0.69	0.473	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL301	Cyclin-dependent kinase 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Cyclin-dependent kinase 2 -Log(IC50) M model	1305	261	external test set validation 	0.75	0.686	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3012	"High affinity cAMP-specific 3',5'-cyclic phosphodiesterase 7A -Log(IC50) M"	Regression	-Log(IC50) M	Target	"High affinity cAMP-specific 3',5'-cyclic phosphodiesterase 7A -Log(IC50) M model"	605	121	external test set validation 	0.82	0.474	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3018	Suppressor of tumorigenicity 14 protein -Log(IC50) M	Regression	-Log(IC50) M	Target	Suppressor of tumorigenicity 14 protein -Log(IC50) M model	20	4	external test set validation 	0.81	0.787	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3019	Cytochrome P450 3A5 -Log(IC50) M	Regression	-Log(IC50) M	Target	Cytochrome P450 3A5 -Log(IC50) M model	25	5	external test set validation 	0.01	0.639	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3023	Sphingosine kinase 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Sphingosine kinase 2 -Log(IC50) M model	65	13	external test set validation 	0.5	0.738	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3024	Serine/threonine-protein kinase PLK1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Serine/threonine-protein kinase PLK1 -Log(IC50) M model	600	120	external test set validation 	0.76	0.635	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3028	Egl nine homolog 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Egl nine homolog 2 -Log(IC50) M model	65	13	external test set validation 	0.85	0.468	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3038469	Cyclin-dependent kinase 2|Cyclin-A2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Cyclin-dependent kinase 2|Cyclin-A2 -Log(IC50) M model	275	55	external test set validation 	0.65	0.686	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3038488	G protein-activated inward rectifier potassium channel 1|G protein-activated inward rectifier potassium channel 4 -Log(IC50) M	Regression	-Log(IC50) M	Target	G protein-activated inward rectifier potassium channel 1|G protein-activated inward rectifier potassium channel 4 -Log(IC50) M model	40	8	external test set validation 	0.52	0.537	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3038489	G protein-activated inward rectifier potassium channel 1|G protein-activated inward rectifier potassium channel 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	G protein-activated inward rectifier potassium channel 1|G protein-activated inward rectifier potassium channel 2 -Log(IC50) M model	20	4	external test set validation 	0.96	0.417	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3038491	Tyrosine-protein kinase JAK1|Tyrosine-protein kinase JAK1|Tyrosine-protein kinase JAK3|Tyrosine-protein kinase JAK3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Tyrosine-protein kinase JAK1|Tyrosine-protein kinase JAK1|Tyrosine-protein kinase JAK3|Tyrosine-protein kinase JAK3 -Log(IC50) M model	25	5	external test set validation 	0.29	1.333	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3038499	Mitogen-activated protein kinase kinase kinase 7|TGF-beta-activated kinase 1 and MAP3K7-binding protein 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Mitogen-activated protein kinase kinase kinase 7|TGF-beta-activated kinase 1 and MAP3K7-binding protein 1 -Log(IC50) M model	105	21	external test set validation 	0.56	0.473	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3045	Protein kinase C beta type -Log(IC50) M	Regression	-Log(IC50) M	Target	Protein kinase C beta type -Log(IC50) M model	280	56	external test set validation 	0.56	0.941	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3047	Transmembrane prolyl 4-hydroxylase -Log(IC50) M	Regression	-Log(IC50) M	Target	Transmembrane prolyl 4-hydroxylase -Log(IC50) M model	95	19	external test set validation 	0.66	0.524	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3055	Cyclin-dependent kinase 7 -Log(IC50) M	Regression	-Log(IC50) M	Target	Cyclin-dependent kinase 7 -Log(IC50) M model	95	19	external test set validation 	0.29	0.806	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3060	Sodium- and chloride-dependent glycine transporter 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Sodium- and chloride-dependent glycine transporter 2 -Log(IC50) M model	110	22	external test set validation 	0.61	0.565	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3068	Neuronal acetylcholine receptor subunit alpha-3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Neuronal acetylcholine receptor subunit alpha-3 -Log(IC50) M model	5	1	external test set validation 	nan	0.319	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3070	Intercellular adhesion molecule 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Intercellular adhesion molecule 1 -Log(IC50) M model	105	21	external test set validation 	0.55	0.703	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL308	Cyclin-dependent kinase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Cyclin-dependent kinase 1 -Log(IC50) M model	960	192	external test set validation 	0.67	0.718	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3085	Excitatory amino acid transporter 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Excitatory amino acid transporter 1 -Log(IC50) M model	55	11	external test set validation 	0.83	0.379	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3091	Sortilin -Log(IC50) M	Regression	-Log(IC50) M	Target	Sortilin -Log(IC50) M model	40	8	external test set validation 	0.17	0.622	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3091268	Nuclear receptor ROR-beta -Log(IC50) M	Regression	-Log(IC50) M	Target	Nuclear receptor ROR-beta -Log(IC50) M model	25	5	external test set validation 	0.36	0.209	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3092	Cathepsin E -Log(IC50) M	Regression	-Log(IC50) M	Target	Cathepsin E -Log(IC50) M model	40	8	external test set validation 	0.82	1.02	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3094	MAP kinase-activated protein kinase 5 -Log(IC50) M	Regression	-Log(IC50) M	Target	MAP kinase-activated protein kinase 5 -Log(IC50) M model	45	9	external test set validation 	0.61	0.555	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3100	Retinol-binding protein 4 -Log(IC50) M	Regression	-Log(IC50) M	Target	Retinol-binding protein 4 -Log(IC50) M model	90	18	external test set validation 	0.12	0.732	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3102	Dopamine beta-hydroxylase -Log(IC50) M	Regression	-Log(IC50) M	Target	Dopamine beta-hydroxylase -Log(IC50) M model	50	10	external test set validation 	0.01	0.632	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3105	Poly [ADP-ribose] polymerase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Poly [ADP-ribose] polymerase 1 -Log(IC50) M model	1495	299	external test set validation 	0.7	0.621	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3106	Thymidine phosphorylase -Log(IC50) M	Regression	-Log(IC50) M	Target	Thymidine phosphorylase -Log(IC50) M model	105	21	external test set validation 	0.77	0.223	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3108638	Transcription intermediary factor 1-alpha|Transcription intermediary factor 1-alpha -Log(IC50) M	Regression	-Log(IC50) M	Target	Transcription intermediary factor 1-alpha|Transcription intermediary factor 1-alpha -Log(IC50) M model	85	17	external test set validation 	0.88	0.274	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3108640	Bromodomain-containing protein 9 -Log(IC50) M	Regression	-Log(IC50) M	Target	Bromodomain-containing protein 9 -Log(IC50) M model	60	12	external test set validation 	0.7	0.801	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3108642	Bromodomain adjacent to zinc finger domain protein 2A|Bromodomain adjacent to zinc finger domain protein 2A -Log(IC50) M	Regression	-Log(IC50) M	Target	Bromodomain adjacent to zinc finger domain protein 2A|Bromodomain adjacent to zinc finger domain protein 2A -Log(IC50) M model	30	6	external test set validation 	0.66	0.486	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3111	Ribosomal protein S6 kinase beta-2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Ribosomal protein S6 kinase beta-2 -Log(IC50) M model	20	4	external test set validation 	0.49	0.83	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3112376	Alpha-ketoglutarate-dependent dioxygenase alkB homolog 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Alpha-ketoglutarate-dependent dioxygenase alkB homolog 3 -Log(IC50) M model	65	13	external test set validation 	0.08	0.449	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3116	Cyclin-dependent kinase 9 -Log(IC50) M	Regression	-Log(IC50) M	Target	Cyclin-dependent kinase 9 -Log(IC50) M model	415	83	external test set validation 	0.29	0.675	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3117	Choline kinase alpha -Log(IC50) M	Regression	-Log(IC50) M	Target	Choline kinase alpha -Log(IC50) M model	115	23	external test set validation 	0.77	0.394	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3119	Transient receptor potential cation channel subfamily V member 4 -Log(IC50) M	Regression	-Log(IC50) M	Target	Transient receptor potential cation channel subfamily V member 4 -Log(IC50) M model	75	15	external test set validation 	0.76	0.641	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3130	"Phosphatidylinositol 4,5-bisphosphate 3-kinase catalytic subunit delta isoform -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Phosphatidylinositol 4,5-bisphosphate 3-kinase catalytic subunit delta isoform -Log(IC50) M model"	1190	238	external test set validation 	0.7	0.644	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3132741	Peregrin|Peregrin|Peregrin -Log(IC50) M	Regression	-Log(IC50) M	Target	Peregrin|Peregrin|Peregrin -Log(IC50) M model	100	20	external test set validation 	0.01	0.667	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3142	DNA-dependent protein kinase catalytic subunit -Log(IC50) M	Regression	-Log(IC50) M	Target	DNA-dependent protein kinase catalytic subunit -Log(IC50) M model	740	148	external test set validation 	0.69	0.69	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3145	"Phosphatidylinositol 4,5-bisphosphate 3-kinase catalytic subunit beta isoform -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Phosphatidylinositol 4,5-bisphosphate 3-kinase catalytic subunit beta isoform -Log(IC50) M model"	1125	225	external test set validation 	0.8	0.599	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3155	5-hydroxytryptamine receptor 7 -Log(IC50) M	Regression	-Log(IC50) M	Target	5-hydroxytryptamine receptor 7 -Log(IC50) M model	215	43	external test set validation 	0.52	0.585	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3157	B2 bradykinin receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	B2 bradykinin receptor -Log(IC50) M model	225	45	external test set validation 	0.73	0.686	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3161	L-selectin -Log(IC50) M	Regression	-Log(IC50) M	Target	L-selectin -Log(IC50) M model	50	10	external test set validation 	0.37	0.814	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3166	Tyrosine-protein phosphatase non-receptor type 6 -Log(IC50) M	Regression	-Log(IC50) M	Target	Tyrosine-protein phosphatase non-receptor type 6 -Log(IC50) M model	185	37	external test set validation 	0.32	0.513	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3176	Galanin receptor type 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Galanin receptor type 2 -Log(IC50) M model	40	8	external test set validation 	0.4	0.333	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3180	Cocaine esterase -Log(IC50) M	Regression	-Log(IC50) M	Target	Cocaine esterase -Log(IC50) M model	55	11	external test set validation 	0.72	1.069	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3181	Estradiol 17-beta-dehydrogenase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Estradiol 17-beta-dehydrogenase 1 -Log(IC50) M model	365	73	external test set validation 	0.71	0.563	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3189	Adenylate cyclase type 5 -Log(IC50) M	Regression	-Log(IC50) M	Target	Adenylate cyclase type 5 -Log(IC50) M model	15	3	external test set validation 	0.59	0.414	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3192	Histone deacetylase 8 -Log(IC50) M	Regression	-Log(IC50) M	Target	Histone deacetylase 8 -Log(IC50) M model	960	192	external test set validation 	0.59	0.557	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3194	Transthyretin -Log(IC50) M	Regression	-Log(IC50) M	Target	Transthyretin -Log(IC50) M model	65	13	external test set validation 	0.06	0.651	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3202	Prolyl endopeptidase -Log(IC50) M	Regression	-Log(IC50) M	Target	Prolyl endopeptidase -Log(IC50) M model	230	46	external test set validation 	0.81	0.76	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL321	Matrix metalloproteinase-9 -Log(IC50) M	Regression	-Log(IC50) M	Target	Matrix metalloproteinase-9 -Log(IC50) M model	1570	314	external test set validation 	0.77	0.705	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3218	B-cell receptor CD22|B-cell receptor CD22|B-cell receptor CD22 -Log(IC50) M	Regression	-Log(IC50) M	Target	B-cell receptor CD22|B-cell receptor CD22|B-cell receptor CD22 -Log(IC50) M model	55	11	external test set validation 	0.8	0.753	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3227	Metabotropic glutamate receptor 5 -Log(IC50) M	Regression	-Log(IC50) M	Target	Metabotropic glutamate receptor 5 -Log(IC50) M model	1245	249	external test set validation 	0.47	0.703	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3230	Sphingosine 1-phosphate receptor 4 -Log(IC50) M	Regression	-Log(IC50) M	Target	Sphingosine 1-phosphate receptor 4 -Log(IC50) M model	215	43	external test set validation 	0.52	0.452	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3231	Rho-associated protein kinase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Rho-associated protein kinase 1 -Log(IC50) M model	405	81	external test set validation 	0.57	0.788	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3234	Tyrosine-protein kinase HCK -Log(IC50) M	Regression	-Log(IC50) M	Target	Tyrosine-protein kinase HCK -Log(IC50) M model	275	55	external test set validation 	0.6	0.768	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3235	"Isoleucine--tRNA ligase, cytoplasmic -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Isoleucine--tRNA ligase, cytoplasmic -Log(IC50) M model"	30	6	external test set validation 	0.71	0.688	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3236	Tyrosine-protein phosphatase non-receptor type 12 -Log(IC50) M	Regression	-Log(IC50) M	Target	Tyrosine-protein phosphatase non-receptor type 12 -Log(IC50) M model	95	19	external test set validation 	0.54	0.424	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3238	"Carnitine O-palmitoyltransferase 2, mitochondrial -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Carnitine O-palmitoyltransferase 2, mitochondrial -Log(IC50) M model"	220	44	external test set validation 	0.4	0.549	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3240	Cation-independent mannose-6-phosphate receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Cation-independent mannose-6-phosphate receptor -Log(IC50) M model	5	1	external test set validation 	nan	0.715	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3242	Carbonic anhydrase 12 -Log(IC50) M	Regression	-Log(IC50) M	Target	Carbonic anhydrase 12 -Log(IC50) M model	60	12	external test set validation 	0.14	0.792	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3243	Receptor-type tyrosine-protein phosphatase C -Log(IC50) M	Regression	-Log(IC50) M	Target	Receptor-type tyrosine-protein phosphatase C -Log(IC50) M model	215	43	external test set validation 	0.59	0.558	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL325	Histone deacetylase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Histone deacetylase 1 -Log(IC50) M model	2825	565	external test set validation 	0.75	0.537	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3250	Lysophosphatidic acid receptor 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Lysophosphatidic acid receptor 3 -Log(IC50) M model	60	12	external test set validation 	0.32	0.804	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3251	Nuclear factor NF-kappa-B p105 subunit|Nuclear factor NF-kappa-B p105 subunit -Log(IC50) M	Regression	-Log(IC50) M	Target	Nuclear factor NF-kappa-B p105 subunit|Nuclear factor NF-kappa-B p105 subunit -Log(IC50) M model	45	9	external test set validation 	0.78	0.466	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3255	Cyclic AMP-dependent transcription factor ATF-1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Cyclic AMP-dependent transcription factor ATF-1 -Log(IC50) M model	15	3	external test set validation 	0.23	0.41	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3258	"Leucine--tRNA ligase, cytoplasmic -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Leucine--tRNA ligase, cytoplasmic -Log(IC50) M model"	5	1	external test set validation 	nan	0.443	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3267	"Phosphatidylinositol 4,5-bisphosphate 3-kinase catalytic subunit gamma isoform -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Phosphatidylinositol 4,5-bisphosphate 3-kinase catalytic subunit gamma isoform -Log(IC50) M model"	1290	258	external test set validation 	0.6	0.664	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3268	Phosphatidylinositol 4-kinase beta -Log(IC50) M	Regression	-Log(IC50) M	Target	Phosphatidylinositol 4-kinase beta -Log(IC50) M model	135	27	external test set validation 	0.48	0.702	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3272	Cathepsin L2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Cathepsin L2 -Log(IC50) M model	140	28	external test set validation 	0.56	0.608	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3286	Urokinase-type plasminogen activator -Log(IC50) M	Regression	-Log(IC50) M	Target	Urokinase-type plasminogen activator -Log(IC50) M model	245	49	external test set validation 	0.83	0.599	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3286061	Sphingosine-1-phosphate lyase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Sphingosine-1-phosphate lyase 1 -Log(IC50) M model	45	9	external test set validation 	0.19	0.791	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3286067	DNA-directed RNA polymerase I subunit RPA1 -Log(IC50) M	Regression	-Log(IC50) M	Target	DNA-directed RNA polymerase I subunit RPA1 -Log(IC50) M model	25	5	external test set validation 	0.91	0.572	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3290	Ephrin type-B receptor 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Ephrin type-B receptor 2 -Log(IC50) M model	60	12	external test set validation 	0.31	1.413	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3305	Sex hormone-binding globulin -Log(IC50) M	Regression	-Log(IC50) M	Target	Sex hormone-binding globulin -Log(IC50) M model	15	3	external test set validation 	1	1.356	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3308	Caspase-6 -Log(IC50) M	Regression	-Log(IC50) M	Target	Caspase-6 -Log(IC50) M model	130	26	external test set validation 	0.8	0.486	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL331	Cyclin-dependent kinase 4 -Log(IC50) M	Regression	-Log(IC50) M	Target	Cyclin-dependent kinase 4 -Log(IC50) M model	975	195	external test set validation 	0.79	0.584	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3310	Histone deacetylase 11 -Log(IC50) M	Regression	-Log(IC50) M	Target	Histone deacetylase 11 -Log(IC50) M model	130	26	external test set validation 	0.11	1.4	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3313832	Lysine-specific demethylase 4B -Log(IC50) M	Regression	-Log(IC50) M	Target	Lysine-specific demethylase 4B -Log(IC50) M model	45	9	external test set validation 	0.9	0.481	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3317335	Sodium-dependent proline transporter -Log(IC50) M	Regression	-Log(IC50) M	Target	Sodium-dependent proline transporter -Log(IC50) M model	65	13	external test set validation 	0.48	0.558	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL332	Interstitial collagenase -Log(IC50) M	Regression	-Log(IC50) M	Target	Interstitial collagenase -Log(IC50) M model	1830	366	external test set validation 	0.69	0.668	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL333	72 kDa type IV collagenase -Log(IC50) M	Regression	-Log(IC50) M	Target	72 kDa type IV collagenase -Log(IC50) M model	2545	509	external test set validation 	0.85	0.649	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3338	Squalene synthase -Log(IC50) M	Regression	-Log(IC50) M	Target	Squalene synthase -Log(IC50) M model	180	36	external test set validation 	0.22	1.013	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3344	"Fatty acid-binding protein, heart -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Fatty acid-binding protein, heart -Log(IC50) M model"	20	4	external test set validation 	0.13	1.447	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL335	Tyrosine-protein phosphatase non-receptor type 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Tyrosine-protein phosphatase non-receptor type 1 -Log(IC50) M model	2150	430	external test set validation 	0.75	0.485	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3351	Acetyl-CoA carboxylase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Acetyl-CoA carboxylase 1 -Log(IC50) M model	320	64	external test set validation 	0.52	0.54	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3351200	Kinesin-like protein KIFC1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Kinesin-like protein KIFC1 -Log(IC50) M model	30	6	external test set validation 	0.6	0.514	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3356	Cytochrome P450 1A2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Cytochrome P450 1A2 -Log(IC50) M model	845	169	external test set validation 	0.44	0.71	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3359	fMet-Leu-Phe receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	fMet-Leu-Phe receptor -Log(IC50) M model	120	24	external test set validation 	0.28	0.563	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3363	Protein-glutamine gamma-glutamyltransferase E -Log(IC50) M	Regression	-Log(IC50) M	Target	Protein-glutamine gamma-glutamyltransferase E -Log(IC50) M model	20	4	external test set validation 	0.01	0.239	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3371	5-hydroxytryptamine receptor 6 -Log(IC50) M	Regression	-Log(IC50) M	Target	5-hydroxytryptamine receptor 6 -Log(IC50) M model	795	159	external test set validation 	0.71	0.557	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3378	Tumor necrosis factor receptor superfamily member 1A -Log(IC50) M	Regression	-Log(IC50) M	Target	Tumor necrosis factor receptor superfamily member 1A -Log(IC50) M model	75	15	external test set validation 	0.15	0.377	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3381	Small conductance calcium-activated potassium channel protein 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Small conductance calcium-activated potassium channel protein 3 -Log(IC50) M model	10	2	external test set validation 	1	0.309	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3384	Serine/threonine-protein kinase N1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Serine/threonine-protein kinase N1 -Log(IC50) M model	25	5	external test set validation 	0.24	1.326	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3385	Mitogen-activated protein kinase 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Mitogen-activated protein kinase 3 -Log(IC50) M model	75	15	external test set validation 	0.59	0.868	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3392950	Histone chaperone ASF1A -Log(IC50) M	Regression	-Log(IC50) M	Target	Histone chaperone ASF1A -Log(IC50) M model	10	2	external test set validation 	1	0.387	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3397	Cytochrome P450 2C9 -Log(IC50) M	Regression	-Log(IC50) M	Target	Cytochrome P450 2C9 -Log(IC50) M model	1640	328	external test set validation 	0.42	0.543	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL340	Cytochrome P450 3A4 -Log(IC50) M	Regression	-Log(IC50) M	Target	Cytochrome P450 3A4 -Log(IC50) M model	3115	623	external test set validation 	0.63	0.562	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3401	Nuclear receptor subfamily 1 group I member 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Nuclear receptor subfamily 1 group I member 2 -Log(IC50) M model	20	4	external test set validation 	0.7	0.905	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3402	"Alkaline phosphatase, placental-like -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Alkaline phosphatase, placental-like -Log(IC50) M model"	315	63	external test set validation 	0.49	0.667	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3407328	Pantothenate kinase 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Pantothenate kinase 3 -Log(IC50) M model	15	3	external test set validation 	0.16	0.531	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3414409	Macrophage-expressed gene 1 protein -Log(IC50) M	Regression	-Log(IC50) M	Target	Macrophage-expressed gene 1 protein -Log(IC50) M model	5	1	external test set validation 	nan	0	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3419	Carboxypeptidase B2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Carboxypeptidase B2 -Log(IC50) M model	125	25	external test set validation 	0.62	0.672	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3421524	"6-phosphofructo-2-kinase/fructose-2,6-bisphosphatase 1 -Log(IC50) M"	Regression	-Log(IC50) M	Target	"6-phosphofructo-2-kinase/fructose-2,6-bisphosphatase 1 -Log(IC50) M model"	30	6	external test set validation 	0.28	0.595	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3421525	"6-phosphofructo-2-kinase/fructose-2,6-bisphosphatase 2 -Log(IC50) M"	Regression	-Log(IC50) M	Target	"6-phosphofructo-2-kinase/fructose-2,6-bisphosphatase 2 -Log(IC50) M model"	30	6	external test set validation 	0.59	0.563	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3429	Steroid hormone receptor ERR1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Steroid hormone receptor ERR1 -Log(IC50) M model	95	19	external test set validation 	0.25	0.708	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3430888	Kinesin-1 heavy chain|Proto-oncogene tyrosine-protein kinase receptor Ret -Log(IC50) M	Regression	-Log(IC50) M	Target	Kinesin-1 heavy chain|Proto-oncogene tyrosine-protein kinase receptor Ret -Log(IC50) M model	50	10	external test set validation 	0.43	0.539	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3430907	Aurora kinase B|Inner centromere protein -Log(IC50) M	Regression	-Log(IC50) M	Target	Aurora kinase B|Inner centromere protein -Log(IC50) M model	45	9	external test set validation 	0.77	0.611	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3437	Membrane primary amine oxidase -Log(IC50) M	Regression	-Log(IC50) M	Target	Membrane primary amine oxidase -Log(IC50) M model	150	30	external test set validation 	0.46	0.853	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3438	Protein kinase C zeta type -Log(IC50) M	Regression	-Log(IC50) M	Target	Protein kinase C zeta type -Log(IC50) M model	155	31	external test set validation 	0.49	0.572	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL344	Melanin-concentrating hormone receptor 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Melanin-concentrating hormone receptor 1 -Log(IC50) M model	1965	393	external test set validation 	0.66	0.623	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3468	Caspase-7 -Log(IC50) M	Regression	-Log(IC50) M	Target	Caspase-7 -Log(IC50) M model	360	72	external test set validation 	0.76	0.612	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3473	C-C chemokine receptor type 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	C-C chemokine receptor type 3 -Log(IC50) M model	685	137	external test set validation 	0.77	0.609	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3474	"Phospholipase A2, membrane associated -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Phospholipase A2, membrane associated -Log(IC50) M model"	345	69	external test set validation 	0.75	0.655	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3475	Plasminogen activator inhibitor 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Plasminogen activator inhibitor 1 -Log(IC50) M model	260	52	external test set validation 	0.56	0.631	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3476	Inhibitor of nuclear factor kappa-B kinase subunit alpha -Log(IC50) M	Regression	-Log(IC50) M	Target	Inhibitor of nuclear factor kappa-B kinase subunit alpha -Log(IC50) M model	345	69	external test set validation 	0.56	0.522	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3491	Cytochrome P450 2J2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Cytochrome P450 2J2 -Log(IC50) M model	75	15	external test set validation 	0.05	0.575	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3492	Proteasome subunit beta type-2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Proteasome subunit beta type-2 -Log(IC50) M model	55	11	external test set validation 	0.89	0.444	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3510	Carbonic anhydrase 14 -Log(IC50) M	Regression	-Log(IC50) M	Target	Carbonic anhydrase 14 -Log(IC50) M model	35	7	external test set validation 	0.13	0.638	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3514	Platelet-activating factor acetylhydrolase -Log(IC50) M	Regression	-Log(IC50) M	Target	Platelet-activating factor acetylhydrolase -Log(IC50) M model	335	67	external test set validation 	0.81	0.558	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3521	Receptor-type tyrosine-protein phosphatase F|Receptor-type tyrosine-protein phosphatase F -Log(IC50) M	Regression	-Log(IC50) M	Target	Receptor-type tyrosine-protein phosphatase F|Receptor-type tyrosine-protein phosphatase F -Log(IC50) M model	110	22	external test set validation 	0.66	0.387	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3522	"Steroid 17-alpha-hydroxylase/17,20 lyase -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Steroid 17-alpha-hydroxylase/17,20 lyase -Log(IC50) M model"	485	97	external test set validation 	0.52	0.647	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3524	Histone deacetylase 4 -Log(IC50) M	Regression	-Log(IC50) M	Target	Histone deacetylase 4 -Log(IC50) M model	475	95	external test set validation 	0.67	0.689	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3526	"Glycogen phosphorylase, muscle form -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Glycogen phosphorylase, muscle form -Log(IC50) M model"	75	15	external test set validation 	0.71	0.566	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3529	Inhibitor of nuclear factor kappa-B kinase subunit epsilon -Log(IC50) M	Regression	-Log(IC50) M	Target	Inhibitor of nuclear factor kappa-B kinase subunit epsilon -Log(IC50) M model	170	34	external test set validation 	0.65	0.532	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3530	Dual specificity mitogen-activated protein kinase kinase 7 -Log(IC50) M	Regression	-Log(IC50) M	Target	Dual specificity mitogen-activated protein kinase kinase 7 -Log(IC50) M model	30	6	external test set validation 	0.3	0.252	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3535	"High affinity cGMP-specific 3',5'-cyclic phosphodiesterase 9A -Log(IC50) M"	Regression	-Log(IC50) M	Target	"High affinity cGMP-specific 3',5'-cyclic phosphodiesterase 9A -Log(IC50) M model"	150	30	external test set validation 	0.54	0.52	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3553	Non-receptor tyrosine-protein kinase TYK2|Non-receptor tyrosine-protein kinase TYK2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Non-receptor tyrosine-protein kinase TYK2|Non-receptor tyrosine-protein kinase TYK2 -Log(IC50) M model	275	55	external test set validation 	0.39	0.724	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3559	Steryl-sulfatase -Log(IC50) M	Regression	-Log(IC50) M	Target	Steryl-sulfatase -Log(IC50) M model	270	54	external test set validation 	0.75	0.662	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3561	Gamma-aminobutyric acid receptor subunit rho-1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Gamma-aminobutyric acid receptor subunit rho-1 -Log(IC50) M model	20	4	external test set validation 	0.42	0.541	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3568	"Nitric oxide synthase, brain -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Nitric oxide synthase, brain -Log(IC50) M model"	630	126	external test set validation 	0.77	0.543	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3572	Cholesteryl ester transfer protein -Log(IC50) M	Regression	-Log(IC50) M	Target	Cholesteryl ester transfer protein -Log(IC50) M model	780	156	external test set validation 	0.61	0.685	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3577	Retinal dehydrogenase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Retinal dehydrogenase 1 -Log(IC50) M model	35	7	external test set validation 	0.51	0.445	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3578	"Aldehyde dehydrogenase, dimeric NADP-preferring -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Aldehyde dehydrogenase, dimeric NADP-preferring -Log(IC50) M model"	15	3	external test set validation 	0.8	0.488	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3582	Protein kinase C epsilon type -Log(IC50) M	Regression	-Log(IC50) M	Target	Protein kinase C epsilon type -Log(IC50) M model	205	41	external test set validation 	0.59	0.881	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3587	Dual specificity mitogen-activated protein kinase kinase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Dual specificity mitogen-activated protein kinase kinase 1 -Log(IC50) M model	595	119	external test set validation 	0.57	0.648	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3589	Adenosine kinase -Log(IC50) M	Regression	-Log(IC50) M	Target	Adenosine kinase -Log(IC50) M model	325	65	external test set validation 	0.73	0.766	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3590	Hormone-sensitive lipase -Log(IC50) M	Regression	-Log(IC50) M	Target	Hormone-sensitive lipase -Log(IC50) M model	275	55	external test set validation 	0.43	0.695	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3592	Squalene monooxygenase -Log(IC50) M	Regression	-Log(IC50) M	Target	Squalene monooxygenase -Log(IC50) M model	50	10	external test set validation 	0.81	0.886	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3593	Lanosterol synthase -Log(IC50) M	Regression	-Log(IC50) M	Target	Lanosterol synthase -Log(IC50) M model	90	18	external test set validation 	0.58	0.736	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3594	Carbonic anhydrase 9 -Log(IC50) M	Regression	-Log(IC50) M	Target	Carbonic anhydrase 9 -Log(IC50) M model	80	16	external test set validation 	0.7	0.845	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3616	Protein kinase C eta type -Log(IC50) M	Regression	-Log(IC50) M	Target	Protein kinase C eta type -Log(IC50) M model	170	34	external test set validation 	0.17	1.091	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3616354	"Branched-chain-amino-acid aminotransferase, mitochondrial -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Branched-chain-amino-acid aminotransferase, mitochondrial -Log(IC50) M model"	80	16	external test set validation 	0.67	0.715	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3616361	CCR4-NOT transcription complex subunit 7 -Log(IC50) M	Regression	-Log(IC50) M	Target	CCR4-NOT transcription complex subunit 7 -Log(IC50) M model	25	5	external test set validation 	0.97	0.685	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3622	Cytochrome P450 2C19 -Log(IC50) M	Regression	-Log(IC50) M	Target	Cytochrome P450 2C19 -Log(IC50) M model	980	196	external test set validation 	0.32	0.625	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3623	NAD(P)H dehydrogenase [quinone] 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	NAD(P)H dehydrogenase [quinone] 1 -Log(IC50) M model	70	14	external test set validation 	0.64	0.574	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3629	Casein kinase II subunit alpha -Log(IC50) M	Regression	-Log(IC50) M	Target	Casein kinase II subunit alpha -Log(IC50) M model	385	77	external test set validation 	0.78	0.609	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3632452	Mucosa-associated lymphoid tissue lymphoma translocation protein 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Mucosa-associated lymphoid tissue lymphoma translocation protein 1 -Log(IC50) M model	60	12	external test set validation 	0.54	0.484	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3636	Neuromedin-B receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Neuromedin-B receptor -Log(IC50) M model	35	7	external test set validation 	0.74	0.707	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3650	Fibroblast growth factor receptor 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Fibroblast growth factor receptor 1 -Log(IC50) M model	1140	228	external test set validation 	0.81	0.511	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3660	Integrin alpha-V -Log(IC50) M	Regression	-Log(IC50) M	Target	Integrin alpha-V -Log(IC50) M model	40	8	external test set validation 	0.64	0.959	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3663	Growth factor receptor-bound protein 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Growth factor receptor-bound protein 2 -Log(IC50) M model	205	41	external test set validation 	0.87	0.688	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3667	Phosphatidylinositol 4-kinase alpha -Log(IC50) M	Regression	-Log(IC50) M	Target	Phosphatidylinositol 4-kinase alpha -Log(IC50) M model	35	7	external test set validation 	0.76	0.586	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3683	"Glutamate receptor ionotropic, kainate 2 -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Glutamate receptor ionotropic, kainate 2 -Log(IC50) M model"	10	2	external test set validation 	1	0.346	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3687	"Arachidonate 12-lipoxygenase, 12S-type -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Arachidonate 12-lipoxygenase, 12S-type -Log(IC50) M model"	125	25	external test set validation 	0.13	0.611	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3691	Ectonucleotide pyrophosphatase/phosphodiesterase family member 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Ectonucleotide pyrophosphatase/phosphodiesterase family member 2 -Log(IC50) M model	130	26	external test set validation 	0.5	0.777	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3706	Disintegrin and metalloproteinase domain-containing protein 17 -Log(IC50) M	Regression	-Log(IC50) M	Target	Disintegrin and metalloproteinase domain-containing protein 17 -Log(IC50) M model	1005	201	external test set validation 	0.72	0.546	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3710	Prostaglandin E2 receptor EP3 subtype -Log(IC50) M	Regression	-Log(IC50) M	Target	Prostaglandin E2 receptor EP3 subtype -Log(IC50) M model	205	41	external test set validation 	0.37	0.78	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3713687	Cytochrome P450 26B1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Cytochrome P450 26B1 -Log(IC50) M model	25	5	external test set validation 	0.01	0.622	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3714130	G-protein coupled receptor 6 -Log(IC50) M	Regression	-Log(IC50) M	Target	G-protein coupled receptor 6 -Log(IC50) M model	300	60	external test set validation 	0.6	0.654	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3715	Sodium- and chloride-dependent betaine transporter -Log(IC50) M	Regression	-Log(IC50) M	Target	Sodium- and chloride-dependent betaine transporter -Log(IC50) M model	10	2	external test set validation 	1	1.397	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3717	Hepatocyte growth factor receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Hepatocyte growth factor receptor -Log(IC50) M model	2340	468	external test set validation 	0.76	0.568	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3721	Cytochrome P450 2C8 -Log(IC50) M	Regression	-Log(IC50) M	Target	Cytochrome P450 2C8 -Log(IC50) M model	180	36	external test set validation 	0.4	0.687	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3724	Lysophosphatidic acid receptor 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Lysophosphatidic acid receptor 2 -Log(IC50) M model	50	10	external test set validation 	0.68	0.86	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3730	Dihydropteridine reductase -Log(IC50) M	Regression	-Log(IC50) M	Target	Dihydropteridine reductase -Log(IC50) M model	10	2	external test set validation 	1	0.447	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3735	Vascular cell adhesion protein 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Vascular cell adhesion protein 1 -Log(IC50) M model	90	18	external test set validation 	0.64	0.516	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3746	Corticosteroid 11-beta-dehydrogenase isozyme 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Corticosteroid 11-beta-dehydrogenase isozyme 2 -Log(IC50) M model	140	28	external test set validation 	0.46	0.847	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3751647	Telomeric repeat-binding factor 2-interacting protein 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Telomeric repeat-binding factor 2-interacting protein 1 -Log(IC50) M model	15	3	external test set validation 	0.09	0.469	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3759	Histamine H4 receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Histamine H4 receptor -Log(IC50) M model	40	8	external test set validation 	0.02	1.196	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3761	Non-lysosomal glucosylceramidase -Log(IC50) M	Regression	-Log(IC50) M	Target	Non-lysosomal glucosylceramidase -Log(IC50) M model	70	14	external test set validation 	0.92	0.659	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3764	Urotensin-2 receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Urotensin-2 receptor -Log(IC50) M model	115	23	external test set validation 	0.32	0.606	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3772	Metabotropic glutamate receptor 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Metabotropic glutamate receptor 1 -Log(IC50) M model	625	125	external test set validation 	0.79	0.578	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3774295	Lysine-specific demethylase 5B -Log(IC50) M	Regression	-Log(IC50) M	Target	Lysine-specific demethylase 5B -Log(IC50) M model	55	11	external test set validation 	0.79	0.522	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3775	M-phase inducer phosphatase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	M-phase inducer phosphatase 1 -Log(IC50) M model	185	37	external test set validation 	0.15	0.436	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3776	Caspase-8 -Log(IC50) M	Regression	-Log(IC50) M	Target	Caspase-8 -Log(IC50) M model	185	37	external test set validation 	0.66	0.65	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3778	Interleukin-1 receptor-associated kinase 4 -Log(IC50) M	Regression	-Log(IC50) M	Target	Interleukin-1 receptor-associated kinase 4 -Log(IC50) M model	265	53	external test set validation 	0.54	0.833	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3784	Histone acetyltransferase p300|Histone acetyltransferase p300 -Log(IC50) M	Regression	-Log(IC50) M	Target	Histone acetyltransferase p300|Histone acetyltransferase p300 -Log(IC50) M model	55	11	external test set validation 	0.07	0.653	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3785	Hydroxycarboxylic acid receptor 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Hydroxycarboxylic acid receptor 2 -Log(IC50) M model	215	43	external test set validation 	0.71	0.655	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3788	Serine/threonine-protein kinase PLK4 -Log(IC50) M	Regression	-Log(IC50) M	Target	Serine/threonine-protein kinase PLK4 -Log(IC50) M model	75	15	external test set validation 	0.85	0.68	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3795	Melanocyte-stimulating hormone receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Melanocyte-stimulating hormone receptor -Log(IC50) M model	100	20	external test set validation 	0.43	0.929	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3797	Serine-protein kinase ATM -Log(IC50) M	Regression	-Log(IC50) M	Target	Serine-protein kinase ATM -Log(IC50) M model	100	20	external test set validation 	0.18	0.664	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3798	Calcitonin gene-related peptide type 1 receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Calcitonin gene-related peptide type 1 receptor -Log(IC50) M model	410	82	external test set validation 	0.88	0.583	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3807	Tyrosine-protein phosphatase non-receptor type 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Tyrosine-protein phosphatase non-receptor type 2 -Log(IC50) M model	490	98	external test set validation 	0.65	0.519	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3816	Cytosolic phospholipase A2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Cytosolic phospholipase A2 -Log(IC50) M model	320	64	external test set validation 	0.62	0.648	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3817	Bcl2-associated agonist of cell death -Log(IC50) M	Regression	-Log(IC50) M	Target	Bcl2-associated agonist of cell death -Log(IC50) M model	40	8	external test set validation 	0.37	0.451	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3819	Lysophosphatidic acid receptor 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Lysophosphatidic acid receptor 1 -Log(IC50) M model	120	24	external test set validation 	0.24	0.831	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3831281	P2X purinoceptor 3|P2X purinoceptor 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	P2X purinoceptor 3|P2X purinoceptor 2 -Log(IC50) M model	105	21	external test set validation 	0.06	0.584	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3832646	"6-phosphofructo-2-kinase/fructose-2,6-bisphosphatase 3|6-phosphofructo-2-kinase/fructose-2,6-bisphosphatase 4 -Log(IC50) M"	Regression	-Log(IC50) M	Target	"6-phosphofructo-2-kinase/fructose-2,6-bisphosphatase 3|6-phosphofructo-2-kinase/fructose-2,6-bisphosphatase 4 -Log(IC50) M model"	10	2	external test set validation 	1	0.466	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3834	Cytoplasmic tyrosine-protein kinase BMX -Log(IC50) M	Regression	-Log(IC50) M	Target	Cytoplasmic tyrosine-protein kinase BMX -Log(IC50) M model	50	10	external test set validation 	0.76	0.973	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3835	Serine/threonine-protein kinase Nek2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Serine/threonine-protein kinase Nek2 -Log(IC50) M model	85	17	external test set validation 	0.55	0.618	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3836	LIM domain kinase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	LIM domain kinase 1 -Log(IC50) M model	170	34	external test set validation 	0.52	0.572	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3837	Cathepsin L1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Cathepsin L1 -Log(IC50) M model	870	174	external test set validation 	0.71	0.684	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3849	Lanosterol 14-alpha demethylase -Log(IC50) M	Regression	-Log(IC50) M	Target	Lanosterol 14-alpha demethylase -Log(IC50) M model	30	6	external test set validation 	0.25	0.863	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3861	"[Pyruvate dehydrogenase (acetyl-transferring)] kinase isozyme 2, mitochondrial -Log(IC50) M"	Regression	-Log(IC50) M	Target	"[Pyruvate dehydrogenase (acetyl-transferring)] kinase isozyme 2, mitochondrial -Log(IC50) M model"	555	111	external test set validation 	0.9	0.276	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3863	Serine/threonine-protein kinase D1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Serine/threonine-protein kinase D1 -Log(IC50) M model	85	17	external test set validation 	0.61	0.6	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3864	Tyrosine-protein phosphatase non-receptor type 11 -Log(IC50) M	Regression	-Log(IC50) M	Target	Tyrosine-protein phosphatase non-receptor type 11 -Log(IC50) M model	430	86	external test set validation 	0.3	0.544	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3869	Matrix metalloproteinase-14 -Log(IC50) M	Regression	-Log(IC50) M	Target	Matrix metalloproteinase-14 -Log(IC50) M model	570	114	external test set validation 	0.72	0.658	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3880	Heat shock protein HSP 90-alpha -Log(IC50) M	Regression	-Log(IC50) M	Target	Heat shock protein HSP 90-alpha -Log(IC50) M model	690	138	external test set validation 	0.76	0.429	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3884	Sodium/glucose cotransporter 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Sodium/glucose cotransporter 2 -Log(IC50) M model	945	189	external test set validation 	0.71	0.619	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3886	Mitogen-activated protein kinase kinase kinase MLT -Log(IC50) M	Regression	-Log(IC50) M	Target	Mitogen-activated protein kinase kinase kinase MLT -Log(IC50) M model	10	2	external test set validation 	1	0.471	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3890	E-selectin -Log(IC50) M	Regression	-Log(IC50) M	Target	E-selectin -Log(IC50) M model	195	39	external test set validation 	0.88	0.594	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3891	Calpain-1 catalytic subunit -Log(IC50) M	Regression	-Log(IC50) M	Target	Calpain-1 catalytic subunit -Log(IC50) M model	305	61	external test set validation 	0.5	0.69	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3892	Sphingosine 1-phosphate receptor 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Sphingosine 1-phosphate receptor 3 -Log(IC50) M model	315	63	external test set validation 	0.38	0.877	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3898	Bone morphogenetic protein 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Bone morphogenetic protein 1 -Log(IC50) M model	350	70	external test set validation 	0.79	0.57	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3900	Myeloblastin -Log(IC50) M	Regression	-Log(IC50) M	Target	Myeloblastin -Log(IC50) M model	10	2	external test set validation 	1	0.226	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3902	Glutathione S-transferase P -Log(IC50) M	Regression	-Log(IC50) M	Target	Glutathione S-transferase P -Log(IC50) M model	120	24	external test set validation 	0.76	0.52	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3905	Tyrosine-protein kinase Lyn -Log(IC50) M	Regression	-Log(IC50) M	Target	Tyrosine-protein kinase Lyn -Log(IC50) M model	220	44	external test set validation 	0.64	0.715	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3906	Ribosomal protein S6 kinase alpha-2|Ribosomal protein S6 kinase alpha-2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Ribosomal protein S6 kinase alpha-2|Ribosomal protein S6 kinase alpha-2 -Log(IC50) M model	40	8	external test set validation 	0.21	0.776	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3911	Leukotriene B4 receptor 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Leukotriene B4 receptor 1 -Log(IC50) M model	185	37	external test set validation 	0.72	0.681	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3918	Receptor-type tyrosine-protein phosphatase alpha -Log(IC50) M	Regression	-Log(IC50) M	Target	Receptor-type tyrosine-protein phosphatase alpha -Log(IC50) M model	20	4	external test set validation 	0.49	0.365	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3920	Protein kinase C theta type -Log(IC50) M	Regression	-Log(IC50) M	Target	Protein kinase C theta type -Log(IC50) M model	445	89	external test set validation 	0.71	0.676	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3921	Heparanase -Log(IC50) M	Regression	-Log(IC50) M	Target	Heparanase -Log(IC50) M model	125	25	external test set validation 	0.62	0.373	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3922	Methionine aminopeptidase 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Methionine aminopeptidase 2 -Log(IC50) M model	260	52	external test set validation 	0.73	0.515	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3959	Ribosyldihydronicotinamide dehydrogenase [quinone] -Log(IC50) M	Regression	-Log(IC50) M	Target	Ribosyldihydronicotinamide dehydrogenase [quinone] -Log(IC50) M model	245	49	external test set validation 	0.75	0.622	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3961	Mitogen-activated protein kinase 11 -Log(IC50) M	Regression	-Log(IC50) M	Target	Mitogen-activated protein kinase 11 -Log(IC50) M model	130	26	external test set validation 	0.64	0.664	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3964	"1-phosphatidylinositol 4,5-bisphosphate phosphodiesterase gamma-1 -Log(IC50) M"	Regression	-Log(IC50) M	Target	"1-phosphatidylinositol 4,5-bisphosphate phosphodiesterase gamma-1 -Log(IC50) M model"	5	1	external test set validation 	nan	0.159	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3965	Cytosol aminopeptidase -Log(IC50) M	Regression	-Log(IC50) M	Target	Cytosol aminopeptidase -Log(IC50) M model	10	2	external test set validation 	1	0.605	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3972	Trifunctional purine biosynthetic protein adenosine-3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Trifunctional purine biosynthetic protein adenosine-3 -Log(IC50) M model	40	8	external test set validation 	0.52	1.278	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3973	Fibroblast growth factor receptor 4 -Log(IC50) M	Regression	-Log(IC50) M	Target	Fibroblast growth factor receptor 4 -Log(IC50) M model	50	10	external test set validation 	0.59	0.876	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3974	Proteinase-activated receptor 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Proteinase-activated receptor 1 -Log(IC50) M model	650	130	external test set validation 	0.58	0.62	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3975	"Fructose-1,6-bisphosphatase 1 -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Fructose-1,6-bisphosphatase 1 -Log(IC50) M model"	390	78	external test set validation 	0.83	0.515	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3976	Dipeptidyl peptidase 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Dipeptidyl peptidase 2 -Log(IC50) M model	1095	219	external test set validation 	0.76	0.539	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3979	Peroxisome proliferator-activated receptor delta -Log(IC50) M	Regression	-Log(IC50) M	Target	Peroxisome proliferator-activated receptor delta -Log(IC50) M model	335	67	external test set validation 	0.79	0.516	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3980	Serine/threonine-protein kinase 17B -Log(IC50) M	Regression	-Log(IC50) M	Target	Serine/threonine-protein kinase 17B -Log(IC50) M model	10	2	external test set validation 	1	0.59	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3983	Dual specificity protein kinase TTK -Log(IC50) M	Regression	-Log(IC50) M	Target	Dual specificity protein kinase TTK -Log(IC50) M model	300	60	external test set validation 	0.6	0.722	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL3991	Coagulation factor VII -Log(IC50) M	Regression	-Log(IC50) M	Target	Coagulation factor VII -Log(IC50) M model	150	30	external test set validation 	0.57	0.8	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4005	"Phosphatidylinositol 4,5-bisphosphate 3-kinase catalytic subunit alpha isoform -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Phosphatidylinositol 4,5-bisphosphate 3-kinase catalytic subunit alpha isoform -Log(IC50) M model"	3345	669	external test set validation 	0.72	0.562	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4015	C-C chemokine receptor type 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	C-C chemokine receptor type 2 -Log(IC50) M model	1345	269	external test set validation 	0.67	0.675	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4018	Neuropeptide Y receptor type 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Neuropeptide Y receptor type 2 -Log(IC50) M model	340	68	external test set validation 	0.82	0.538	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL402	3-hydroxy-3-methylglutaryl-coenzyme A reductase -Log(IC50) M	Regression	-Log(IC50) M	Target	3-hydroxy-3-methylglutaryl-coenzyme A reductase -Log(IC50) M model	170	34	external test set validation 	0.83	0.745	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4026	Signal transducer and activator of transcription 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Signal transducer and activator of transcription 3 -Log(IC50) M model	645	129	external test set validation 	0.51	0.445	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4029	C-X-C chemokine receptor type 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	C-X-C chemokine receptor type 1 -Log(IC50) M model	205	41	external test set validation 	0.5	0.547	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4036	Cyclin-dependent-like kinase 5 -Log(IC50) M	Regression	-Log(IC50) M	Target	Cyclin-dependent-like kinase 5 -Log(IC50) M model	130	26	external test set validation 	0.49	0.884	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4039	Choline O-acetyltransferase -Log(IC50) M	Regression	-Log(IC50) M	Target	Choline O-acetyltransferase -Log(IC50) M model	30	6	external test set validation 	0	1.335	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4040	Mitogen-activated protein kinase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Mitogen-activated protein kinase 1 -Log(IC50) M model	960	192	external test set validation 	0.82	0.62	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4050	Peptidyl-prolyl cis-trans isomerase FKBP4 -Log(IC50) M	Regression	-Log(IC50) M	Target	Peptidyl-prolyl cis-trans isomerase FKBP4 -Log(IC50) M model	40	8	external test set validation 	0.05	1.159	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4051	Cystic fibrosis transmembrane conductance regulator -Log(IC50) M	Regression	-Log(IC50) M	Target	Cystic fibrosis transmembrane conductance regulator -Log(IC50) M model	135	27	external test set validation 	0.49	0.702	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4059	Lysosomal alpha-mannosidase -Log(IC50) M	Regression	-Log(IC50) M	Target	Lysosomal alpha-mannosidase -Log(IC50) M model	10	2	external test set validation 	1	1.872	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4068	Chymase -Log(IC50) M	Regression	-Log(IC50) M	Target	Chymase -Log(IC50) M model	260	52	external test set validation 	0.65	0.731	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4069	Corticotropin-releasing factor receptor 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Corticotropin-releasing factor receptor 2 -Log(IC50) M model	50	10	external test set validation 	0.51	1.032	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4070	Casein kinase II subunit alpha' -Log(IC50) M	Regression	-Log(IC50) M	Target	Casein kinase II subunit alpha' -Log(IC50) M model	150	30	external test set validation 	0.91	0.49	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4071	Cathepsin G -Log(IC50) M	Regression	-Log(IC50) M	Target	Cathepsin G -Log(IC50) M model	105	21	external test set validation 	0.23	1.041	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4072	Cathepsin B -Log(IC50) M	Regression	-Log(IC50) M	Target	Cathepsin B -Log(IC50) M model	640	128	external test set validation 	0.73	0.566	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4073	Matrilysin -Log(IC50) M	Regression	-Log(IC50) M	Target	Matrilysin -Log(IC50) M model	295	59	external test set validation 	0.76	0.487	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4076	Sodium/calcium exchanger 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Sodium/calcium exchanger 1 -Log(IC50) M model	340	68	external test set validation 	0.37	0.439	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4079	Beta-adrenergic receptor kinase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Beta-adrenergic receptor kinase 1 -Log(IC50) M model	75	15	external test set validation 	0.84	0.575	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4081	Tissue factor|Tissue factor -Log(IC50) M	Regression	-Log(IC50) M	Target	Tissue factor|Tissue factor -Log(IC50) M model	95	19	external test set validation 	0.9	0.485	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4093	Oxysterols receptor LXR-beta -Log(IC50) M	Regression	-Log(IC50) M	Target	Oxysterols receptor LXR-beta -Log(IC50) M model	510	102	external test set validation 	0.78	0.648	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4096	Cellular tumor antigen p53 -Log(IC50) M	Regression	-Log(IC50) M	Target	Cellular tumor antigen p53 -Log(IC50) M model	30	6	external test set validation 	0.23	0.601	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4101	cAMP-dependent protein kinase catalytic subunit alpha -Log(IC50) M	Regression	-Log(IC50) M	Target	cAMP-dependent protein kinase catalytic subunit alpha -Log(IC50) M model	55	11	external test set validation 	0.1	1.36	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4112	Retina-specific copper amine oxidase -Log(IC50) M	Regression	-Log(IC50) M	Target	Retina-specific copper amine oxidase -Log(IC50) M model	5	1	external test set validation 	nan	2.569	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4116	Alcohol dehydrogenase class-3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Alcohol dehydrogenase class-3 -Log(IC50) M model	20	4	external test set validation 	0.75	0.81	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4128	Angiopoietin-1 receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Angiopoietin-1 receptor -Log(IC50) M model	550	110	external test set validation 	0.58	0.726	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4135	Geranylgeranyl transferase type-1 subunit beta -Log(IC50) M	Regression	-Log(IC50) M	Target	Geranylgeranyl transferase type-1 subunit beta -Log(IC50) M model	60	12	external test set validation 	0.14	1.13	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4136	Cytosolic phospholipase A2 beta -Log(IC50) M	Regression	-Log(IC50) M	Target	Cytosolic phospholipase A2 beta -Log(IC50) M model	30	6	external test set validation 	0.81	0.289	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4142	Fibroblast growth factor receptor 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Fibroblast growth factor receptor 2 -Log(IC50) M model	240	48	external test set validation 	0.75	0.599	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4145	Histone deacetylase 9 -Log(IC50) M	Regression	-Log(IC50) M	Target	Histone deacetylase 9 -Log(IC50) M model	130	26	external test set validation 	0.65	0.799	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4147	Calcium/calmodulin-dependent protein kinase type II subunit alpha -Log(IC50) M	Regression	-Log(IC50) M	Target	Calcium/calmodulin-dependent protein kinase type II subunit alpha -Log(IC50) M model	5	1	external test set validation 	nan	0	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4158	Fatty acid synthase -Log(IC50) M	Regression	-Log(IC50) M	Target	Fatty acid synthase -Log(IC50) M model	540	108	external test set validation 	0.6	0.58	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4162	Protein tyrosine phosphatase type IVA 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Protein tyrosine phosphatase type IVA 3 -Log(IC50) M model	40	8	external test set validation 	0.6	0.435	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4163	Toll-like receptor 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Toll-like receptor 2 -Log(IC50) M model	5	1	external test set validation 	nan	1.154	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4174	Sialidase-4 -Log(IC50) M	Regression	-Log(IC50) M	Target	Sialidase-4 -Log(IC50) M model	25	5	external test set validation 	0.56	0.634	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4176	Tissue alpha-L-fucosidase -Log(IC50) M	Regression	-Log(IC50) M	Target	Tissue alpha-L-fucosidase -Log(IC50) M model	75	15	external test set validation 	0.65	0.973	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4179	Mitogen-activated protein kinase 9 -Log(IC50) M	Regression	-Log(IC50) M	Target	Mitogen-activated protein kinase 9 -Log(IC50) M model	630	126	external test set validation 	0.64	0.614	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4181	S-adenosylmethionine decarboxylase proenzyme -Log(IC50) M	Regression	-Log(IC50) M	Target	S-adenosylmethionine decarboxylase proenzyme -Log(IC50) M model	5	1	external test set validation 	nan	0.308	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4187	Sodium channel protein type 2 subunit alpha -Log(IC50) M	Regression	-Log(IC50) M	Target	Sodium channel protein type 2 subunit alpha -Log(IC50) M model	90	18	external test set validation 	0.09	0.579	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4191	Monoglyceride lipase -Log(IC50) M	Regression	-Log(IC50) M	Target	Monoglyceride lipase -Log(IC50) M model	310	62	external test set validation 	0.67	0.739	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4198	E3 ubiquitin-protein ligase XIAP -Log(IC50) M	Regression	-Log(IC50) M	Target	E3 ubiquitin-protein ligase XIAP -Log(IC50) M model	565	113	external test set validation 	0.84	0.417	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4203	Dual specificity protein kinase CLK4 -Log(IC50) M	Regression	-Log(IC50) M	Target	Dual specificity protein kinase CLK4 -Log(IC50) M model	135	27	external test set validation 	0.86	0.447	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4204	MAP kinase-interacting serine/threonine-protein kinase 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	MAP kinase-interacting serine/threonine-protein kinase 2 -Log(IC50) M model	195	39	external test set validation 	0.55	0.492	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4208	Proteasome subunit beta type-1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Proteasome subunit beta type-1 -Log(IC50) M model	50	10	external test set validation 	0.84	0.35	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4224	Dual specificity protein kinase CLK1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Dual specificity protein kinase CLK1 -Log(IC50) M model	215	43	external test set validation 	0.62	0.485	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4225	Dual specificity protein kinase CLK2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Dual specificity protein kinase CLK2 -Log(IC50) M model	55	11	external test set validation 	0.42	0.471	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4226	Dual specificity protein kinase CLK3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Dual specificity protein kinase CLK3 -Log(IC50) M model	25	5	external test set validation 	0.18	0.255	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4234	Testosterone 17-beta-dehydrogenase 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Testosterone 17-beta-dehydrogenase 3 -Log(IC50) M model	185	37	external test set validation 	0.57	0.742	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4235	Corticosteroid 11-beta-dehydrogenase isozyme 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Corticosteroid 11-beta-dehydrogenase isozyme 1 -Log(IC50) M model	2290	458	external test set validation 	0.66	0.661	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4237	Ribosomal protein S6 kinase alpha-5|Ribosomal protein S6 kinase alpha-5 -Log(IC50) M	Regression	-Log(IC50) M	Target	Ribosomal protein S6 kinase alpha-5|Ribosomal protein S6 kinase alpha-5 -Log(IC50) M model	75	15	external test set validation 	0.41	1.019	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4244	Legumain -Log(IC50) M	Regression	-Log(IC50) M	Target	Legumain -Log(IC50) M model	65	13	external test set validation 	0.41	0.715	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4247	ALK tyrosine kinase receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	ALK tyrosine kinase receptor -Log(IC50) M model	785	157	external test set validation 	0.82	0.609	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4261	Hypoxia-inducible factor 1-alpha -Log(IC50) M	Regression	-Log(IC50) M	Target	Hypoxia-inducible factor 1-alpha -Log(IC50) M model	155	31	external test set validation 	0.68	0.608	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4267	TGF-beta receptor type-2 -Log(IC50) M	Regression	-Log(IC50) M	Target	TGF-beta receptor type-2 -Log(IC50) M model	45	9	external test set validation 	0.86	0.534	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4270	Stromelysin-2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Stromelysin-2 -Log(IC50) M model	35	7	external test set validation 	0.74	0.873	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4282	RAC-alpha serine/threonine-protein kinase -Log(IC50) M	Regression	-Log(IC50) M	Target	RAC-alpha serine/threonine-protein kinase -Log(IC50) M model	2090	418	external test set validation 	0.77	0.584	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4296	Sodium channel protein type 9 subunit alpha -Log(IC50) M	Regression	-Log(IC50) M	Target	Sodium channel protein type 9 subunit alpha -Log(IC50) M model	2950	590	external test set validation 	0.71	0.519	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4302	Multidrug resistance protein 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Multidrug resistance protein 1 -Log(IC50) M model	630	126	external test set validation 	0.66	0.614	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4303	Heat shock protein HSP 90-beta -Log(IC50) M	Regression	-Log(IC50) M	Target	Heat shock protein HSP 90-beta -Log(IC50) M model	355	71	external test set validation 	0.69	0.625	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4304	Calcium-activated potassium channel subunit alpha-1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Calcium-activated potassium channel subunit alpha-1 -Log(IC50) M model	75	15	external test set validation 	0.32	0.779	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4305	Intermediate conductance calcium-activated potassium channel protein 4 -Log(IC50) M	Regression	-Log(IC50) M	Target	Intermediate conductance calcium-activated potassium channel protein 4 -Log(IC50) M model	40	8	external test set validation 	0.77	0.51	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4306	Potassium voltage-gated channel subfamily A member 5 -Log(IC50) M	Regression	-Log(IC50) M	Target	Potassium voltage-gated channel subfamily A member 5 -Log(IC50) M model	620	124	external test set validation 	0.63	0.406	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4308	B1 bradykinin receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	B1 bradykinin receptor -Log(IC50) M model	270	54	external test set validation 	0.68	0.734	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4315	P2Y purinoceptor 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	P2Y purinoceptor 1 -Log(IC50) M model	95	19	external test set validation 	0.64	1.065	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4323	Calcium-dependent phospholipase A2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Calcium-dependent phospholipase A2 -Log(IC50) M model	30	6	external test set validation 	0.6	0.688	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4327	Long-chain fatty acid transport protein 4 -Log(IC50) M	Regression	-Log(IC50) M	Target	Long-chain fatty acid transport protein 4 -Log(IC50) M model	30	6	external test set validation 	0	0.497	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4330	Cysteinyl leukotriene receptor 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Cysteinyl leukotriene receptor 2 -Log(IC50) M model	50	10	external test set validation 	0.89	0.682	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4333	Sphingosine 1-phosphate receptor 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Sphingosine 1-phosphate receptor 1 -Log(IC50) M model	435	87	external test set validation 	0.8	0.693	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4338	Purine nucleoside phosphorylase -Log(IC50) M	Regression	-Log(IC50) M	Target	Purine nucleoside phosphorylase -Log(IC50) M model	55	11	external test set validation 	0.7	0.92	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4342	Group 10 secretory phospholipase A2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Group 10 secretory phospholipase A2 -Log(IC50) M model	75	15	external test set validation 	0.67	0.878	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4349	N-acylethanolamine-hydrolyzing acid amidase -Log(IC50) M	Regression	-Log(IC50) M	Target	N-acylethanolamine-hydrolyzing acid amidase -Log(IC50) M model	65	13	external test set validation 	0.59	0.973	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4361	Induced myeloid leukemia cell differentiation protein Mcl-1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Induced myeloid leukemia cell differentiation protein Mcl-1 -Log(IC50) M model	1235	247	external test set validation 	0.45	0.443	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4370	UDP-glucuronosyltransferase 2B7 -Log(IC50) M	Regression	-Log(IC50) M	Target	UDP-glucuronosyltransferase 2B7 -Log(IC50) M model	25	5	external test set validation 	0.86	0.571	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4374	Photoreceptor-specific nuclear receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Photoreceptor-specific nuclear receptor -Log(IC50) M model	125	25	external test set validation 	0.11	0.415	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4376	Dual specificity tyrosine-phosphorylation-regulated kinase 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Dual specificity tyrosine-phosphorylation-regulated kinase 2 -Log(IC50) M model	75	15	external test set validation 	0.6	0.48	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4383	Gastric inhibitory polypeptide receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Gastric inhibitory polypeptide receptor -Log(IC50) M model	105	21	external test set validation 	0.46	0.371	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4393	Macrophage metalloelastase -Log(IC50) M	Regression	-Log(IC50) M	Target	Macrophage metalloelastase -Log(IC50) M model	295	59	external test set validation 	0.81	0.657	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4394	Sphingosine kinase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Sphingosine kinase 1 -Log(IC50) M model	100	20	external test set validation 	0.53	0.891	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4408	"High affinity cAMP-specific and IBMX-insensitive 3',5'-cyclic phosphodiesterase 8B -Log(IC50) M"	Regression	-Log(IC50) M	Target	"High affinity cAMP-specific and IBMX-insensitive 3',5'-cyclic phosphodiesterase 8B -Log(IC50) M model"	145	29	external test set validation 	0.54	0.626	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4409	"cAMP and cAMP-inhibited cGMP 3',5'-cyclic phosphodiesterase 10A -Log(IC50) M"	Regression	-Log(IC50) M	Target	"cAMP and cAMP-inhibited cGMP 3',5'-cyclic phosphodiesterase 10A -Log(IC50) M model"	2510	502	external test set validation 	0.68	0.637	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4411	Prostaglandin E synthase 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Prostaglandin E synthase 2 -Log(IC50) M model	30	6	external test set validation 	0.77	0.524	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4422	Free fatty acid receptor 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Free fatty acid receptor 1 -Log(IC50) M model	70	14	external test set validation 	0.52	0.703	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4423	C-C chemokine receptor type 6 -Log(IC50) M	Regression	-Log(IC50) M	Target	C-C chemokine receptor type 6 -Log(IC50) M model	265	53	external test set validation 	0.01	0.314	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4425	"Calcium/calmodulin-dependent 3',5'-cyclic nucleotide phosphodiesterase 1B -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Calcium/calmodulin-dependent 3',5'-cyclic nucleotide phosphodiesterase 1B -Log(IC50) M model"	25	5	external test set validation 	0.61	0.591	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4426	Phospholipase A2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Phospholipase A2 -Log(IC50) M model	425	85	external test set validation 	0.69	0.454	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4427	Prostaglandin D2 receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Prostaglandin D2 receptor -Log(IC50) M model	225	45	external test set validation 	0.55	0.741	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4429	Neuromedin-K receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Neuromedin-K receptor -Log(IC50) M model	290	58	external test set validation 	0.59	0.73	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4439	TGF-beta receptor type-1 -Log(IC50) M	Regression	-Log(IC50) M	Target	TGF-beta receptor type-1 -Log(IC50) M model	680	136	external test set validation 	0.76	0.568	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4441	C-X-C chemokine receptor type 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	C-X-C chemokine receptor type 3 -Log(IC50) M model	560	112	external test set validation 	0.72	0.474	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4444	Vitamin K-dependent protein C -Log(IC50) M	Regression	-Log(IC50) M	Target	Vitamin K-dependent protein C -Log(IC50) M model	95	19	external test set validation 	0.36	0.743	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4447	Kallikrein-5 -Log(IC50) M	Regression	-Log(IC50) M	Target	Kallikrein-5 -Log(IC50) M model	35	7	external test set validation 	0.32	0.752	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4454	Tyrosine-protein kinase Fgr -Log(IC50) M	Regression	-Log(IC50) M	Target	Tyrosine-protein kinase Fgr -Log(IC50) M model	30	6	external test set validation 	0.88	0.89	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4461	"NAD-dependent protein deacetylase sirtuin-3, mitochondrial -Log(IC50) M"	Regression	-Log(IC50) M	Target	"NAD-dependent protein deacetylase sirtuin-3, mitochondrial -Log(IC50) M model"	170	34	external test set validation 	0.68	0.595	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4462	NAD-dependent protein deacetylase sirtuin-2 -Log(IC50) M	Regression	-Log(IC50) M	Target	NAD-dependent protein deacetylase sirtuin-2 -Log(IC50) M model	535	107	external test set validation 	0.76	0.549	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4463	Platelet-activating factor acetylhydrolase IB subunit beta -Log(IC50) M	Regression	-Log(IC50) M	Target	Platelet-activating factor acetylhydrolase IB subunit beta -Log(IC50) M model	5	1	external test set validation 	nan	0	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4465	Sterol O-acyltransferase 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Sterol O-acyltransferase 2 -Log(IC50) M model	110	22	external test set validation 	0.89	0.798	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4478	Voltage-dependent N-type calcium channel subunit alpha-1B -Log(IC50) M	Regression	-Log(IC50) M	Target	Voltage-dependent N-type calcium channel subunit alpha-1B -Log(IC50) M model	310	62	external test set validation 	0.5	0.343	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4481	"Nitric oxide synthase, inducible -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Nitric oxide synthase, inducible -Log(IC50) M model"	620	124	external test set validation 	0.75	0.718	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4482	Serine/threonine-protein kinase PAK 4 -Log(IC50) M	Regression	-Log(IC50) M	Target	Serine/threonine-protein kinase PAK 4 -Log(IC50) M model	30	6	external test set validation 	0.64	0.877	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4501	Ribosomal protein S6 kinase beta-1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Ribosomal protein S6 kinase beta-1 -Log(IC50) M model	705	141	external test set validation 	0.54	0.716	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4502	Cytidine deaminase -Log(IC50) M	Regression	-Log(IC50) M	Target	Cytidine deaminase -Log(IC50) M model	20	4	external test set validation 	0.25	0.986	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4506	NAD-dependent protein deacetylase sirtuin-1 -Log(IC50) M	Regression	-Log(IC50) M	Target	NAD-dependent protein deacetylase sirtuin-1 -Log(IC50) M model	440	88	external test set validation 	0.77	0.471	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4507	High affinity choline transporter 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	High affinity choline transporter 1 -Log(IC50) M model	10	2	external test set validation 	1	0.097	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4508	Glutaminyl-peptide cyclotransferase -Log(IC50) M	Regression	-Log(IC50) M	Target	Glutaminyl-peptide cyclotransferase -Log(IC50) M model	110	22	external test set validation 	0.81	0.289	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4516	Serine/threonine-protein kinase MRCK alpha -Log(IC50) M	Regression	-Log(IC50) M	Target	Serine/threonine-protein kinase MRCK alpha -Log(IC50) M model	10	2	external test set validation 	1	0.305	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4521	"1,25-dihydroxyvitamin D(3) 24-hydroxylase, mitochondrial -Log(IC50) M"	Regression	-Log(IC50) M	Target	"1,25-dihydroxyvitamin D(3) 24-hydroxylase, mitochondrial -Log(IC50) M model"	60	12	external test set validation 	0.83	0.573	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4523	Serine/threonine-protein kinase pim-2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Serine/threonine-protein kinase pim-2 -Log(IC50) M model	825	165	external test set validation 	0.76	0.562	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4525	Serine/threonine-protein kinase 17A -Log(IC50) M	Regression	-Log(IC50) M	Target	Serine/threonine-protein kinase 17A -Log(IC50) M model	5	1	external test set validation 	nan	0.323	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4527	TRAF2 and NCK-interacting protein kinase -Log(IC50) M	Regression	-Log(IC50) M	Target	TRAF2 and NCK-interacting protein kinase -Log(IC50) M model	125	25	external test set validation 	0.56	0.359	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4530	Coagulation factor XIII A chain -Log(IC50) M	Regression	-Log(IC50) M	Target	Coagulation factor XIII A chain -Log(IC50) M model	100	20	external test set validation 	0.88	0.603	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4550	Arachidonate 5-lipoxygenase-activating protein -Log(IC50) M	Regression	-Log(IC50) M	Target	Arachidonate 5-lipoxygenase-activating protein -Log(IC50) M model	410	82	external test set validation 	0.67	0.553	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4561	Neuropeptide Y receptor type 5 -Log(IC50) M	Regression	-Log(IC50) M	Target	Neuropeptide Y receptor type 5 -Log(IC50) M model	725	145	external test set validation 	0.35	0.881	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4573	Metabotropic glutamate receptor 6 -Log(IC50) M	Regression	-Log(IC50) M	Target	Metabotropic glutamate receptor 6 -Log(IC50) M model	15	3	external test set validation 	0.63	0.796	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4578	Maternal embryonic leucine zipper kinase -Log(IC50) M	Regression	-Log(IC50) M	Target	Maternal embryonic leucine zipper kinase -Log(IC50) M model	670	134	external test set validation 	0.74	0.376	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4580	"Thymidine kinase 2, mitochondrial -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Thymidine kinase 2, mitochondrial -Log(IC50) M model"	25	5	external test set validation 	0.96	0.736	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4581	Kinesin-like protein KIF11 -Log(IC50) M	Regression	-Log(IC50) M	Target	Kinesin-like protein KIF11 -Log(IC50) M model	750	150	external test set validation 	0.64	0.775	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4588	Neutrophil collagenase -Log(IC50) M	Regression	-Log(IC50) M	Target	Neutrophil collagenase -Log(IC50) M model	430	86	external test set validation 	0.73	0.79	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4596	C-C chemokine receptor type 8 -Log(IC50) M	Regression	-Log(IC50) M	Target	C-C chemokine receptor type 8 -Log(IC50) M model	95	19	external test set validation 	0.36	0.723	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4599	Activated CDC42 kinase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Activated CDC42 kinase 1 -Log(IC50) M model	335	67	external test set validation 	0.61	0.569	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4600	Serine/threonine-protein kinase PAK 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Serine/threonine-protein kinase PAK 1 -Log(IC50) M model	120	24	external test set validation 	0.56	0.71	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4601	Protein-tyrosine kinase 6 -Log(IC50) M	Regression	-Log(IC50) M	Target	Protein-tyrosine kinase 6 -Log(IC50) M model	115	23	external test set validation 	0.47	0.631	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4605	Solute carrier family 15 member 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Solute carrier family 15 member 1 -Log(IC50) M model	85	17	external test set validation 	0.08	0.571	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4607	Type-2 angiotensin II receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Type-2 angiotensin II receptor -Log(IC50) M model	265	53	external test set validation 	0.46	0.777	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4608	Melanocortin receptor 5 -Log(IC50) M	Regression	-Log(IC50) M	Target	Melanocortin receptor 5 -Log(IC50) M model	145	29	external test set validation 	0.48	0.992	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4610	Xaa-Pro aminopeptidase 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Xaa-Pro aminopeptidase 2 -Log(IC50) M model	5	1	external test set validation 	nan	1.147	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4611	Complement C1r subcomponent -Log(IC50) M	Regression	-Log(IC50) M	Target	Complement C1r subcomponent -Log(IC50) M model	55	11	external test set validation 	0.3	0.431	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4616	Growth hormone secretagogue receptor type 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Growth hormone secretagogue receptor type 1 -Log(IC50) M model	635	127	external test set validation 	0.58	0.721	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4617	Phenylethanolamine N-methyltransferase -Log(IC50) M	Regression	-Log(IC50) M	Target	Phenylethanolamine N-methyltransferase -Log(IC50) M model	25	5	external test set validation 	0.71	0.768	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4618	Leukotriene A-4 hydrolase -Log(IC50) M	Regression	-Log(IC50) M	Target	Leukotriene A-4 hydrolase -Log(IC50) M model	300	60	external test set validation 	0.59	0.698	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4619	"Calcium/calmodulin-dependent 3',5'-cyclic nucleotide phosphodiesterase 1C -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Calcium/calmodulin-dependent 3',5'-cyclic nucleotide phosphodiesterase 1C -Log(IC50) M model"	60	12	external test set validation 	0.56	0.341	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4625	Bcl-2-like protein 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Bcl-2-like protein 1 -Log(IC50) M model	450	90	external test set validation 	0.79	0.603	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4630	Serine/threonine-protein kinase Chk1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Serine/threonine-protein kinase Chk1 -Log(IC50) M model	1695	339	external test set validation 	0.67	0.698	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4633	Potassium voltage-gated channel subfamily A member 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Potassium voltage-gated channel subfamily A member 3 -Log(IC50) M model	320	64	external test set validation 	0.77	0.66	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4641	Voltage-dependent T-type calcium channel subunit alpha-1G -Log(IC50) M	Regression	-Log(IC50) M	Target	Voltage-dependent T-type calcium channel subunit alpha-1G -Log(IC50) M model	270	54	external test set validation 	0.41	0.477	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4644	Melanocortin receptor 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Melanocortin receptor 3 -Log(IC50) M model	130	26	external test set validation 	0.57	0.88	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4647	"Peptide deformylase, mitochondrial -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Peptide deformylase, mitochondrial -Log(IC50) M model"	25	5	external test set validation 	0.9	0.682	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4657	Dipeptidyl peptidase 8 -Log(IC50) M	Regression	-Log(IC50) M	Target	Dipeptidyl peptidase 8 -Log(IC50) M model	915	183	external test set validation 	0.68	0.535	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4660	ADP-ribosyl cyclase/cyclic ADP-ribose hydrolase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	ADP-ribosyl cyclase/cyclic ADP-ribose hydrolase 1 -Log(IC50) M model	105	21	external test set validation 	0.87	0.504	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4662	Proteasome subunit beta type-5 -Log(IC50) M	Regression	-Log(IC50) M	Target	Proteasome subunit beta type-5 -Log(IC50) M model	260	52	external test set validation 	0.69	0.791	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4674	Mitogen-activated protein kinase 12 -Log(IC50) M	Regression	-Log(IC50) M	Target	Mitogen-activated protein kinase 12 -Log(IC50) M model	45	9	external test set validation 	0.45	0.604	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4679	"Branched-chain-amino-acid aminotransferase, cytosolic -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Branched-chain-amino-acid aminotransferase, cytosolic -Log(IC50) M model"	45	9	external test set validation 	0.32	0.967	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4681	Aldo-keto reductase family 1 member C3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Aldo-keto reductase family 1 member C3 -Log(IC50) M model	325	65	external test set validation 	0.72	0.534	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4683	Prolyl endopeptidase FAP -Log(IC50) M	Regression	-Log(IC50) M	Target	Prolyl endopeptidase FAP -Log(IC50) M model	185	37	external test set validation 	0.69	0.707	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4685	"Indoleamine 2,3-dioxygenase 1 -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Indoleamine 2,3-dioxygenase 1 -Log(IC50) M model"	405	81	external test set validation 	0.55	0.746	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4691	Proteinase-activated receptor 4 -Log(IC50) M	Regression	-Log(IC50) M	Target	Proteinase-activated receptor 4 -Log(IC50) M model	535	107	external test set validation 	0.62	0.667	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4692	HLA class II histocompatibility antigen gamma chain -Log(IC50) M	Regression	-Log(IC50) M	Target	HLA class II histocompatibility antigen gamma chain -Log(IC50) M model	15	3	external test set validation 	0.54	0.235	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4698	"Malonyl-CoA decarboxylase, mitochondrial -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Malonyl-CoA decarboxylase, mitochondrial -Log(IC50) M model"	255	51	external test set validation 	0.7	0.656	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4699	Protein-S-isoprenylcysteine O-methyltransferase -Log(IC50) M	Regression	-Log(IC50) M	Target	Protein-S-isoprenylcysteine O-methyltransferase -Log(IC50) M model	215	43	external test set validation 	0.64	0.576	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4718	MAP kinase-interacting serine/threonine-protein kinase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	MAP kinase-interacting serine/threonine-protein kinase 1 -Log(IC50) M model	100	20	external test set validation 	0.5	0.738	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4722	Aurora kinase A -Log(IC50) M	Regression	-Log(IC50) M	Target	Aurora kinase A -Log(IC50) M model	2060	412	external test set validation 	0.74	0.697	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4729	Cytochrome P450 2B6 -Log(IC50) M	Regression	-Log(IC50) M	Target	Cytochrome P450 2B6 -Log(IC50) M model	95	19	external test set validation 	0.19	0.536	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4736	DNA repair protein complementing XP-G cells -Log(IC50) M	Regression	-Log(IC50) M	Target	DNA repair protein complementing XP-G cells -Log(IC50) M model	25	5	external test set validation 	0.9	0.743	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4761	C3a anaphylatoxin chemotactic receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	C3a anaphylatoxin chemotactic receptor -Log(IC50) M model	180	36	external test set validation 	0.44	0.822	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4766	"[Pyruvate dehydrogenase (acetyl-transferring)] kinase isozyme 1, mitochondrial -Log(IC50) M"	Regression	-Log(IC50) M	Target	"[Pyruvate dehydrogenase (acetyl-transferring)] kinase isozyme 1, mitochondrial -Log(IC50) M model"	440	88	external test set validation 	0.39	0.649	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4767	Vesicular acetylcholine transporter -Log(IC50) M	Regression	-Log(IC50) M	Target	Vesicular acetylcholine transporter -Log(IC50) M model	70	14	external test set validation 	0.45	0.751	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4769	Geranylgeranyl pyrophosphate synthase -Log(IC50) M	Regression	-Log(IC50) M	Target	Geranylgeranyl pyrophosphate synthase -Log(IC50) M model	70	14	external test set validation 	0.73	0.731	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4772	1-acyl-sn-glycerol-3-phosphate acyltransferase beta -Log(IC50) M	Regression	-Log(IC50) M	Target	1-acyl-sn-glycerol-3-phosphate acyltransferase beta -Log(IC50) M model	100	20	external test set validation 	0.52	0.625	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4777	Neuropeptide Y receptor type 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Neuropeptide Y receptor type 1 -Log(IC50) M model	385	77	external test set validation 	0.82	0.634	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4789	"Carbonic anhydrase 5A, mitochondrial -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Carbonic anhydrase 5A, mitochondrial -Log(IC50) M model"	5	1	external test set validation 	nan	0.017	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4791	Endothelin-converting enzyme 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Endothelin-converting enzyme 1 -Log(IC50) M model	270	54	external test set validation 	0.74	0.455	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4792	Orexin receptor type 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Orexin receptor type 2 -Log(IC50) M model	465	93	external test set validation 	0.58	0.66	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4793	Dipeptidyl peptidase 9 -Log(IC50) M	Regression	-Log(IC50) M	Target	Dipeptidyl peptidase 9 -Log(IC50) M model	580	116	external test set validation 	0.7	0.54	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4794	Transient receptor potential cation channel subfamily V member 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Transient receptor potential cation channel subfamily V member 1 -Log(IC50) M model	1730	346	external test set validation 	0.56	0.647	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4796	Chymotrypsinogen B -Log(IC50) M	Regression	-Log(IC50) M	Target	Chymotrypsinogen B -Log(IC50) M model	60	12	external test set validation 	0.82	0.634	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4801	Caspase-1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Caspase-1 -Log(IC50) M model	390	78	external test set validation 	0.77	0.787	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4803	"Nitric oxide synthase, endothelial -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Nitric oxide synthase, endothelial -Log(IC50) M model"	495	99	external test set validation 	0.62	0.556	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4804	M-phase inducer phosphatase 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	M-phase inducer phosphatase 2 -Log(IC50) M model	425	85	external test set validation 	0.6	0.4	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4805	P2X purinoceptor 7 -Log(IC50) M	Regression	-Log(IC50) M	Target	P2X purinoceptor 7 -Log(IC50) M model	1910	382	external test set validation 	0.63	0.529	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4808	Acetylcholine receptor subunit alpha -Log(IC50) M	Regression	-Log(IC50) M	Target	Acetylcholine receptor subunit alpha -Log(IC50) M model	15	3	external test set validation 	0.78	0.69	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4816	RAC-gamma serine/threonine-protein kinase -Log(IC50) M	Regression	-Log(IC50) M	Target	RAC-gamma serine/threonine-protein kinase -Log(IC50) M model	200	40	external test set validation 	0.77	0.641	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4822	Beta-secretase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Beta-secretase 1 -Log(IC50) M model	4165	833	external test set validation 	0.75	0.619	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4829	Acetyl-CoA carboxylase 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Acetyl-CoA carboxylase 2 -Log(IC50) M model	2410	482	external test set validation 	0.65	0.442	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4835	L-lactate dehydrogenase A chain -Log(IC50) M	Regression	-Log(IC50) M	Target	L-lactate dehydrogenase A chain -Log(IC50) M model	165	33	external test set validation 	0.31	0.908	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4848	Eukaryotic translation initiation factor 4E -Log(IC50) M	Regression	-Log(IC50) M	Target	Eukaryotic translation initiation factor 4E -Log(IC50) M model	200	40	external test set validation 	0.42	0.579	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4860	Apoptosis regulator Bcl-2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Apoptosis regulator Bcl-2 -Log(IC50) M model	445	89	external test set validation 	0.78	0.472	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4878	Cytochrome P450 1B1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Cytochrome P450 1B1 -Log(IC50) M model	135	27	external test set validation 	0.32	0.92	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4883	Urokinase plasminogen activator surface receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Urokinase plasminogen activator surface receptor -Log(IC50) M model	50	10	external test set validation 	0.97	0.52	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4884	Caspase-2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Caspase-2 -Log(IC50) M model	15	3	external test set validation 	0.99	0.418	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4894	Galanin receptor type 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Galanin receptor type 1 -Log(IC50) M model	25	5	external test set validation 	0.34	0.346	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4895	Tyrosine-protein kinase receptor UFO -Log(IC50) M	Regression	-Log(IC50) M	Target	Tyrosine-protein kinase receptor UFO -Log(IC50) M model	225	45	external test set validation 	0.53	0.666	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4896	Lymphokine-activated killer T-cell-originated protein kinase -Log(IC50) M	Regression	-Log(IC50) M	Target	Lymphokine-activated killer T-cell-originated protein kinase -Log(IC50) M model	80	16	external test set validation 	0.7	0.58	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4897	Serine/threonine-protein kinase PLK3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Serine/threonine-protein kinase PLK3 -Log(IC50) M model	90	18	external test set validation 	0.6	0.671	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4898	BDNF/NT-3 growth factors receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	BDNF/NT-3 growth factors receptor -Log(IC50) M model	130	26	external test set validation 	0.53	0.775	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4899	Mitogen-activated protein kinase kinase kinase 8 -Log(IC50) M	Regression	-Log(IC50) M	Target	Mitogen-activated protein kinase kinase kinase 8 -Log(IC50) M model	210	42	external test set validation 	0.39	0.783	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4901	Ephrin type-B receptor 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Ephrin type-B receptor 3 -Log(IC50) M model	40	8	external test set validation 	0.4	0.818	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4903	Low molecular weight phosphotyrosine protein phosphatase -Log(IC50) M	Regression	-Log(IC50) M	Target	Low molecular weight phosphotyrosine protein phosphatase -Log(IC50) M model	75	15	external test set validation 	0.13	0.424	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4905	Receptor-type tyrosine-protein phosphatase gamma -Log(IC50) M	Regression	-Log(IC50) M	Target	Receptor-type tyrosine-protein phosphatase gamma -Log(IC50) M model	40	8	external test set validation 	0.43	0.676	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4917	Complement C3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Complement C3 -Log(IC50) M model	20	4	external test set validation 	0.52	0.519	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4940	L-lactate dehydrogenase B chain -Log(IC50) M	Regression	-Log(IC50) M	Target	L-lactate dehydrogenase B chain -Log(IC50) M model	60	12	external test set validation 	0.64	0.387	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4973	Excitatory amino acid transporter 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Excitatory amino acid transporter 2 -Log(IC50) M model	60	12	external test set validation 	0.66	0.681	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4977	Transcription factor AP-1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Transcription factor AP-1 -Log(IC50) M model	125	25	external test set validation 	0.57	0.636	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL4979	Sodium/glucose cotransporter 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Sodium/glucose cotransporter 1 -Log(IC50) M model	675	135	external test set validation 	0.92	0.415	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5021	Cyclin-dependent kinase inhibitor 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Cyclin-dependent kinase inhibitor 1 -Log(IC50) M model	5	1	external test set validation 	nan	0	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5023	E3 ubiquitin-protein ligase Mdm2 -Log(IC50) M	Regression	-Log(IC50) M	Target	E3 ubiquitin-protein ligase Mdm2 -Log(IC50) M model	1405	281	external test set validation 	0.88	0.54	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5024	Serine/threonine-protein kinase ATR -Log(IC50) M	Regression	-Log(IC50) M	Target	Serine/threonine-protein kinase ATR -Log(IC50) M model	105	21	external test set validation 	0.79	0.669	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5026	Eukaryotic elongation factor 2 kinase -Log(IC50) M	Regression	-Log(IC50) M	Target	Eukaryotic elongation factor 2 kinase -Log(IC50) M model	35	7	external test set validation 	0.01	0.609	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5027	Flap endonuclease 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Flap endonuclease 1 -Log(IC50) M model	50	10	external test set validation 	0.75	0.928	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5028	Disintegrin and metalloproteinase domain-containing protein 10 -Log(IC50) M	Regression	-Log(IC50) M	Target	Disintegrin and metalloproteinase domain-containing protein 10 -Log(IC50) M model	120	24	external test set validation 	0.43	0.618	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5038	Melanin-concentrating hormone receptor 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Melanin-concentrating hormone receptor 2 -Log(IC50) M model	25	5	external test set validation 	0.98	1.128	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5043	Endoplasmic reticulum aminopeptidase 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Endoplasmic reticulum aminopeptidase 2 -Log(IC50) M model	70	14	external test set validation 	0.32	0.809	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5048	Neutral cholesterol ester hydrolase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Neutral cholesterol ester hydrolase 1 -Log(IC50) M model	45	9	external test set validation 	0.82	0.391	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5071	Prostaglandin D2 receptor 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Prostaglandin D2 receptor 2 -Log(IC50) M model	1230	246	external test set validation 	0.59	0.619	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5080	Endothelial lipase -Log(IC50) M	Regression	-Log(IC50) M	Target	Endothelial lipase -Log(IC50) M model	330	66	external test set validation 	0.5	0.776	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5083	Poly [ADP-ribose] polymerase 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Poly [ADP-ribose] polymerase 3 -Log(IC50) M model	25	5	external test set validation 	0.85	0.575	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5101	Arylamine N-acetyltransferase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Arylamine N-acetyltransferase 1 -Log(IC50) M model	65	13	external test set validation 	0.3	0.454	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5103	Histone deacetylase 10 -Log(IC50) M	Regression	-Log(IC50) M	Target	Histone deacetylase 10 -Log(IC50) M model	195	39	external test set validation 	0.51	0.964	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5113	Orexin receptor type 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Orexin receptor type 1 -Log(IC50) M model	605	121	external test set validation 	0.71	0.576	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5119	Zinc finger protein GLI2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Zinc finger protein GLI2 -Log(IC50) M model	10	2	external test set validation 	1	0.22	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5122	Discoidin domain-containing receptor 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Discoidin domain-containing receptor 2 -Log(IC50) M model	70	14	external test set validation 	0.24	0.886	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5137	Metabotropic glutamate receptor 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Metabotropic glutamate receptor 2 -Log(IC50) M model	70	14	external test set validation 	0.76	0.536	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5141	Cytochrome P450 26A1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Cytochrome P450 26A1 -Log(IC50) M model	190	38	external test set validation 	0.55	0.649	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5145	Serine/threonine-protein kinase B-raf -Log(IC50) M	Regression	-Log(IC50) M	Target	Serine/threonine-protein kinase B-raf -Log(IC50) M model	1400	280	external test set validation 	0.81	0.529	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5147	Ephrin type-B receptor 4 -Log(IC50) M	Regression	-Log(IC50) M	Target	Ephrin type-B receptor 4 -Log(IC50) M model	385	77	external test set validation 	0.68	0.65	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5162	Neuropeptide S receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Neuropeptide S receptor -Log(IC50) M model	40	8	external test set validation 	0.31	0.514	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5163	Sodium channel protein type 3 subunit alpha -Log(IC50) M	Regression	-Log(IC50) M	Target	Sodium channel protein type 3 subunit alpha -Log(IC50) M model	25	5	external test set validation 	0	0.374	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5203	"Deoxyuridine 5'-triphosphate nucleotidohydrolase, mitochondrial -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Deoxyuridine 5'-triphosphate nucleotidohydrolase, mitochondrial -Log(IC50) M model"	145	29	external test set validation 	0.48	0.521	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5212	"Membrane-associated guanylate kinase, WW and PDZ domain-containing protein 3 -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Membrane-associated guanylate kinase, WW and PDZ domain-containing protein 3 -Log(IC50) M model"	10	2	external test set validation 	1	0.409	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5215	"Solute carrier family 2, facilitated glucose transporter member 3 -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Solute carrier family 2, facilitated glucose transporter member 3 -Log(IC50) M model"	35	7	external test set validation 	0.03	0.829	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5247	Integrin-linked protein kinase -Log(IC50) M	Regression	-Log(IC50) M	Target	Integrin-linked protein kinase -Log(IC50) M model	195	39	external test set validation 	0.51	0.467	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5251	Tyrosine-protein kinase BTK -Log(IC50) M	Regression	-Log(IC50) M	Target	Tyrosine-protein kinase BTK -Log(IC50) M model	690	138	external test set validation 	0.67	0.651	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5255	Toll-like receptor 4 -Log(IC50) M	Regression	-Log(IC50) M	Target	Toll-like receptor 4 -Log(IC50) M model	30	6	external test set validation 	0.79	0.904	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5260	Serine/threonine-protein kinase TNNI3K -Log(IC50) M	Regression	-Log(IC50) M	Target	Serine/threonine-protein kinase TNNI3K -Log(IC50) M model	70	14	external test set validation 	0.83	0.426	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5281	Cytochrome P450 2E1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Cytochrome P450 2E1 -Log(IC50) M model	50	10	external test set validation 	0.23	0.329	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5282	Cytochrome P450 2A6 -Log(IC50) M	Regression	-Log(IC50) M	Target	Cytochrome P450 2A6 -Log(IC50) M model	185	37	external test set validation 	0.18	0.86	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5285	Mitogen-activated protein kinase kinase kinase 5 -Log(IC50) M	Regression	-Log(IC50) M	Target	Mitogen-activated protein kinase kinase kinase 5 -Log(IC50) M model	85	17	external test set validation 	0.85	0.558	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5287	Sodium/bile acid cotransporter -Log(IC50) M	Regression	-Log(IC50) M	Target	Sodium/bile acid cotransporter -Log(IC50) M model	25	5	external test set validation 	0.55	0.803	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5310	Activin receptor type-1B -Log(IC50) M	Regression	-Log(IC50) M	Target	Activin receptor type-1B -Log(IC50) M model	35	7	external test set validation 	0.07	1.341	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5314	Tyrosine-protein kinase receptor TYRO3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Tyrosine-protein kinase receptor TYRO3 -Log(IC50) M model	215	43	external test set validation 	0.5	0.619	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5331	Tyrosine-protein kinase Mer -Log(IC50) M	Regression	-Log(IC50) M	Target	Tyrosine-protein kinase Mer -Log(IC50) M model	195	39	external test set validation 	0.79	0.608	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5347	Glucose-6-phosphate 1-dehydrogenase -Log(IC50) M	Regression	-Log(IC50) M	Target	Glucose-6-phosphate 1-dehydrogenase -Log(IC50) M model	275	55	external test set validation 	0.14	0.332	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5366	Poly [ADP-ribose] polymerase 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Poly [ADP-ribose] polymerase 2 -Log(IC50) M model	40	8	external test set validation 	0.68	0.82	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5368	Acid-sensing ion channel 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Acid-sensing ion channel 3 -Log(IC50) M model	75	15	external test set validation 	0.63	0.596	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5378	P-selectin -Log(IC50) M	Regression	-Log(IC50) M	Target	P-selectin -Log(IC50) M model	170	34	external test set validation 	0.39	0.937	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5393	ATP-binding cassette sub-family G member 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	ATP-binding cassette sub-family G member 2 -Log(IC50) M model	440	88	external test set validation 	0.54	0.548	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5399	Pituitary adenylate cyclase-activating polypeptide type I receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Pituitary adenylate cyclase-activating polypeptide type I receptor -Log(IC50) M model	60	12	external test set validation 	0.85	0.52	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5401	Signal transducer and activator of transcription 6 -Log(IC50) M	Regression	-Log(IC50) M	Target	Signal transducer and activator of transcription 6 -Log(IC50) M model	115	23	external test set validation 	0.82	0.542	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5406	Histone-arginine methyltransferase CARM1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Histone-arginine methyltransferase CARM1 -Log(IC50) M model	60	12	external test set validation 	0.82	0.695	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5407	Serine/threonine-protein kinase pim-3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Serine/threonine-protein kinase pim-3 -Log(IC50) M model	495	99	external test set validation 	0.67	0.471	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5408	Serine/threonine-protein kinase TBK1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Serine/threonine-protein kinase TBK1 -Log(IC50) M model	210	42	external test set validation 	0.38	0.655	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5409	G-protein coupled bile acid receptor 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	G-protein coupled bile acid receptor 1 -Log(IC50) M model	35	7	external test set validation 	0.14	0.661	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5413	KiSS-1 receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	KiSS-1 receptor -Log(IC50) M model	70	14	external test set validation 	0.2	1.177	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5427	Homeodomain-interacting protein kinase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Homeodomain-interacting protein kinase 1 -Log(IC50) M model	5	1	external test set validation 	nan	0.872	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5443	Cell division cycle 7-related protein kinase -Log(IC50) M	Regression	-Log(IC50) M	Target	Cell division cycle 7-related protein kinase -Log(IC50) M model	265	53	external test set validation 	0.58	0.567	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5451	Sodium channel protein type 10 subunit alpha -Log(IC50) M	Regression	-Log(IC50) M	Target	Sodium channel protein type 10 subunit alpha -Log(IC50) M model	105	21	external test set validation 	0.54	0.716	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5460	Heat shock 70 kDa protein 1A -Log(IC50) M	Regression	-Log(IC50) M	Target	Heat shock 70 kDa protein 1A -Log(IC50) M model	60	12	external test set validation 	0.13	0.691	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5461	Zinc finger protein GLI1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Zinc finger protein GLI1 -Log(IC50) M model	10	2	external test set validation 	1	0.216	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5462	Baculoviral IAP repeat-containing protein 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Baculoviral IAP repeat-containing protein 2 -Log(IC50) M model	345	69	external test set validation 	0.87	0.421	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5463	Acid ceramidase -Log(IC50) M	Regression	-Log(IC50) M	Target	Acid ceramidase -Log(IC50) M model	65	13	external test set validation 	0.51	0.503	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5464	Receptor-interacting serine/threonine-protein kinase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Receptor-interacting serine/threonine-protein kinase 1 -Log(IC50) M model	20	4	external test set validation 	0.71	0.883	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5469	Protein-tyrosine kinase 2-beta -Log(IC50) M	Regression	-Log(IC50) M	Target	Protein-tyrosine kinase 2-beta -Log(IC50) M model	185	37	external test set validation 	0.76	0.516	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5480	Perforin-1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Perforin-1 -Log(IC50) M model	180	36	external test set validation 	0.38	0.392	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5485	D-amino-acid oxidase -Log(IC50) M	Regression	-Log(IC50) M	Target	D-amino-acid oxidase -Log(IC50) M model	200	40	external test set validation 	0.37	0.941	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5491	Wee1-like protein kinase -Log(IC50) M	Regression	-Log(IC50) M	Target	Wee1-like protein kinase -Log(IC50) M model	240	48	external test set validation 	0.69	0.51	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5493	Free fatty acid receptor 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Free fatty acid receptor 2 -Log(IC50) M model	175	35	external test set validation 	0.56	0.638	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5500	Histone acetyltransferase KAT2B|Histone acetyltransferase KAT2B -Log(IC50) M	Regression	-Log(IC50) M	Target	Histone acetyltransferase KAT2B|Histone acetyltransferase KAT2B -Log(IC50) M model	55	11	external test set validation 	0.55	0.394	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5503	Nuclear receptor subfamily 1 group I member 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Nuclear receptor subfamily 1 group I member 3 -Log(IC50) M model	60	12	external test set validation 	0.38	0.635	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5514	Huntingtin -Log(IC50) M	Regression	-Log(IC50) M	Target	Huntingtin -Log(IC50) M model	10	2	external test set validation 	1	0.468	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5522	Transient receptor potential cation channel subfamily V member 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Transient receptor potential cation channel subfamily V member 3 -Log(IC50) M model	225	45	external test set validation 	0.34	0.465	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5523	Histone-lysine N-methyltransferase SETD7 -Log(IC50) M	Regression	-Log(IC50) M	Target	Histone-lysine N-methyltransferase SETD7 -Log(IC50) M model	45	9	external test set validation 	0.08	0.581	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5524	Protein arginine N-methyltransferase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Protein arginine N-methyltransferase 1 -Log(IC50) M model	115	23	external test set validation 	0.68	0.574	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5533	Transcription factor p65 -Log(IC50) M	Regression	-Log(IC50) M	Target	Transcription factor p65 -Log(IC50) M model	65	13	external test set validation 	0.64	0.349	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5543	Dual specificity tyrosine-phosphorylation-regulated kinase 1B -Log(IC50) M	Regression	-Log(IC50) M	Target	Dual specificity tyrosine-phosphorylation-regulated kinase 1B -Log(IC50) M model	175	35	external test set validation 	0.51	0.649	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5545	Sn1-specific diacylglycerol lipase alpha -Log(IC50) M	Regression	-Log(IC50) M	Target	Sn1-specific diacylglycerol lipase alpha -Log(IC50) M model	90	18	external test set validation 	0.32	0.812	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5554	Phosphatidylinositol 4-phosphate 3-kinase C2 domain-containing subunit beta -Log(IC50) M	Regression	-Log(IC50) M	Target	Phosphatidylinositol 4-phosphate 3-kinase C2 domain-containing subunit beta -Log(IC50) M model	45	9	external test set validation 	0.54	0.799	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5555	Acyl-CoA desaturase -Log(IC50) M	Regression	-Log(IC50) M	Target	Acyl-CoA desaturase -Log(IC50) M model	365	73	external test set validation 	0.34	0.701	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5568	Proto-oncogene tyrosine-protein kinase ROS -Log(IC50) M	Regression	-Log(IC50) M	Target	Proto-oncogene tyrosine-protein kinase ROS -Log(IC50) M model	60	12	external test set validation 	0.69	0.772	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5573	Intestinal-type alkaline phosphatase -Log(IC50) M	Regression	-Log(IC50) M	Target	Intestinal-type alkaline phosphatase -Log(IC50) M model	60	12	external test set validation 	0	0.546	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5575	Lysine--tRNA ligase -Log(IC50) M	Regression	-Log(IC50) M	Target	Lysine--tRNA ligase -Log(IC50) M model	70	14	external test set validation 	0.82	0.534	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5586	Carbonyl reductase [NADPH] 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Carbonyl reductase [NADPH] 1 -Log(IC50) M model	15	3	external test set validation 	0.73	0.753	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5619	DNA-(apurinic or apyrimidinic site) lyase -Log(IC50) M	Regression	-Log(IC50) M	Target	DNA-(apurinic or apyrimidinic site) lyase -Log(IC50) M model	90	18	external test set validation 	0.35	0.35	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5649	Prokineticin receptor 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Prokineticin receptor 1 -Log(IC50) M model	25	5	external test set validation 	0.06	0.662	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5658	Prostaglandin E synthase -Log(IC50) M	Regression	-Log(IC50) M	Target	Prostaglandin E synthase -Log(IC50) M model	735	147	external test set validation 	0.89	0.452	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5678	G protein-coupled receptor kinase 5 -Log(IC50) M	Regression	-Log(IC50) M	Target	G protein-coupled receptor kinase 5 -Log(IC50) M model	100	20	external test set validation 	0.36	0.714	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5685	Solute carrier family 22 member 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Solute carrier family 22 member 1 -Log(IC50) M model	90	18	external test set validation 	0.02	0.549	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5689	Tryptophan 5-hydroxylase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Tryptophan 5-hydroxylase 1 -Log(IC50) M model	155	31	external test set validation 	0.61	0.571	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5694	DNA ligase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	DNA ligase 1 -Log(IC50) M model	5	1	external test set validation 	nan	0.512	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5697	Egl nine homolog 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Egl nine homolog 1 -Log(IC50) M model	545	109	external test set validation 	0.81	0.72	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5704	Elongation of very long chain fatty acids protein 6 -Log(IC50) M	Regression	-Log(IC50) M	Target	Elongation of very long chain fatty acids protein 6 -Log(IC50) M model	130	26	external test set validation 	0.13	0.715	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5705	Egl nine homolog 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Egl nine homolog 3 -Log(IC50) M model	75	15	external test set validation 	0.59	0.617	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5731	Complement factor B -Log(IC50) M	Regression	-Log(IC50) M	Target	Complement factor B -Log(IC50) M model	5	1	external test set validation 	nan	0	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5742	Translocator protein -Log(IC50) M	Regression	-Log(IC50) M	Target	Translocator protein -Log(IC50) M model	5	1	external test set validation 	nan	0.598	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5747	CREB-binding protein|CREB-binding protein -Log(IC50) M	Regression	-Log(IC50) M	Target	CREB-binding protein|CREB-binding protein -Log(IC50) M model	190	38	external test set validation 	0.73	0.406	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5748	Canalicular multispecific organic anion transporter 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Canalicular multispecific organic anion transporter 1 -Log(IC50) M model	40	8	external test set validation 	0.58	0.572	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5776	Mitogen-activated protein kinase kinase kinase 7 -Log(IC50) M	Regression	-Log(IC50) M	Target	Mitogen-activated protein kinase kinase kinase 7 -Log(IC50) M model	90	18	external test set validation 	0.74	0.538	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5780	Sodium/nucleoside cotransporter 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Sodium/nucleoside cotransporter 2 -Log(IC50) M model	40	8	external test set validation 	0.72	0.49	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5784	NUAK family SNF1-like kinase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	NUAK family SNF1-like kinase 1 -Log(IC50) M model	30	6	external test set validation 	0.79	0.669	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5785	"Interferon-induced, double-stranded RNA-activated protein kinase -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Interferon-induced, double-stranded RNA-activated protein kinase -Log(IC50) M model"	65	13	external test set validation 	0.33	0.538	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5804	Toll-like receptor 9 -Log(IC50) M	Regression	-Log(IC50) M	Target	Toll-like receptor 9 -Log(IC50) M model	290	58	external test set validation 	0.74	0.271	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5807	Myelin-associated glycoprotein -Log(IC50) M	Regression	-Log(IC50) M	Target	Myelin-associated glycoprotein -Log(IC50) M model	35	7	external test set validation 	0.78	0.997	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5812	Dynamin-2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Dynamin-2 -Log(IC50) M model	15	3	external test set validation 	0	0.551	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5815	C-C chemokine receptor type 9 -Log(IC50) M	Regression	-Log(IC50) M	Target	C-C chemokine receptor type 9 -Log(IC50) M model	110	22	external test set validation 	0.01	0.928	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5845	Glycine receptor subunit alpha-1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Glycine receptor subunit alpha-1 -Log(IC50) M model	15	3	external test set validation 	1	0.393	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5847	Aldo-keto reductase family 1 member C2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Aldo-keto reductase family 1 member C2 -Log(IC50) M model	160	32	external test set validation 	0.72	0.398	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5852	Pyroglutamylated RFamide peptide receptor -Log(IC50) M	Regression	-Log(IC50) M	Target	Pyroglutamylated RFamide peptide receptor -Log(IC50) M model	55	11	external test set validation 	0.25	0.789	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5857	Trace amine-associated receptor 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Trace amine-associated receptor 1 -Log(IC50) M model	390	78	external test set validation 	0.03	0.511	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5866	Catenin beta-1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Catenin beta-1 -Log(IC50) M model	15	3	external test set validation 	0.77	0.741	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5868	Nuclear receptor ROR-alpha -Log(IC50) M	Regression	-Log(IC50) M	Target	Nuclear receptor ROR-alpha -Log(IC50) M model	45	9	external test set validation 	0.56	0.225	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5873	"Solute carrier family 2, facilitated glucose transporter member 2 -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Solute carrier family 2, facilitated glucose transporter member 2 -Log(IC50) M model"	25	5	external test set validation 	0.01	0.792	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5879	Hematopoietic prostaglandin D synthase -Log(IC50) M	Regression	-Log(IC50) M	Target	Hematopoietic prostaglandin D synthase -Log(IC50) M model	95	19	external test set validation 	0.73	0.636	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5887	D-aspartate oxidase -Log(IC50) M	Regression	-Log(IC50) M	Target	D-aspartate oxidase -Log(IC50) M model	15	3	external test set validation 	0.55	0.99	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5888	Mitogen-activated protein kinase kinase kinase 14 -Log(IC50) M	Regression	-Log(IC50) M	Target	Mitogen-activated protein kinase kinase kinase 14 -Log(IC50) M model	30	6	external test set validation 	0.35	0.929	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5891	Protein arginine N-methyltransferase 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Protein arginine N-methyltransferase 3 -Log(IC50) M model	30	6	external test set validation 	0.83	0.476	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5896	Lysine-specific demethylase 4A|Lysine-specific demethylase 4A|Lysine-specific demethylase 4A -Log(IC50) M	Regression	-Log(IC50) M	Target	Lysine-specific demethylase 4A|Lysine-specific demethylase 4A|Lysine-specific demethylase 4A -Log(IC50) M model	100	20	external test set validation 	0.75	0.446	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5905	Aldo-keto reductase family 1 member C1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Aldo-keto reductase family 1 member C1 -Log(IC50) M model	80	16	external test set validation 	0.06	1.194	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5917	"Malate dehydrogenase, mitochondrial -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Malate dehydrogenase, mitochondrial -Log(IC50) M model"	35	7	external test set validation 	0.99	0.419	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5925	Ectonucleotide pyrophosphatase/phosphodiesterase family member 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Ectonucleotide pyrophosphatase/phosphodiesterase family member 1 -Log(IC50) M model	10	2	external test set validation 	1	0.543	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5932	LIM domain kinase 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	LIM domain kinase 2 -Log(IC50) M model	135	27	external test set validation 	0.74	0.519	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5936	Toll-like receptor 7 -Log(IC50) M	Regression	-Log(IC50) M	Target	Toll-like receptor 7 -Log(IC50) M model	45	9	external test set validation 	0.6	0.617	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5938	Serine/threonine-protein kinase PLK2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Serine/threonine-protein kinase PLK2 -Log(IC50) M model	85	17	external test set validation 	0.48	0.681	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5939	Endoplasmic reticulum aminopeptidase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Endoplasmic reticulum aminopeptidase 1 -Log(IC50) M model	35	7	external test set validation 	0.08	1	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5957	5'-nucleotidase -Log(IC50) M	Regression	-Log(IC50) M	Target	5'-nucleotidase -Log(IC50) M model	5	1	external test set validation 	nan	0.389	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5963	Proteinase-activated receptor 2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Proteinase-activated receptor 2 -Log(IC50) M model	10	2	external test set validation 	1	2.092	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5966	IgG receptor FcRn large subunit p51 -Log(IC50) M	Regression	-Log(IC50) M	Target	IgG receptor FcRn large subunit p51 -Log(IC50) M model	140	28	external test set validation 	0.86	0.634	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5971	Smoothened homolog -Log(IC50) M	Regression	-Log(IC50) M	Target	Smoothened homolog -Log(IC50) M model	440	88	external test set validation 	0.65	0.454	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5979	"Alkaline phosphatase, tissue-nonspecific isozyme -Log(IC50) M"	Regression	-Log(IC50) M	Target	"Alkaline phosphatase, tissue-nonspecific isozyme -Log(IC50) M model"	280	56	external test set validation 	0.46	0.653	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5982	Disintegrin and metalloproteinase domain-containing protein 9 -Log(IC50) M	Regression	-Log(IC50) M	Target	Disintegrin and metalloproteinase domain-containing protein 9 -Log(IC50) M model	55	11	external test set validation 	0.46	0.665	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL5983	Aldo-keto reductase family 1 member B10 -Log(IC50) M	Regression	-Log(IC50) M	Target	Aldo-keto reductase family 1 member B10 -Log(IC50) M model	105	21	external test set validation 	0.62	0.829	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL6007	Transient receptor potential cation channel subfamily A member 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Transient receptor potential cation channel subfamily A member 1 -Log(IC50) M model	255	51	external test set validation 	0.52	0.543	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL6009	Diacylglycerol O-acyltransferase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Diacylglycerol O-acyltransferase 1 -Log(IC50) M model	535	107	external test set validation 	0.35	0.613	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL6020	Bile salt export pump -Log(IC50) M	Regression	-Log(IC50) M	Target	Bile salt export pump -Log(IC50) M model	65	13	external test set validation 	0.35	0.624	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL6029	Eukaryotic translation initiation factor 2-alpha kinase 1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Eukaryotic translation initiation factor 2-alpha kinase 1 -Log(IC50) M model	20	4	external test set validation 	0.45	0.478	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL6030	Eukaryotic translation initiation factor 2-alpha kinase 3 -Log(IC50) M	Regression	-Log(IC50) M	Target	Eukaryotic translation initiation factor 2-alpha kinase 3 -Log(IC50) M model	65	13	external test set validation 	0.87	0.7	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL6031	Histone-lysine N-methyltransferase EHMT1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Histone-lysine N-methyltransferase EHMT1 -Log(IC50) M model	10	2	external test set validation 	1	0.612	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL6032	Histone-lysine N-methyltransferase EHMT2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Histone-lysine N-methyltransferase EHMT2 -Log(IC50) M model	100	20	external test set validation 	0.58	1.256	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL6044	Bcl-2-related protein A1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Bcl-2-related protein A1 -Log(IC50) M model	35	7	external test set validation 	0.3	0.313	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL6088	Cell division control protein 42 homolog -Log(IC50) M	Regression	-Log(IC50) M	Target	Cell division control protein 42 homolog -Log(IC50) M model	20	4	external test set validation 	0.24	0.618	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL6089	Ubiquitin-conjugating enzyme E2 N -Log(IC50) M	Regression	-Log(IC50) M	Target	Ubiquitin-conjugating enzyme E2 N -Log(IC50) M model	345	69	external test set validation 	0.06	0.336	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL6095	DNA (cytosine-5)-methyltransferase 3B|DNA (cytosine-5)-methyltransferase 3B -Log(IC50) M	Regression	-Log(IC50) M	Target	DNA (cytosine-5)-methyltransferase 3B|DNA (cytosine-5)-methyltransferase 3B -Log(IC50) M model	30	6	external test set validation 	0.02	0.829	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL6111	Protein-arginine deiminase type-4 -Log(IC50) M	Regression	-Log(IC50) M	Target	Protein-arginine deiminase type-4 -Log(IC50) M model	30	6	external test set validation 	0.78	0.722	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL6113	Phosphoethanolamine/phosphocholine phosphatase -Log(IC50) M	Regression	-Log(IC50) M	Target	Phosphoethanolamine/phosphocholine phosphatase -Log(IC50) M model	75	15	external test set validation 	0.09	0.868	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL6114	Phospholipase A-2-activating protein -Log(IC50) M	Regression	-Log(IC50) M	Target	Phospholipase A-2-activating protein -Log(IC50) M model	5	1	external test set validation 	nan	0.59	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL6115	Lysosomal protective protein -Log(IC50) M	Regression	-Log(IC50) M	Target	Lysosomal protective protein -Log(IC50) M model	425	85	external test set validation 	0.6	0.465	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL6117	Tyrosine-protein phosphatase non-receptor type 9 -Log(IC50) M	Regression	-Log(IC50) M	Target	Tyrosine-protein phosphatase non-receptor type 9 -Log(IC50) M model	25	5	external test set validation 	0.52	0.495	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL6120	Solute carrier family 22 member 12 -Log(IC50) M	Regression	-Log(IC50) M	Target	Solute carrier family 22 member 12 -Log(IC50) M model	300	60	external test set validation 	0.39	0.54	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL6136	Lysine-specific histone demethylase 1A -Log(IC50) M	Regression	-Log(IC50) M	Target	Lysine-specific histone demethylase 1A -Log(IC50) M model	270	54	external test set validation 	0.64	0.597	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL6144	G protein-coupled receptor kinase 6 -Log(IC50) M	Regression	-Log(IC50) M	Target	G protein-coupled receptor kinase 6 -Log(IC50) M model	95	19	external test set validation 	0.08	0.628	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL6152	Alpha-synuclein -Log(IC50) M	Regression	-Log(IC50) M	Target	Alpha-synuclein -Log(IC50) M model	35	7	external test set validation 	0.75	0.445	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL6154	Tankyrase-2 -Log(IC50) M	Regression	-Log(IC50) M	Target	Tankyrase-2 -Log(IC50) M model	260	52	external test set validation 	0.63	0.614	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL6164	Tankyrase-1 -Log(IC50) M	Regression	-Log(IC50) M	Target	Tankyrase-1 -Log(IC50) M model	195	39	external test set validation 	0.69	0.615	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL6166	Mitogen-activated protein kinase kinase kinase kinase 4 -Log(IC50) M	Regression	-Log(IC50) M	Target	Mitogen-activated protein kinase kinase kinase kinase 4 -Log(IC50) M model	105	21	external test set validation 	0.63	0.625	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
CHEMBL6175	Lysine-specific demethylase 4C|Lysine-specific demethylase 4C|Lysine-specific demethylase 4C -Log(IC50) M	Regression	-Log(IC50) M	Target	Lysine-specific demethylase 4C|Lysine-specific demethylase 4C|Lysine-specific demethylase 4C -Log(IC50) M model	145	29	external test set validation 	0.52	0.698	N/A	N/A	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 700, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"ChEMBL database, version 23"	chembl_training_set.csv
tox21-ahr-p1	"Aryl hydrocarbon receptor activator, Activity tested in HepG2 cells"	Classification	Probability	Toxicity	"Aryl hydrocarbon receptor activator, Activity in HepG2 cells model"	5194	1284	external test set validation 	N/A	N/A	0.76	0.9	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
tox21-ar-bla-agonist-p1	"Androgen receptor agonist, Activity tested in HEK293 cells"	Classification	Probability	Toxicity	"Androgen receptor agonist, Activity in HEK293 cells model"	5351	1335	external test set validation 	N/A	N/A	0.82	0.91	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
tox21-ar-bla-antagonist-p1	"Androgen receptor antagonist, Activity tested in HEK293 cells"	Classification	Probability	Toxicity	"Androgen receptor antagonist, Activity in HEK293 cells model"	4864	1224	external test set validation 	N/A	N/A	0.72	0.88	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
tox21-ar-mda-kb2-luc-agonist-p1	"Androgen receptor agonist, Activity tested in MDA-kb2 cells"	Classification	Probability	Toxicity	"Androgen receptor agonist, Activity in MDA-kb2 cells model"	5743	1423	external test set validation 	N/A	N/A	0.79	0.85	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
tox21-aromatase-p1	"Aromatase inhibitor, Activity tested in MCF-7 aro ERE cells"	Classification	Probability	Toxicity	"Aromatase inhibitor, Activity in MCF-7 aro ERE cells model"	4759	1174	external test set validation 	N/A	N/A	0.72	0.85	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
tox21-elg1-luc-agonist-p1	"Genotoxicity inducer, Activity tested in HEK293T cells"	Classification	Probability	Toxicity	"Genotoxicity inducer, Activity in HEK293T cells model"	5573	1406	external test set validation 	N/A	N/A	0.65	0.87	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
tox21-er-bla-agonist-p2	"Estrogen receptor alpha agonist, Activity tested in HEK293 cells"	Classification	Probability	Toxicity	"Estrogen receptor alpha agonist, Activity in HEK293 cells model"	5544	1397	external test set validation 	N/A	N/A	0.71	0.85	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
tox21-er-bla-antagonist-p1	"Estrogen receptor alpha antagonist, Activity tested in HEK293 cells"	Classification	Probability	Toxicity	"Estrogen receptor alpha antagonist, Activity in HEK293 cells model"	4861	1207	external test set validation 	N/A	N/A	0.64	0.86	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
tox21-er-luc-bg1-4e2-agonist-p2	"Estrogen receptor alpha agonist, Activity tested in BG1 cells"	Classification	Probability	Toxicity	"Estrogen receptor alpha agonist, Activity in BG1 cells model"	4913	1235	external test set validation 	N/A	N/A	0.65	0.74	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
tox21-er-luc-bg1-4e2-antagonist-p1	"Estrogen receptor alpha antagonist, Activity tested in BG1 cells"	Classification	Probability	Toxicity	"Estrogen receptor alpha antagonist, Activity in BG1 cells model"	4964	1243	external test set validation 	N/A	N/A	0.64	0.83	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
tox21-gh3-tre-agonist-p1	"Thyroid receptor agonist, Activity tested in GH3.TRE-Luc cells"	Classification	Probability	Toxicity	"Thyroid receptor agonist, Activity in GH3.TRE-Luc cells model"	5377	1341	external test set validation 	N/A	N/A	0.5	0.59	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
tox21-gh3-tre-antagonist-p1	"Thyroid receptor antagonist, Activity tested in GH3.TRE-Luc cells"	Classification	Probability	Toxicity	"Thyroid receptor antagonist, Activity in GH3.TRE-Luc cells model"	4308	1079	external test set validation 	N/A	N/A	0.66	0.89	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
tox21-gr-hela-bla-agonist-p1	"Glucocorticoid receptor agonist, Activity tested in HeLa cells"	Classification	Probability	Toxicity	"Glucocorticoid receptor agonist, Activity in HeLa cells model"	5327	1349	external test set validation 	N/A	N/A	0.79	0.9	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
tox21-gr-hela-bla-antagonist-p1	"Glucocorticoid receptor antagonist, Activity tested in HeLa cells"	Classification	Probability	Toxicity	"Glucocorticoid receptor antagonist, Activity in HeLa cells model"	4735	1185	external test set validation 	N/A	N/A	0.69	0.84	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
tox21-mitotox-p1	"Mitochondrial membrane potential disruptor, Activity tested in HepG2 cells"	Classification	Probability	Toxicity	"Mitochondrial membrane potential disruptor, Activity in HepG2 cells model"	4104	1042	external test set validation 	N/A	N/A	0.67	0.91	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
tox21-p53-bla-p1	"p53 agonist, Activity tested in HCT-116 cells"	Classification	Probability	Toxicity	"p53 agonist, Activity in HCT-116 cells model"	5396	1343	external test set validation 	N/A	N/A	0.69	0.86	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
tox21-pparg-bla-agonist-p1	"Peroxisome proliferator-activated receptor gamma agonist, Activity tested in HEK293H cells"	Classification	Probability	Toxicity	"Peroxisome proliferator-activated receptor gamma agonist, Activity in HEK293H cells model"	5140	1285	external test set validation 	N/A	N/A	0.61	0.89	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
tox21-ar-mda-kb2-luc-antagonist-p1	"Androgen receptor antagonist, Activity tested in MDA-kb2 cells"	Classification	Probability	Toxicity	"Androgen receptor antagonist, Activity in MDA-kb2 cells model"	4820	1201	external test set validation 	N/A	N/A	0.65	0.85	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
tox21-hse-bla-p1	"Heat shock response activator, Activity tested in HeLa cells"	Classification	Probability	Toxicity	"Heat shock response activator, Activity in HeLa cells model"	4204	1054	external test set validation 	N/A	N/A	0.64	0.8	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
tox21-nfkb-bla-agonist-p1	"Nuclear factor-kappa B agonist, Activity tested in ME-180 cells"	Classification	Probability	Toxicity	"Nuclear factor-kappa B agonist, Activity in ME-180 cells model"	4329	1093	external test set validation 	N/A	N/A	0.71	0.83	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
tox21-rar-antagonist-p2	"Retinoic acid receptor antagonist, Activity tested in C3RL4 cells"	Classification	Probability	Toxicity	"Retinoic acid receptor antagonist, Activity in C3RL4 cells model"	3488	895	external test set validation 	N/A	N/A	0.69	0.86	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
tox21-ror-cho-antagonist-p1	"Retinoid-related orphan receptor gamma antagonist, Activity tested in CHO cells"	Classification	Probability	Toxicity	"Retinoid-related orphan receptor gamma antagonist, Activity in CHO cells model"	3410	875	external test set validation 	N/A	N/A	0.68	0.89	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
tox21-tshr-agonist-p1	"Thyroid stimulating hormone receptor agonist, Activity tested in Hek293 cells"	Classification	Probability	Toxicity	"Thyroid stimulating hormone receptor agonist, Activity in Hek293 cells model"	4453	1108	external test set validation 	N/A	N/A	0.67	0.85	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
tox21-ap1-agonist-p1	"Activator protein-1 agonist, Activity tested in ME-180 cells"	Classification	Probability	Toxicity	"Activator protein-1 agonist, Activity in ME-180 cells model"	4357	1105	external test set validation 	N/A	N/A	0.64	0.76	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
tox21-are-bla-p1	"Antioxidant response element agonist, Activity tested in HepG2 cells"	Classification	Probability	Toxicity	"Antioxidant response element agonist, Activity in HepG2 cells model"	3796	936	external test set validation 	N/A	N/A	0.68	0.85	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
tox21-car-agonist-p1	"Constitutive androstane receptor agonist, Activity tested in HepG2 cells"	Classification	Probability	Toxicity	"Constitutive androstane receptor agonist, Activity in HepG2 cells model"	4376	1091	external test set validation 	N/A	N/A	0.72	0.88	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
tox21-car-antagonist-p1	"Constitutive androstane receptor antagonist, Activity tested in HepG2 cells"	Classification	Probability	Toxicity	"Constitutive androstane receptor antagonist, Activity in HepG2 cells model"	3375	862	external test set validation 	N/A	N/A	0.63	0.84	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
tox21-esre-bla-p1	"Endoplasmic reticulum stress response agonist, Activity tested in HeLa cells"	Classification	Probability	Toxicity	"Endoplasmic reticulum stress response agonist, Activity in HeLa cells model"	4151	1042	external test set validation 	N/A	N/A	0.69	0.8	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
tox21-fxr-bla-agonist-p2	"Farnesoid-X-receptor agonist, Activity tested in HEK 293T cells"	Classification	Probability	Toxicity	"Farnesoid-X-receptor agonist, Activity in HEK 293T cells model"	4328	1087	external test set validation 	N/A	N/A	0.65	0.9	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
tox21-fxr-bla-antagonist-p1	"Farnesoid-X-receptor antagonist, Activity tested in HEK 293T cells"	Classification	Probability	Toxicity	"Farnesoid-X-receptor antagonist, Activity in HEK 293T cells model"	3951	985	external test set validation 	N/A	N/A	0.65	0.89	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
tox21-h2ax-cho-p2	"H2AX agonist, Activity tested in CHO-K1 cells"	Classification	Probability	Toxicity	"H2AX agonist, Activity in CHO-K1 cells model"	4518	1127	external test set validation 	N/A	N/A	0.63	0.85	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
tox21-hre-bla-agonist-p1	"Hypoxia-inducible factor-1 agonist, Activity tested in ME-180 cells"	Classification	Probability	Toxicity	"Hypoxia-inducible factor-1 agonist, Activity in ME-180 cells model"	4460	1122	external test set validation 	N/A	N/A	0.7	0.82	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
tox21-ppard-bla-agonist-p1	"Peroxisome proliferator-activated receptor delta agonist, Activity tested in HEK293H cells"	Classification	Probability	Toxicity	"Peroxisome proliferator-activated receptor delta agonist, Activity in HEK293H cells model"	4157	1043	external test set validation 	N/A	N/A	0.64	0.86	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
tox21-ppard-bla-antagonist-p1	"Peroxisome proliferator-activated receptor delta antagonist, Activity tested in HEK293H cells"	Classification	Probability	Toxicity	"Peroxisome proliferator-activated receptor delta antagonist, Activity in HEK293H cells model"	4004	1011	external test set validation 	N/A	N/A	0.62	0.86	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
tox21-pparg-bla-antagonist-p1	"Peroxisome proliferator-activated receptor gamma antagonist, Activity tested in HEK293H cells"	Classification	Probability	Toxicity	"Peroxisome proliferator-activated receptor gamma antagonist, Activity in HEK293H cells model"	3878	965	external test set validation 	N/A	N/A	0.66	0.89	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
tox21-rar-agonist-p1	"Retinoic acid receptor agonist, Activity tested in C3RL4 cells"	Classification	Probability	Toxicity	"Retinoic acid receptor agonist, Activity in C3RL4 cells model"	3948	1003	external test set validation 	N/A	N/A	0.74	0.86	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
tox21-rxr-bla-agonist-p1	"Retinoid X receptor agonist, Activity tested in HEK 293T cells"	Classification	Probability	Toxicity	"Retinoid X receptor agonist, Activity in HEK 293T cells model"	3781	946	external test set validation 	N/A	N/A	0.57	0.65	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
tox21-vdr-bla-agonist-p1	"Vitamin D receptor agonist, Activity tested in HEK 293T cells"	Classification	Probability	Toxicity	"Vitamin D receptor agonist, Activity in HEK 293T cells model"	4192	1053	external test set validation 	N/A	N/A	0.6	0.74	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
tox21-vdr-bla-antagonist-p1	"Vitamin D receptor antagonist, Activity tested in HEK 293T cells"	Classification	Probability	Toxicity	"Vitamin D receptor antagonist, Activity in HEK 293T cells model"	3925	962	external test set validation 	N/A	N/A	0.69	0.84	"The model was developed using the RDkit fingerprints: Morgan, Avalon and Atom Pairs (ref1)."	"Deep Learning model was built using multi-layer feedforward neural networks implemented in Keras using the Theano backend (ref2). For minimization of the loss function we used ADAM algorithm, which computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. To construct the best DL model the following parameters were considered: the number of layers {4}, the number of neurons {4000, 2000, 1000, 500}, activation function {Rectified Linear}, batch normalization was performed for each layer."	The nearest neighbor from the training set is calculated for each test compound using similarity estimation. The pair-wise Tanimoto similarity of this compound with each of its neighbor is calculated using the Morgan fingerprints. The similarity values is used for assessment of the applicability domain (AD) of the model. For that model an AD threshold of 0.7 was used.	Ref1: http://www.rdkit.org/|Ref2: https://keras.io/	"Tox21 data, ""https://tripod.nih.gov/tox21"""	tox21_training_set.csv
